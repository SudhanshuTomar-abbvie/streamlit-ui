{
  "metadata": {
    "kernelspec": {
      "display_name": "Python in dku-cdl-prod-eks-cluster-001 (env cdl-p-diku-psbts-py39-llm-env)",
      "language": "python",
      "name": "py-dku-containerized-venv-cdl-p-diku-psbts-py39-llm-env-dku-cdl-prod-eks-cluster-001"
    },
    "associatedRecipe": "compute_output_test",
    "creator": "BERRYKX2",
    "createdOn": 1747070147590,
    "tags": [
      "recipe-editor"
    ],
    "customFields": {}
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import logging\n",
        "import pandas as pd\n",
        "from io import BytesIO\n",
        "from typing import Dict, List, Any, Optional\n",
        "\n",
        "# Configure logging\n",
        "logger \u003d logging.getLogger(__name__)\n",
        "\n",
        "class ExcelExtractor:\n",
        "    \"\"\"\n",
        "    Extracts content from Excel files (xlsx, xls) using pandas.\n",
        "    Can be integrated into the existing document digitization pipeline.\n",
        "    \"\"\"\n",
        "\n",
        "    def extract_excel_content(self, file_data: bytes, file_path: str) -\u003e Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Extract content from Excel files.\n",
        "\n",
        "        Args:\n",
        "            file_data: Raw bytes of the Excel file\n",
        "            file_path: Path to the Excel file (used for logging and metadata)\n",
        "\n",
        "        Returns:\n",
        "            Dict with extracted content:\n",
        "                - text: Extracted text representation\n",
        "                - sheets: List of sheet data\n",
        "                - metadata: File metadata\n",
        "        \"\"\"\n",
        "        try:\n",
        "            file_name \u003d os.path.basename(file_path)\n",
        "            logger.info(f\"Extracting content from Excel file: {file_name}\")\n",
        "\n",
        "            # Create a BytesIO object from file data\n",
        "            excel_stream \u003d BytesIO(file_data)\n",
        "\n",
        "            # Read all sheets from the Excel file\n",
        "            excel_file \u003d pd.ExcelFile(excel_stream)\n",
        "            sheet_names \u003d excel_file.sheet_names\n",
        "\n",
        "            # Process each sheet\n",
        "            sheets_data \u003d []\n",
        "            all_text_content \u003d []\n",
        "\n",
        "            for sheet_name in sheet_names:\n",
        "                # Read the sheet into a DataFrame\n",
        "                df \u003d pd.read_excel(excel_file, sheet_name\u003dsheet_name)\n",
        "\n",
        "                # Handle empty sheets\n",
        "                if df.empty:\n",
        "                    sheets_data.append({\n",
        "                        \"sheet_name\": sheet_name,\n",
        "                        \"is_empty\": True,\n",
        "                        \"text\": f\"[Sheet \u0027{sheet_name}\u0027 is empty]\"\n",
        "                    })\n",
        "                    all_text_content.append(f\"\\n--- Sheet: {sheet_name} ---\\n[Empty sheet]\")\n",
        "                    continue\n",
        "\n",
        "                # Get sheet content as text\n",
        "                text_representation \u003d self._dataframe_to_text(df, sheet_name)\n",
        "                all_text_content.append(text_representation)\n",
        "\n",
        "                # Store sheet data\n",
        "                sheets_data.append({\n",
        "                    \"sheet_name\": sheet_name,\n",
        "                    \"is_empty\": False,\n",
        "                    \"row_count\": len(df),\n",
        "                    \"column_count\": len(df.columns),\n",
        "                    \"columns\": df.columns.tolist(),\n",
        "                    \"text\": text_representation\n",
        "                })\n",
        "\n",
        "            # Create metadata\n",
        "            metadata \u003d {\n",
        "                \"file_name\": file_name,\n",
        "                \"file_path\": file_path,\n",
        "                \"file_type\": os.path.splitext(file_name)[1].lower(),\n",
        "                \"sheet_count\": len(sheet_names),\n",
        "                \"sheet_names\": sheet_names\n",
        "            }\n",
        "\n",
        "            # Combine all content\n",
        "            full_text_content \u003d \"\\n\\n\".join(all_text_content)\n",
        "\n",
        "            result \u003d {\n",
        "                \"text\": full_text_content,\n",
        "                \"sheets\": sheets_data,\n",
        "                \"metadata\": metadata\n",
        "            }\n",
        "\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error extracting content from Excel file {file_path}: {e}\")\n",
        "            return {\n",
        "                \"text\": f\"Error extracting Excel content: {str(e)}\",\n",
        "                \"sheets\": [],\n",
        "                \"metadata\": {\"file_name\": os.path.basename(file_path), \"file_path\": file_path, \"error\": str(e)}\n",
        "            }\n",
        "\n",
        "    def _dataframe_to_text(self, df: pd.DataFrame, sheet_name: str) -\u003e str:\n",
        "        \"\"\"\n",
        "        Convert a DataFrame to a text representation.\n",
        "\n",
        "        Args:\n",
        "            df: The pandas DataFrame to convert\n",
        "            sheet_name: Name of the sheet\n",
        "\n",
        "        Returns:\n",
        "            str: Text representation of the DataFrame\n",
        "        \"\"\"\n",
        "        # Get dataframe shape\n",
        "        rows, cols \u003d df.shape\n",
        "\n",
        "        # Start with sheet header\n",
        "        text_lines \u003d [f\"--- Sheet: {sheet_name} ---\"]\n",
        "        text_lines.append(f\"Rows: {rows}, Columns: {cols}\")\n",
        "        text_lines.append(\"\")\n",
        "\n",
        "        # Check if DataFrame is too large for complete text extraction\n",
        "        max_rows \u003d 500\n",
        "        max_cols \u003d 20\n",
        "\n",
        "        if rows \u003e max_rows or cols \u003e max_cols:\n",
        "            # For large DataFrames, extract summary and sample data\n",
        "            text_lines.append(\"Note: Large sheet - showing sample data and summary\")\n",
        "            text_lines.append(\"\")\n",
        "\n",
        "            # Add column names\n",
        "            if cols \u003e max_cols:\n",
        "                # Show truncated column list\n",
        "                col_sample \u003d list(df.columns[:max_cols])\n",
        "                text_lines.append(f\"Columns (first {max_cols} of {cols}): {\u0027, \u0027.join(str(col) for col in col_sample)}...\")\n",
        "            else:\n",
        "                text_lines.append(f\"Columns: {\u0027, \u0027.join(str(col) for col in df.columns)}\")\n",
        "\n",
        "            # Add sample data (first 50 rows)\n",
        "            text_lines.append(\"\")\n",
        "            text_lines.append(\"Sample data (first rows):\")\n",
        "\n",
        "            # Truncate DataFrame for text representation\n",
        "            sample_df \u003d df.iloc[:min(50, rows), :min(max_cols, cols)]\n",
        "\n",
        "            # Convert sample to string and add to text\n",
        "            sample_text \u003d sample_df.fillna(\"\").to_string(index\u003dFalse)\n",
        "            text_lines.append(sample_text)\n",
        "\n",
        "            # Add data summary\n",
        "            text_lines.append(\"\")\n",
        "            text_lines.append(\"Numerical columns summary:\")\n",
        "\n",
        "            # Get summary of numerical columns\n",
        "            numeric_cols \u003d df.select_dtypes(include\u003d[\u0027number\u0027]).columns\n",
        "            if not numeric_cols.empty:\n",
        "                summary_df \u003d df[numeric_cols].describe().round(2)\n",
        "                summary_text \u003d summary_df.to_string()\n",
        "                text_lines.append(summary_text)\n",
        "            else:\n",
        "                text_lines.append(\"[No numeric columns found]\")\n",
        "        else:\n",
        "            # For smaller DataFrames, include full data\n",
        "            filled_df \u003d df.fillna(\"\")\n",
        "            text_lines.append(filled_df.to_string(index\u003dFalse))\n",
        "\n",
        "        return \"\\n\".join(text_lines)\n",
        "\n",
        "\n",
        "# Integration code for your document processor\n",
        "def handle_excel_file(file_data, file_path):\n",
        "    \"\"\"\n",
        "    Handler function for Excel files that you can call from your document processor.\n",
        "\n",
        "    Args:\n",
        "        file_data: Raw bytes of the Excel file\n",
        "        file_path: Path to the Excel file\n",
        "\n",
        "    Returns:\n",
        "        Dict with extracted content\n",
        "    \"\"\"\n",
        "    extractor \u003d ExcelExtractor()\n",
        "    result \u003d extractor.extract_excel_content(file_data, file_path)\n",
        "\n",
        "    # Format result for the combined extracted data format\n",
        "    combined_data \u003d []\n",
        "\n",
        "    # Add metadata\n",
        "    metadata \u003d result.get(\"metadata\", {})\n",
        "    combined_data.append(f\"DOCUMENT METADATA:\")\n",
        "    for key, value in metadata.items():\n",
        "        combined_data.append(f\"{key}: {value}\")\n",
        "    combined_data.append(\"\\n\")\n",
        "\n",
        "    # Add text content from Excel\n",
        "    text_content \u003d result.get(\"text\", \"\")\n",
        "    if text_content:\n",
        "        combined_data.append(\"EXCEL CONTENT:\")\n",
        "        combined_data.append(text_content)\n",
        "        combined_data.append(\"\\n\")\n",
        "\n",
        "    return {\n",
        "        \"text\": result.get(\"text\", \"\"),\n",
        "        \"tables\": [{\"text\": sheet.get(\"text\", \"\"), \"metadata\": {\"sheet_name\": sheet.get(\"sheet_name\", \"\")}}\n",
        "                  for sheet in result.get(\"sheets\", [])],\n",
        "        \"images\": [],\n",
        "        \"metadata\": result.get(\"metadata\", {})\n",
        "    }\n",
        "\n",
        "import os\n",
        "import platform\n",
        "import logging\n",
        "import subprocess\n",
        "import tempfile\n",
        "import shutil\n",
        "import zipfile\n",
        "import tarfile\n",
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "logger \u003d logging.getLogger(__name__)\n",
        "\n",
        "def download_file(url, destination):\n",
        "    \"\"\"Download a file from URL to destination.\"\"\"\n",
        "    try:\n",
        "        logger.info(f\"Downloading {url} to {destination}\")\n",
        "        response \u003d requests.get(url, stream\u003dTrue)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        with open(destination, \u0027wb\u0027) as f:\n",
        "            for chunk in response.iter_content(chunk_size\u003d8192):\n",
        "                f.write(chunk)\n",
        "        logger.info(f\"Successfully downloaded {url}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error downloading file from {url}: {e}\")\n",
        "        return False\n",
        "\n",
        "def extract_archive(archive_path, extract_to, delete_after\u003dTrue):\n",
        "    \"\"\"Extract a zip or tar file and optionally delete it after.\"\"\"\n",
        "    try:\n",
        "        logger.info(f\"Extracting {archive_path} to {extract_to}\")\n",
        "        if archive_path.endswith(\u0027.zip\u0027):\n",
        "            with zipfile.ZipFile(archive_path, \u0027r\u0027) as zip_ref:\n",
        "                zip_ref.extractall(extract_to)\n",
        "        elif any(archive_path.endswith(ext) for ext in [\u0027.tar.gz\u0027, \u0027.tgz\u0027, \u0027.tar.bz2\u0027, \u0027.tar.xz\u0027, \u0027.tar\u0027]):\n",
        "            with tarfile.open(archive_path) as tar_ref:\n",
        "                tar_ref.extractall(extract_to)\n",
        "        else:\n",
        "            logger.error(f\"Unsupported archive format: {archive_path}\")\n",
        "            return False\n",
        "\n",
        "        if delete_after:\n",
        "            os.remove(archive_path)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error extracting {archive_path}: {e}\")\n",
        "        return False\n",
        "\n",
        "def compile_from_source(source_dir, build_dir, install_dir, configure_cmd, make_cmd\u003dNone):\n",
        "    \"\"\"Compile and install software from source.\"\"\"\n",
        "    try:\n",
        "        # Create build directory\n",
        "        os.makedirs(build_dir, exist_ok\u003dTrue)\n",
        "\n",
        "        # Configure\n",
        "        logger.info(f\"Running configure command: {configure_cmd}\")\n",
        "        subprocess.run(configure_cmd, cwd\u003dbuild_dir, check\u003dTrue)\n",
        "\n",
        "        # Make and install\n",
        "        if make_cmd:\n",
        "            logger.info(f\"Running make command: {make_cmd}\")\n",
        "            subprocess.run(make_cmd, cwd\u003dbuild_dir, check\u003dTrue)\n",
        "        else:\n",
        "            # Default make commands\n",
        "            logger.info(\"Running make\")\n",
        "            subprocess.run([\"make\"], cwd\u003dbuild_dir, check\u003dTrue)\n",
        "\n",
        "            logger.info(\"Running make install\")\n",
        "            subprocess.run([\"make\", \"install\"], cwd\u003dbuild_dir, check\u003dTrue)\n",
        "\n",
        "        return True\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        logger.error(f\"Error during compilation: {e}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Unexpected error during compilation: {e}\")\n",
        "        return False\n",
        "\n",
        "def install_build_dependencies():\n",
        "    \"\"\"Try to install build dependencies for compiling software.\"\"\"\n",
        "    system \u003d platform.system().lower()\n",
        "\n",
        "    if system !\u003d \"linux\":\n",
        "        # We only handle Linux build dependencies for now\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        # Check for common Linux package managers\n",
        "        if os.path.exists(\"/etc/debian_version\"):\n",
        "            # Debian/Ubuntu\n",
        "            try:\n",
        "                packages \u003d [\n",
        "                    \"build-essential\", \"cmake\", \"pkg-config\", \"libpng-dev\",\n",
        "                    \"libjpeg-dev\", \"libtiff-dev\", \"libopenjp2-7-dev\",\n",
        "                    \"libfreetype6-dev\", \"libfontconfig1-dev\", \"libcairo2-dev\"\n",
        "                ]\n",
        "                subprocess.run([\"apt-get\", \"update\"], check\u003dTrue)\n",
        "                subprocess.run([\"apt-get\", \"install\", \"-y\"] + packages, check\u003dTrue)\n",
        "                return True\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Failed to install build dependencies with apt: {e}\")\n",
        "\n",
        "        elif os.path.exists(\"/etc/redhat-release\"):\n",
        "            # Red Hat/CentOS/Fedora\n",
        "            try:\n",
        "                packages \u003d [\n",
        "                    \"gcc\", \"gcc-c++\", \"make\", \"cmake\", \"pkgconfig\", \"libpng-devel\",\n",
        "                    \"libjpeg-devel\", \"libtiff-devel\", \"openjpeg2-devel\",\n",
        "                    \"freetype-devel\", \"fontconfig-devel\", \"cairo-devel\"\n",
        "                ]\n",
        "                subprocess.run([\"yum\", \"install\", \"-y\"] + packages, check\u003dTrue)\n",
        "                return True\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Failed to install build dependencies with yum: {e}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Failed to install build dependencies: {e}\")\n",
        "\n",
        "    return False\n",
        "\n",
        "def install_poppler_from_source():\n",
        "    \"\"\"Install poppler by compiling from source.\"\"\"\n",
        "    try:\n",
        "        # Set up directories\n",
        "        base_dir \u003d os.path.join(os.getcwd(), \"poppler_build\")\n",
        "        source_dir \u003d os.path.join(base_dir, \"source\")\n",
        "        build_dir \u003d os.path.join(base_dir, \"build\")\n",
        "        install_dir \u003d os.path.join(os.getcwd(), \"poppler\")\n",
        "\n",
        "        os.makedirs(source_dir, exist_ok\u003dTrue)\n",
        "        os.makedirs(build_dir, exist_ok\u003dTrue)\n",
        "        os.makedirs(install_dir, exist_ok\u003dTrue)\n",
        "\n",
        "        # Download poppler source\n",
        "        poppler_version \u003d \"23.08.0\"\n",
        "        poppler_url \u003d f\"https://poppler.freedesktop.org/poppler-{poppler_version}.tar.xz\"\n",
        "        archive_path \u003d os.path.join(tempfile.gettempdir(), \"poppler.tar.xz\")\n",
        "\n",
        "        if not download_file(poppler_url, archive_path):\n",
        "            # Try alternative mirror\n",
        "            poppler_url \u003d f\"https://sourceforge.net/projects/poppler/files/poppler/{poppler_version}/poppler-{poppler_version}.tar.xz\"\n",
        "            if not download_file(poppler_url, archive_path):\n",
        "                logger.error(\"Failed to download poppler source\")\n",
        "                return False\n",
        "\n",
        "        # Extract source\n",
        "        if not extract_archive(archive_path, source_dir):\n",
        "            return False\n",
        "\n",
        "        # Find the extracted directory\n",
        "        extracted_dirs \u003d [d for d in os.listdir(source_dir) if os.path.isdir(os.path.join(source_dir, d))]\n",
        "        if not extracted_dirs:\n",
        "            logger.error(\"Could not find extracted poppler source directory\")\n",
        "            return False\n",
        "\n",
        "        poppler_source_dir \u003d os.path.join(source_dir, extracted_dirs[0])\n",
        "\n",
        "        # Configure and build\n",
        "        configure_cmd \u003d [\n",
        "            \"cmake\",\n",
        "            poppler_source_dir,\n",
        "            f\"-DCMAKE_INSTALL_PREFIX\u003d{install_dir}\",\n",
        "            \"-DCMAKE_BUILD_TYPE\u003dRelease\",\n",
        "            \"-DENABLE_UNSTABLE_API_ABI_HEADERS\u003dON\"\n",
        "        ]\n",
        "\n",
        "        if compile_from_source(poppler_source_dir, build_dir, install_dir, configure_cmd):\n",
        "            # Add to PATH\n",
        "            bin_dir \u003d os.path.join(install_dir, \"bin\")\n",
        "            logger.info(f\"Adding poppler bin directory to PATH: {bin_dir}\")\n",
        "            os.environ[\"PATH\"] \u003d bin_dir + os.pathsep + os.environ[\"PATH\"]\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error installing poppler from source: {e}\")\n",
        "        return False\n",
        "\n",
        "def install_leptonica_from_source():\n",
        "    \"\"\"Install Leptonica library (dependency for Tesseract) from source.\"\"\"\n",
        "    try:\n",
        "        # Set up directories\n",
        "        base_dir \u003d os.path.join(os.getcwd(), \"leptonica_build\")\n",
        "        source_dir \u003d os.path.join(base_dir, \"source\")\n",
        "        build_dir \u003d os.path.join(base_dir, \"build\")\n",
        "        install_dir \u003d os.path.join(os.getcwd(), \"leptonica\")\n",
        "\n",
        "        os.makedirs(source_dir, exist_ok\u003dTrue)\n",
        "        os.makedirs(build_dir, exist_ok\u003dTrue)\n",
        "        os.makedirs(install_dir, exist_ok\u003dTrue)\n",
        "\n",
        "        # Download leptonica source\n",
        "        leptonica_version \u003d \"1.83.0\"\n",
        "        leptonica_url \u003d f\"https://github.com/DanBloomberg/leptonica/releases/download/{leptonica_version}/leptonica-{leptonica_version}.tar.gz\"\n",
        "        archive_path \u003d os.path.join(tempfile.gettempdir(), \"leptonica.tar.gz\")\n",
        "\n",
        "        if not download_file(leptonica_url, archive_path):\n",
        "            logger.error(\"Failed to download leptonica source\")\n",
        "            return False\n",
        "\n",
        "        # Extract source\n",
        "        if not extract_archive(archive_path, source_dir):\n",
        "            return False\n",
        "\n",
        "        # Find the extracted directory\n",
        "        extracted_dirs \u003d [d for d in os.listdir(source_dir) if os.path.isdir(os.path.join(source_dir, d))]\n",
        "        if not extracted_dirs:\n",
        "            logger.error(\"Could not find extracted leptonica source directory\")\n",
        "            return False\n",
        "\n",
        "        leptonica_source_dir \u003d os.path.join(source_dir, extracted_dirs[0])\n",
        "\n",
        "        # Configure and build\n",
        "        os.chdir(leptonica_source_dir)\n",
        "        configure_cmd \u003d [\n",
        "            \"./configure\",\n",
        "            f\"--prefix\u003d{install_dir}\"\n",
        "        ]\n",
        "\n",
        "        if compile_from_source(leptonica_source_dir, leptonica_source_dir, install_dir, configure_cmd):\n",
        "            # Set environment variables for Tesseract to find Leptonica\n",
        "            os.environ[\"PKG_CONFIG_PATH\"] \u003d f\"{install_dir}/lib/pkgconfig:{os.environ.get(\u0027PKG_CONFIG_PATH\u0027, \u0027\u0027)}\"\n",
        "            os.environ[\"LD_LIBRARY_PATH\"] \u003d f\"{install_dir}/lib:{os.environ.get(\u0027LD_LIBRARY_PATH\u0027, \u0027\u0027)}\"\n",
        "            os.environ[\"LIBLEPT_HEADERSDIR\"] \u003d f\"{install_dir}/include\"\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error installing leptonica from source: {e}\")\n",
        "        return False\n",
        "\n",
        "def install_tesseract_from_source():\n",
        "    \"\"\"Install Tesseract OCR by compiling from source.\"\"\"\n",
        "    try:\n",
        "        # First install Leptonica (dependency)\n",
        "        if not install_leptonica_from_source():\n",
        "            logger.warning(\"Failed to install Leptonica, Tesseract installation may fail\")\n",
        "\n",
        "        # Set up directories\n",
        "        base_dir \u003d os.path.join(os.getcwd(), \"tesseract_build\")\n",
        "        source_dir \u003d os.path.join(base_dir, \"source\")\n",
        "        build_dir \u003d os.path.join(base_dir, \"build\")\n",
        "        install_dir \u003d os.path.join(os.getcwd(), \"tesseract\")\n",
        "\n",
        "        os.makedirs(source_dir, exist_ok\u003dTrue)\n",
        "        os.makedirs(build_dir, exist_ok\u003dTrue)\n",
        "        os.makedirs(install_dir, exist_ok\u003dTrue)\n",
        "\n",
        "        # Download tesseract source\n",
        "        tesseract_version \u003d \"5.3.2\"\n",
        "        tesseract_url \u003d f\"https://github.com/tesseract-ocr/tesseract/archive/refs/tags/{tesseract_version}.tar.gz\"\n",
        "        archive_path \u003d os.path.join(tempfile.gettempdir(), \"tesseract.tar.gz\")\n",
        "\n",
        "        if not download_file(tesseract_url, archive_path):\n",
        "            logger.error(\"Failed to download tesseract source\")\n",
        "            return False\n",
        "\n",
        "        # Extract source\n",
        "        if not extract_archive(archive_path, source_dir):\n",
        "            return False\n",
        "\n",
        "        # Find the extracted directory\n",
        "        extracted_dirs \u003d [d for d in os.listdir(source_dir) if os.path.isdir(os.path.join(source_dir, d))]\n",
        "        if not extracted_dirs:\n",
        "            logger.error(\"Could not find extracted tesseract source directory\")\n",
        "            return False\n",
        "\n",
        "        tesseract_source_dir \u003d os.path.join(source_dir, extracted_dirs[0])\n",
        "\n",
        "        # Generate configure script (needed for source tarball)\n",
        "        try:\n",
        "            logger.info(\"Running ./autogen.sh\")\n",
        "            subprocess.run([\"./autogen.sh\"], cwd\u003dtesseract_source_dir, check\u003dTrue)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error running autogen.sh: {e}\")\n",
        "            # Continue anyway, it might work with the existing configure script\n",
        "\n",
        "        # Configure and build\n",
        "        leptonica_dir \u003d os.path.join(os.getcwd(), \"leptonica\")\n",
        "        configure_cmd \u003d [\n",
        "            \"./configure\",\n",
        "            f\"--prefix\u003d{install_dir}\",\n",
        "            f\"--with-extra-libraries\u003d{leptonica_dir}/lib\",\n",
        "            f\"--with-extra-includes\u003d{leptonica_dir}/include\"\n",
        "        ]\n",
        "\n",
        "        if compile_from_source(tesseract_source_dir, tesseract_source_dir, install_dir, configure_cmd):\n",
        "            # Download language data\n",
        "            lang_dir \u003d os.path.join(install_dir, \"share\", \"tessdata\")\n",
        "            os.makedirs(lang_dir, exist_ok\u003dTrue)\n",
        "\n",
        "            # Download English language data\n",
        "            eng_url \u003d \"https://github.com/tesseract-ocr/tessdata/raw/main/eng.traineddata\"\n",
        "            eng_path \u003d os.path.join(lang_dir, \"eng.traineddata\")\n",
        "            download_file(eng_url, eng_path)\n",
        "\n",
        "            # Add to PATH and set TESSDATA_PREFIX\n",
        "            bin_dir \u003d os.path.join(install_dir, \"bin\")\n",
        "            logger.info(f\"Adding tesseract bin directory to PATH: {bin_dir}\")\n",
        "            os.environ[\"PATH\"] \u003d bin_dir + os.pathsep + os.environ[\"PATH\"]\n",
        "            os.environ[\"TESSDATA_PREFIX\"] \u003d lang_dir\n",
        "\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error installing tesseract from source: {e}\")\n",
        "        return False\n",
        "\n",
        "def get_portable_poppler():\n",
        "    \"\"\"Download a portable version of poppler based on platform.\"\"\"\n",
        "    system \u003d platform.system().lower()\n",
        "    poppler_path \u003d os.path.join(os.getcwd(), \"poppler\")\n",
        "    os.makedirs(poppler_path, exist_ok\u003dTrue)\n",
        "\n",
        "    if system \u003d\u003d \"windows\":\n",
        "        # Windows portable version\n",
        "        poppler_url \u003d \"https://github.com/oschwartz10612/poppler-windows/releases/download/v23.07.0-0/Release-23.07.0-0.zip\"\n",
        "    elif system \u003d\u003d \"linux\":\n",
        "        # Try a more reliable Linux binary build\n",
        "        poppler_url \u003d \"https://github.com/oschwartz10612/poppler-windows/releases/download/v23.07.0-0/Release-23.07.0-0.zip\"\n",
        "    elif system \u003d\u003d \"darwin\":  # macOS\n",
        "        poppler_url \u003d \"https://github.com/oschwartz10612/poppler-windows/releases/download/v23.07.0-0/Release-23.07.0-0.zip\"\n",
        "    else:\n",
        "        logger.warning(f\"Unsupported platform for portable poppler: {system}\")\n",
        "        return False\n",
        "\n",
        "    archive_path \u003d os.path.join(tempfile.gettempdir(), \"poppler_portable.zip\")\n",
        "\n",
        "    if download_file(poppler_url, archive_path) and extract_archive(archive_path, poppler_path):\n",
        "        # Find the bin directory (might be nested)\n",
        "        bin_dir \u003d None\n",
        "        for root, dirs, files in os.walk(poppler_path):\n",
        "            if any(f.startswith(\u0027pdfinfo\u0027) or f.startswith(\u0027pdfinfo.exe\u0027) for f in files):\n",
        "                bin_dir \u003d root\n",
        "                break\n",
        "\n",
        "        # If bin directory not found, try common locations\n",
        "        if not bin_dir:\n",
        "            potential_paths \u003d [\n",
        "                os.path.join(poppler_path, \"Library\", \"bin\"),\n",
        "                os.path.join(poppler_path, \"bin\"),\n",
        "                os.path.join(poppler_path, \"poppler-23.07.0\", \"Library\", \"bin\")\n",
        "            ]\n",
        "            for path in potential_paths:\n",
        "                if os.path.exists(path):\n",
        "                    bin_dir \u003d path\n",
        "                    break\n",
        "\n",
        "        if bin_dir:\n",
        "            logger.info(f\"Adding poppler bin directory to PATH: {bin_dir}\")\n",
        "            os.environ[\"PATH\"] \u003d bin_dir + os.pathsep + os.environ[\"PATH\"]\n",
        "            return True\n",
        "\n",
        "    logger.warning(\"Failed to install portable poppler\")\n",
        "    return False\n",
        "\n",
        "def get_portable_tesseract():\n",
        "    \"\"\"Download a portable version of tesseract based on platform.\"\"\"\n",
        "    system \u003d platform.system().lower()\n",
        "    tesseract_path \u003d os.path.join(os.getcwd(), \"tesseract\")\n",
        "    os.makedirs(tesseract_path, exist_ok\u003dTrue)\n",
        "\n",
        "    # Only Windows has readily available portable versions\n",
        "    if system \u003d\u003d \"windows\":\n",
        "        tesseract_url \u003d \"https://digi.bib.uni-mannheim.de/tesseract/tesseract-ocr-w64-portable-5.3.1.20230401.zip\"\n",
        "        archive_path \u003d os.path.join(tempfile.gettempdir(), \"tesseract_portable.zip\")\n",
        "\n",
        "        if download_file(tesseract_url, archive_path) and extract_archive(archive_path, tesseract_path):\n",
        "            # Find the bin directory\n",
        "            bin_dir \u003d None\n",
        "            for root, dirs, files in os.walk(tesseract_path):\n",
        "                if \"tesseract.exe\" in files:\n",
        "                    bin_dir \u003d root\n",
        "                    break\n",
        "\n",
        "            if bin_dir:\n",
        "                logger.info(f\"Adding tesseract bin directory to PATH: {bin_dir}\")\n",
        "                os.environ[\"PATH\"] \u003d bin_dir + os.pathsep + os.environ[\"PATH\"]\n",
        "\n",
        "                # Set TESSDATA_PREFIX if tessdata directory exists\n",
        "                for root, dirs, _ in os.walk(tesseract_path):\n",
        "                    if \"tessdata\" in dirs:\n",
        "                        tessdata_dir \u003d os.path.join(root, \"tessdata\")\n",
        "                        logger.info(f\"Setting TESSDATA_PREFIX to {tessdata_dir}\")\n",
        "                        os.environ[\"TESSDATA_PREFIX\"] \u003d tessdata_dir\n",
        "                        break\n",
        "\n",
        "                return True\n",
        "\n",
        "    logger.warning(f\"No portable tesseract available for {system}\")\n",
        "    return False\n",
        "\n",
        "def check_poppler_installed():\n",
        "    \"\"\"Check if poppler is installed and in PATH.\"\"\"\n",
        "    try:\n",
        "        system \u003d platform.system().lower()\n",
        "        check_commands \u003d {\n",
        "            \"windows\": [\"pdfinfo\", \"-v\"],\n",
        "            \"linux\": [\"pdfinfo\", \"-v\"],\n",
        "            \"darwin\": [\"pdfinfo\", \"-v\"]\n",
        "        }\n",
        "\n",
        "        command \u003d check_commands.get(system, [\"pdfinfo\", \"-v\"])\n",
        "\n",
        "        # Try running the command\n",
        "        logger.info(f\"Checking if poppler is installed using command: {\u0027 \u0027.join(command)}\")\n",
        "        result \u003d subprocess.run(\n",
        "            command,\n",
        "            stdout\u003dsubprocess.PIPE,\n",
        "            stderr\u003dsubprocess.PIPE,\n",
        "            text\u003dTrue\n",
        "        )\n",
        "\n",
        "        is_installed \u003d result.returncode \u003d\u003d 0 or \"pdfinfo version\" in (result.stdout + result.stderr)\n",
        "        logger.info(f\"Poppler installed: {is_installed}\")\n",
        "        return is_installed\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Error checking poppler installation: {e}\")\n",
        "        return False\n",
        "\n",
        "def check_tesseract_installed():\n",
        "    \"\"\"Check if tesseract is installed and in PATH.\"\"\"\n",
        "    try:\n",
        "        logger.info(\"Checking if tesseract is installed\")\n",
        "        result \u003d subprocess.run(\n",
        "            [\"tesseract\", \"--version\"],\n",
        "            stdout\u003dsubprocess.PIPE,\n",
        "            stderr\u003dsubprocess.PIPE,\n",
        "            text\u003dTrue\n",
        "        )\n",
        "\n",
        "        is_installed \u003d result.returncode \u003d\u003d 0\n",
        "        logger.info(f\"Tesseract installed: {is_installed}\")\n",
        "        return is_installed\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Error checking tesseract installation: {e}\")\n",
        "        return False\n",
        "\n",
        "def install_poppler():\n",
        "    \"\"\"Install poppler using the best available method.\"\"\"\n",
        "    logger.info(\"Installing poppler...\")\n",
        "\n",
        "    # Try portable version first (easier)\n",
        "    if get_portable_poppler():\n",
        "        return True\n",
        "\n",
        "    # Try compiling from source (harder but more likely to work)\n",
        "    return install_poppler_from_source()\n",
        "\n",
        "def install_tesseract():\n",
        "    \"\"\"Install tesseract using the best available method.\"\"\"\n",
        "    logger.info(\"Installing tesseract OCR...\")\n",
        "\n",
        "    # Try portable version first (easier, but limited platforms)\n",
        "    if get_portable_tesseract():\n",
        "        return True\n",
        "\n",
        "    # Try compiling from source (harder but more likely to work)\n",
        "    return install_tesseract_from_source()\n",
        "\n",
        "def ensure_dependencies_installed():\n",
        "    \"\"\"Ensure all dependencies (pandoc, poppler, tesseract) are installed.\"\"\"\n",
        "    results \u003d {}\n",
        "\n",
        "    # Check for pandoc\n",
        "    try:\n",
        "        import pypandoc\n",
        "        pandoc_installed \u003d pypandoc.get_pandoc_version() is not None\n",
        "        logger.info(f\"Pandoc installed: {pandoc_installed}\")\n",
        "\n",
        "        if not pandoc_installed:\n",
        "            logger.info(\"Downloading pandoc binaries...\")\n",
        "            try:\n",
        "                # Assuming this is from your existing module\n",
        "                pandoc_tmp_directory \u003d os.getcwd()\n",
        "                pypandoc.ensure_pandoc_installed(targetfolder\u003dpandoc_tmp_directory)\n",
        "                pandoc_installed \u003d True\n",
        "                logger.info(\"Pandoc binaries downloaded successfully\")\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Failed to download pandoc binaries: {e}\")\n",
        "\n",
        "        results[\"pandoc\"] \u003d pandoc_installed\n",
        "    except ImportError:\n",
        "        logger.warning(\"pypandoc module not available\")\n",
        "        results[\"pandoc\"] \u003d False\n",
        "\n",
        "    # Check for poppler\n",
        "    poppler_installed \u003d check_poppler_installed()\n",
        "    if not poppler_installed:\n",
        "        poppler_installed \u003d install_poppler()\n",
        "        if poppler_installed:\n",
        "            logger.info(\"Poppler installed successfully\")\n",
        "        else:\n",
        "            logger.warning(\"Failed to install poppler\")\n",
        "\n",
        "    results[\"poppler\"] \u003d poppler_installed\n",
        "\n",
        "    # Check for tesseract\n",
        "    tesseract_installed \u003d check_tesseract_installed()\n",
        "    if not tesseract_installed:\n",
        "        tesseract_installed \u003d install_tesseract()\n",
        "        if tesseract_installed:\n",
        "            logger.info(\"Tesseract OCR installed successfully\")\n",
        "        else:\n",
        "            logger.warning(\"Failed to install tesseract OCR\")\n",
        "\n",
        "    results[\"tesseract\"] \u003d tesseract_installed\n",
        "\n",
        "    return results\n",
        "\n",
        "def set_library_environment_variables(base_dir\u003dNone):\n",
        "    \"\"\"\n",
        "    Set all necessary environment variables to use poppler, leptonica, and tesseract.\n",
        "\n",
        "    Args:\n",
        "        base_dir (str, optional): Base directory where libraries are installed.\n",
        "                                 If None, uses current working directory.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary of set environment variables and their values\n",
        "    \"\"\"\n",
        "    if base_dir is None:\n",
        "        base_dir \u003d os.getcwd()\n",
        "\n",
        "    logger.info(f\"Setting environment variables for installed libraries in {base_dir}\")\n",
        "    env_vars \u003d {}\n",
        "\n",
        "    # Poppler environment variables\n",
        "    poppler_dir \u003d os.path.join(base_dir, \"poppler\")\n",
        "    if os.path.exists(poppler_dir):\n",
        "        # Find bin directory\n",
        "        bin_dir \u003d None\n",
        "        for potential_bin in [\n",
        "            os.path.join(poppler_dir, \"bin\"),\n",
        "            os.path.join(poppler_dir, \"Library\", \"bin\")\n",
        "        ]:\n",
        "            if os.path.exists(potential_bin):\n",
        "                bin_dir \u003d potential_bin\n",
        "                break\n",
        "\n",
        "        # If still not found, search for it\n",
        "        if not bin_dir:\n",
        "            for root, dirs, files in os.walk(poppler_dir):\n",
        "                if any(f.startswith(\u0027pdfinfo\u0027) or f.startswith(\u0027pdfinfo.exe\u0027) for f in files):\n",
        "                    bin_dir \u003d root\n",
        "                    break\n",
        "\n",
        "        if bin_dir:\n",
        "            logger.info(f\"Adding poppler bin directory to PATH: {bin_dir}\")\n",
        "            os.environ[\"PATH\"] \u003d bin_dir + os.pathsep + os.environ.get(\"PATH\", \"\")\n",
        "            env_vars[\"POPPLER_PATH\"] \u003d bin_dir\n",
        "\n",
        "    # Leptonica environment variables\n",
        "    leptonica_dir \u003d os.path.join(base_dir, \"leptonica\")\n",
        "    if os.path.exists(leptonica_dir):\n",
        "        lib_dir \u003d os.path.join(leptonica_dir, \"lib\")\n",
        "        include_dir \u003d os.path.join(leptonica_dir, \"include\")\n",
        "        pkgconfig_dir \u003d os.path.join(lib_dir, \"pkgconfig\")\n",
        "\n",
        "        if os.path.exists(lib_dir):\n",
        "            logger.info(f\"Setting Leptonica library paths: {lib_dir}\")\n",
        "            os.environ[\"LD_LIBRARY_PATH\"] \u003d lib_dir + os.pathsep + os.environ.get(\"LD_LIBRARY_PATH\", \"\")\n",
        "            env_vars[\"LD_LIBRARY_PATH\"] \u003d os.environ[\"LD_LIBRARY_PATH\"]\n",
        "\n",
        "            if os.path.exists(pkgconfig_dir):\n",
        "                os.environ[\"PKG_CONFIG_PATH\"] \u003d pkgconfig_dir + os.pathsep + os.environ.get(\"PKG_CONFIG_PATH\", \"\")\n",
        "                env_vars[\"PKG_CONFIG_PATH\"] \u003d os.environ[\"PKG_CONFIG_PATH\"]\n",
        "\n",
        "        if os.path.exists(include_dir):\n",
        "            os.environ[\"LIBLEPT_HEADERSDIR\"] \u003d include_dir\n",
        "            env_vars[\"LIBLEPT_HEADERSDIR\"] \u003d include_dir\n",
        "\n",
        "    # Tesseract environment variables\n",
        "    tesseract_dir \u003d os.path.join(base_dir, \"tesseract\")\n",
        "    if os.path.exists(tesseract_dir):\n",
        "        # Find bin directory\n",
        "        bin_dir \u003d os.path.join(tesseract_dir, \"bin\")\n",
        "\n",
        "        # If standard bin doesn\u0027t exist, search for it\n",
        "        if not os.path.exists(bin_dir):\n",
        "            for root, dirs, files in os.walk(tesseract_dir):\n",
        "                if \"tesseract\" in files or \"tesseract.exe\" in files:\n",
        "                    bin_dir \u003d root\n",
        "                    break\n",
        "\n",
        "        if os.path.exists(bin_dir):\n",
        "            logger.info(f\"Adding tesseract bin directory to PATH: {bin_dir}\")\n",
        "            os.environ[\"PATH\"] \u003d bin_dir + os.pathsep + os.environ[\"PATH\"]\n",
        "            env_vars[\"TESSERACT_PATH\"] \u003d bin_dir\n",
        "\n",
        "        # Find tessdata directory\n",
        "        tessdata_dir \u003d None\n",
        "        potential_paths \u003d [\n",
        "            os.path.join(tesseract_dir, \"share\", \"tessdata\"),\n",
        "            os.path.join(tesseract_dir, \"tessdata\")\n",
        "        ]\n",
        "\n",
        "        for path in potential_paths:\n",
        "            if os.path.exists(path):\n",
        "                tessdata_dir \u003d path\n",
        "                break\n",
        "\n",
        "        # If still not found, search for it\n",
        "        if not tessdata_dir:\n",
        "            for root, dirs, _ in os.walk(tesseract_dir):\n",
        "                if \"tessdata\" in dirs:\n",
        "                    tessdata_dir \u003d os.path.join(root, \"tessdata\")\n",
        "                    break\n",
        "\n",
        "        if tessdata_dir:\n",
        "            logger.info(f\"Setting TESSDATA_PREFIX to {tessdata_dir}\")\n",
        "            os.environ[\"TESSDATA_PREFIX\"] \u003d tessdata_dir\n",
        "            env_vars[\"TESSDATA_PREFIX\"] \u003d tessdata_dir\n",
        "\n",
        "    # Print summary of set environment variables\n",
        "    logger.info(\"Environment variables set:\")\n",
        "    for var, value in env_vars.items():\n",
        "        logger.info(f\"  {var} \u003d {value}\")\n",
        "\n",
        "    return env_vars\n",
        "\n",
        "# Example usage:\n",
        "# env_vars \u003d set_library_environment_variables()\n",
        "# Example usage\n",
        "# if __name__ \u003d\u003d \"__main__\":\n",
        "#     logging.basicConfig(level\u003dlogging.INFO)\n",
        "#     results \u003d ensure_dependencies_installed()\n",
        "#     print(f\"Dependencies installed: {results}\")\n",
        "\n",
        "import os\n",
        "import logging\n",
        "from io import BytesIO\n",
        "import base64\n",
        "import pandas as pd\n",
        "import concurrent.futures\n",
        "from typing import Dict, List, Any, Optional, Tuple\n",
        "\n",
        "import dataiku\n",
        "from unstructured.partition.auto import partition\n",
        "from unstructured.chunking.title import chunk_by_title\n",
        "from unstructured.staging.base import elements_to_json\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level\u003dlogging.INFO, format\u003d\u0027%(asctime)s - %(levelname)s - %(message)s\u0027)\n",
        "logger \u003d logging.getLogger(__name__)\n",
        "\n",
        "class BaseDigitizer:\n",
        "    \"\"\"\n",
        "    Base class for document digitization that handles file reading from a Dataiku Folder.\n",
        "    \"\"\"\n",
        "    def __init__(self, folder_id: str):\n",
        "        \"\"\"\n",
        "        Initialize the digitizer with a Dataiku folder.\n",
        "\n",
        "        Args:\n",
        "            folder_id: The ID of the Dataiku folder containing the documents\n",
        "        \"\"\"\n",
        "        self.data_source \u003d dataiku.Folder(folder_id)\n",
        "\n",
        "    def get_file_data(self, file_path: str) -\u003e bytes:\n",
        "        \"\"\"\n",
        "        Reads file data from the Dataiku Folder.\n",
        "\n",
        "        Args:\n",
        "            file_path: Path to the file within the Dataiku folder\n",
        "\n",
        "        Returns:\n",
        "            bytes: The file content as bytes\n",
        "        \"\"\"\n",
        "        with self.data_source.get_download_stream(file_path) as f:\n",
        "            return f.read()\n",
        "\n",
        "class UnstructuredDigitizer(BaseDigitizer):\n",
        "    \"\"\"\n",
        "    Digitizes documents using the Unstructured library to extract content.\n",
        "    \"\"\"\n",
        "    def __init__(self, folder_id: str, llm_model_id: str \u003d \"custom:iliad-plugin-conn-prod:Claude_3_5_Sonnet\"):\n",
        "        \"\"\"\n",
        "        Initialize the Unstructured digitizer.\n",
        "\n",
        "        Args:\n",
        "            folder_id: The ID of the Dataiku folder containing the documents\n",
        "            llm_model_id: The ID of the LLM model to use for image description\n",
        "        \"\"\"\n",
        "        super().__init__(folder_id)\n",
        "        self.llm_model_id \u003d llm_model_id\n",
        "\n",
        "        # Initialize Dataiku LLM client\n",
        "        self.client \u003d dataiku.api_client()\n",
        "        self.project \u003d self.client.get_default_project()\n",
        "        try:\n",
        "            self.llm_model \u003d self.project.get_llm(self.llm_model_id)\n",
        "            logger.info(f\"Successfully initialized LLM model: {self.llm_model_id}\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Failed to initialize LLM model: {e}\")\n",
        "            self.llm_model \u003d None\n",
        "\n",
        "    def extract_content(self, file_path: str) -\u003e Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Extract all content from a document using Unstructured.\n",
        "\n",
        "        Args:\n",
        "            file_path: Path to the file within the Dataiku folder\n",
        "\n",
        "        Returns:\n",
        "            Dict with keys:\n",
        "                - text: Extracted text content\n",
        "                - tables: Extracted tables\n",
        "                - images: List of extracted images with descriptions\n",
        "                - metadata: Document metadata\n",
        "        \"\"\"\n",
        "        try:\n",
        "            file_data \u003d self.get_file_data(file_path)\n",
        "            file_name \u003d os.path.basename(file_path)\n",
        "            file_ext \u003d os.path.splitext(file_name)[1].lower()\n",
        "\n",
        "            # Handle Excel files separately\n",
        "            if file_ext in [\u0027.xlsx\u0027, \u0027.xls\u0027]:\n",
        "                #from excel_extractor import handle_excel_file  # Import the module you added\n",
        "                return handle_excel_file(file_data, file_path)\n",
        "\n",
        "            # Create a BytesIO object from the file data\n",
        "            file_stream \u003d BytesIO(file_data)\n",
        "\n",
        "            # Extract elements using Unstructured\n",
        "            elements \u003d partition(\n",
        "                file\u003dfile_stream,\n",
        "                file_filename\u003dfile_name,\n",
        "                strategy\u003d\"auto\",\n",
        "                include_metadata\u003dTrue,\n",
        "                extract_images_in_pdf\u003dTrue,\n",
        "                extract_image_block_types\u003d[\"Image\"],\n",
        "                extract_tables\u003dTrue\n",
        "            )\n",
        "\n",
        "            # Process extracted elements\n",
        "            text_elements \u003d []\n",
        "            table_elements \u003d []\n",
        "            image_elements \u003d []\n",
        "\n",
        "            for element in elements:\n",
        "                element_type \u003d element.category\n",
        "\n",
        "                if element_type \u003d\u003d \"Table\":\n",
        "                    table_elements.append(element)\n",
        "                elif element_type \u003d\u003d \"Image\":\n",
        "                    image_elements.append(element)\n",
        "                elif element_type in [\"Title\", \"NarrativeText\", \"Text\", \"ListItem\", \"Header\"]:\n",
        "                    text_elements.append(element)\n",
        "\n",
        "            # Process text content\n",
        "            text_content \u003d \"\\n\".join([element.text for element in text_elements])\n",
        "\n",
        "            # Process tables\n",
        "            tables \u003d []\n",
        "            for table_element in table_elements:\n",
        "                tables.append({\n",
        "                    \"text\": table_element.text,\n",
        "                    \"metadata\": table_element.metadata.to_dict() if hasattr(table_element, \"metadata\") else {}\n",
        "                })\n",
        "\n",
        "            # Process images and get descriptions using Dataiku LLM\n",
        "            images \u003d []\n",
        "            for image_element in image_elements:\n",
        "                image_data \u003d {}\n",
        "                if hasattr(image_element, \"metadata\") and hasattr(image_element.metadata, \"image_base64\"):\n",
        "                    image_data[\"image_base64\"] \u003d image_element.metadata.image_base64\n",
        "                    if self.llm_model:\n",
        "                        image_data[\"description\"] \u003d self._describe_image(image_element.metadata.image_base64)\n",
        "                    else:\n",
        "                        image_data[\"description\"] \u003d \"Image description not available (LLM model not configured)\"\n",
        "                images.append(image_data)\n",
        "\n",
        "            # Get document metadata\n",
        "            metadata \u003d {\n",
        "                \"file_name\": file_name,\n",
        "                \"file_path\": file_path,\n",
        "                \"file_type\": file_ext,\n",
        "                \"page_count\": len(set([e.metadata.page_number for e in elements if hasattr(e, \"metadata\") and hasattr(e.metadata, \"page_number\")])),\n",
        "            }\n",
        "\n",
        "            # Create structured output\n",
        "            result \u003d {\n",
        "                \"text\": text_content,\n",
        "                \"tables\": tables,\n",
        "                \"images\": images,\n",
        "                \"metadata\": metadata\n",
        "            }\n",
        "\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error extracting content from {file_path}: {e}\")\n",
        "            return {\n",
        "                \"text\": f\"Error extracting content: {str(e)}\",\n",
        "                \"tables\": [],\n",
        "                \"images\": [],\n",
        "                \"metadata\": {\"file_name\": os.path.basename(file_path), \"file_path\": file_path, \"error\": str(e)}\n",
        "            }\n",
        "\n",
        "    def _describe_image(self, image_base64: str) -\u003e str:\n",
        "        \"\"\"\n",
        "        Generate a description for an image using Dataiku\u0027s LLM integration.\n",
        "\n",
        "        Args:\n",
        "            image_base64: Base64 encoded image\n",
        "\n",
        "        Returns:\n",
        "            str: Description of the image\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if not self.llm_model:\n",
        "                return \"Image description not available (LLM model not configured)\"\n",
        "\n",
        "            # Create a completion request\n",
        "            completion \u003d self.llm_model.new_completion()\n",
        "            mp_message \u003d completion.new_multipart_message()\n",
        "\n",
        "            # Add text instructions and image data\n",
        "            mp_message.with_text(\"Please provide a detailed description of this image, including all visible text, elements, and context.\")\n",
        "            mp_message.with_text(f\"Here is the image in base64 format:\\n{image_base64}\")\n",
        "            mp_message.add()\n",
        "\n",
        "            # Execute the completion request\n",
        "            logger.info(\"Executing LLM request for image description...\")\n",
        "            resp \u003d completion.execute()\n",
        "\n",
        "            # Extract response text\n",
        "            if resp.success and hasattr(resp, \"text\"):\n",
        "                return resp.text\n",
        "            else:\n",
        "                logger.warning(f\"LLM request failed or unexpected response format: {resp}\")\n",
        "                return \"Unable to generate image description\"\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error describing image: {e}\")\n",
        "            return f\"Error generating image description: {str(e)}\"\n",
        "\n",
        "    def combine_extracted_data(self, extracted_data: Dict[str, Any]) -\u003e str:\n",
        "        \"\"\"\n",
        "        Combine all extracted data into a single text column.\n",
        "\n",
        "        Args:\n",
        "            extracted_data: Dictionary containing text, tables, and images\n",
        "\n",
        "        Returns:\n",
        "            str: Combined extracted data as a single text string\n",
        "        \"\"\"\n",
        "        combined \u003d []\n",
        "\n",
        "        # Add metadata\n",
        "        metadata \u003d extracted_data.get(\"metadata\", {})\n",
        "        combined.append(f\"DOCUMENT METADATA:\")\n",
        "        for key, value in metadata.items():\n",
        "            combined.append(f\"{key}: {value}\")\n",
        "        combined.append(\"\\n\")\n",
        "\n",
        "        # Add text content\n",
        "        text_content \u003d extracted_data.get(\"text\", \"\")\n",
        "        if text_content:\n",
        "            combined.append(\"TEXT CONTENT:\")\n",
        "            combined.append(text_content)\n",
        "            combined.append(\"\\n\")\n",
        "\n",
        "        # Add tables\n",
        "        tables \u003d extracted_data.get(\"tables\", [])\n",
        "        if tables:\n",
        "            combined.append(\"TABLE CONTENT:\")\n",
        "            for i, table in enumerate(tables):\n",
        "                combined.append(f\"Table {i+1}:\")\n",
        "                combined.append(table.get(\"text\", \"\"))\n",
        "                combined.append(\"\")\n",
        "            combined.append(\"\\n\")\n",
        "\n",
        "        # Add images with descriptions\n",
        "        images \u003d extracted_data.get(\"images\", [])\n",
        "        if images:\n",
        "            combined.append(\"IMAGE CONTENT:\")\n",
        "            for i, image in enumerate(images):\n",
        "                combined.append(f\"Image {i+1} Description:\")\n",
        "                combined.append(image.get(\"description\", \"No description available\"))\n",
        "                combined.append(\"\")\n",
        "\n",
        "        return \"\\n\".join(combined)\n",
        "\n",
        "\n",
        "class DocumentProcessor:\n",
        "    \"\"\"\n",
        "    Processes documents in a folder by extracting content using the UnstructuredDigitizer.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_folder_id: str, llm_model_id: str \u003d \"custom:iliad-plugin-conn-prod:Claude_3_5_Sonnet\",\n",
        "                 max_workers: int \u003d 10):\n",
        "        \"\"\"\n",
        "        Initialize the document processor.\n",
        "\n",
        "        Args:\n",
        "            input_folder_id: The ID of the Dataiku folder containing the documents\n",
        "            llm_model_id: The ID of the LLM model to use for image description\n",
        "            max_workers: Maximum number of worker threads for parallel processing\n",
        "        \"\"\"\n",
        "        self.input_folder_id \u003d input_folder_id\n",
        "        self.digitizer \u003d UnstructuredDigitizer(input_folder_id, llm_model_id)\n",
        "        self.max_workers \u003d max_workers or os.cpu_count()\n",
        "        logger.info(f\"Initialized DocumentProcessor with {self.max_workers} workers\")\n",
        "\n",
        "    def process_file(self, file_path: str) -\u003e Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Process a single document file.\n",
        "\n",
        "        Args:\n",
        "            file_path: Path to the file within the Dataiku folder\n",
        "\n",
        "        Returns:\n",
        "            Dict: Processed document data\n",
        "        \"\"\"\n",
        "        try:\n",
        "            file_name \u003d os.path.basename(file_path)\n",
        "            logger.info(f\"Processing file: {file_name}\")\n",
        "\n",
        "            # Extract content using the digitizer\n",
        "            extracted_data \u003d self.digitizer.extract_content(file_path)\n",
        "\n",
        "            # Combine all extracted data into a single text field\n",
        "            combined_data \u003d self.digitizer.combine_extracted_data(extracted_data)\n",
        "\n",
        "            # Prepare result\n",
        "            result \u003d {\n",
        "                \"file_name\": file_name,\n",
        "                \"file_path\": file_path,\n",
        "                \"extracted_data\": combined_data,\n",
        "                \"metadata\": str(extracted_data.get(\"metadata\", {})),\n",
        "                \"processing_status\": \"success\"\n",
        "            }\n",
        "\n",
        "            logger.info(f\"Completed processing file: {file_name}\")\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing file {file_path}: {e}\")\n",
        "            return {\n",
        "                \"file_name\": os.path.basename(file_path),\n",
        "                \"file_path\": file_path,\n",
        "                \"extracted_data\": f\"Error processing file: {str(e)}\",\n",
        "                \"metadata\": \"{}\",\n",
        "                \"processing_status\": \"error\"\n",
        "            }\n",
        "\n",
        "    def process_all_files(self, file_list: Optional[List[str]] \u003d None) -\u003e pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Process all files in the input folder in parallel.\n",
        "\n",
        "        Args:\n",
        "            file_list: List of file paths to process. If None, all files in the folder are processed.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: DataFrame containing the extracted data\n",
        "        \"\"\"\n",
        "        # If no file list is provided, get all files from the folder\n",
        "        if file_list is None:\n",
        "            data_source \u003d dataiku.Folder(self.input_folder_id)\n",
        "            file_list \u003d data_source.list_paths_in_partition()\n",
        "\n",
        "        logger.info(f\"Starting parallel processing of {len(file_list)} files with {self.max_workers} workers\")\n",
        "        processed_data \u003d []\n",
        "\n",
        "        # Use ThreadPoolExecutor for parallel processing\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers\u003dself.max_workers) as executor:\n",
        "            # Submit all file processing tasks\n",
        "            future_to_file \u003d {executor.submit(self.process_file, file_path): file_path\n",
        "                             for file_path in file_list}\n",
        "\n",
        "            # Process results as they complete\n",
        "            for future in concurrent.futures.as_completed(future_to_file):\n",
        "                file_path \u003d future_to_file[future]\n",
        "                try:\n",
        "                    data \u003d future.result()\n",
        "                    processed_data.append(data)\n",
        "                    logger.info(f\"Added results for {os.path.basename(file_path)}\")\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Exception processing {file_path}: {e}\")\n",
        "                    # Add error information to the results\n",
        "                    processed_data.append({\n",
        "                        \"file_name\": os.path.basename(file_path),\n",
        "                        \"file_path\": file_path,\n",
        "                        \"extracted_data\": f\"Error in parallel processing: {str(e)}\",\n",
        "                        \"metadata\": \"{}\",\n",
        "                        \"processing_status\": \"error\"\n",
        "                    })\n",
        "\n",
        "        logger.info(f\"Completed processing all {len(file_list)} files\")\n",
        "        return pd.DataFrame(processed_data)\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to run the document digitization pipeline.\n",
        "    Example usage in a Dataiku recipe.\n",
        "    \"\"\"\n",
        "    logging.basicConfig(level\u003dlogging.INFO)\n",
        "    results \u003d ensure_dependencies_installed()\n",
        "    print(f\"Dependencies installed: {results}\")\n",
        "    set_library_environment_variables()\n",
        "    # Get input and output datasets from Dataiku\n",
        "    input_folder \u003d \"Input\" #dataiku.get_custom_variables().get(\"input_folder\", \"input_documents\")\n",
        "    output_dataset \u003d \"output_test\" #dataiku.get_custom_variables().get(\"output_dataset\", \"extracted_document_data\")\n",
        "\n",
        "    # Get LLM model ID from custom variables or use default\n",
        "    llm_model_id \u003d \"custom:iliad-plugin-conn-prod:Claude_3_5_Sonnet\" #dataiku.get_custom_variables().get(\"llm_model_id\", \"custom:iliad-plugin-conn-prod:Claude_3_5_Sonnet\")\n",
        "\n",
        "    # Configure parallel processing\n",
        "    max_workers \u003d 10 #int(dataiku.get_custom_variables().get(\"max_workers\", 10))\n",
        "\n",
        "    # Initialize the document processor\n",
        "    processor \u003d DocumentProcessor(\n",
        "        input_folder_id\u003dinput_folder,\n",
        "        llm_model_id\u003dllm_model_id,\n",
        "        max_workers\u003dmax_workers\n",
        "    )\n",
        "\n",
        "    # Process all documents\n",
        "    results_df \u003d processor.process_all_files()\n",
        "    print(results_df)\n",
        "    # Write results to the output dataset\n",
        "    output \u003d dataiku.Dataset(output_dataset)\n",
        "    output.write_with_schema(results_df)\n",
        "\n",
        "    logger.info(f\"Document digitization pipeline completed. Processed {len(results_df)} files.\")\n",
        "\n",
        "\n",
        "if __name__ \u003d\u003d \"__main__\":\n",
        "    main()"
      ],
      "outputs": []
    }
  ]
}