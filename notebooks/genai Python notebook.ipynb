{
  "metadata": {
    "kernelspec": {
      "name": "py-dku-venv-cdl-p-diku-psbts-py39-llm-env",
      "display_name": "Python (env cdl-p-diku-psbts-py39-llm-env)",
      "language": "python"
    },
    "hide_input": false,
    "language_info": {
      "name": "python",
      "version": "3.9.20",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "createdOn": 1743153977260,
    "modifiedBy": "tomarsx1",
    "customFields": {},
    "creator": "singhax113",
    "tags": []
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%pylab inline"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\nfrom dataiku import pandasutils as pdu\nimport pandas as pd"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example: load a DSS dataset as a Pandas dataframe\nmydataset \u003d dataiku.Dataset(\"data_chunked\")\nmydataset_df \u003d mydataset.get_dataframe()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\nclient \u003d dataiku.api_client()\nproject \u003d client.get_default_project()\nllm_list \u003d project.list_llms()\nfor llm in llm_list:\n    print(f\"- {llm.description} (id: {llm.id})\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "connection_name \u003d \"iliad-plugin-conn-prod\" \nconnection \u003d client.get_connection(connection_name)\nconnection_info \u003d connection.get_info()\nconnection_params \u003d connection_info[\"params\"]\nmodels \u003d connection_params[\u0027models\u0027]\nfor model in models:\n    print(f\"{model[\u0027capability\u0027]} {model} \\n\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true
      },
      "source": [
        "# Query on image Multipart message"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "code_folding": [
          22
        ],
        "hidden": true
      },
      "source": [
        "import dataiku\nimport os\nimport base64\nfrom PIL import Image\nfrom io import BytesIO\nimport matplotlib.pyplot as plt\n\n# Create a handle for the LLM model\nclient \u003d dataiku.api_client()\nproject \u003d client.get_default_project()\nLLM_MODEL_ID \u003d \"custom:iliad-plugin-conn-prod:Claude_3_5_Sonnet\"\n# LLM_MODEL_ID \u003d \"custom:iliad-plugin-conn-prod:stable-diffusion-3-large\"\n# LLM_MODEL_ID \u003d \"custom:iliad-plugin-conn-prod:gpt-4o\"\nllm_model \u003d project.get_llm(LLM_MODEL_ID)\n\n# llm \u003d llm_model.as_langchain_llm()\n# Read image from a Dataiku managed folder\nfolder \u003d dataiku.Folder(\"input_images_extracted_custom\")\nimage_filename \u003d \"Phase 1 Requirements - PSIT Patient Journey AI Model_image1.jpeg\"\n\nimport base64\nfrom IPython.display import Image, display\n\ndef display_base64_image(base64_code):\n    # Decode the base64 string to binary\n    image_data \u003d base64.b64decode(base64_code)\n    # Display the image\n    display(Image(data\u003dimage_data))\n\n\n# Read the image data\nwith folder.get_download_stream(image_filename) as stream:\n    image_data \u003d stream.read()\n\n# Convert image to base64\nimg_base64 \u003d base64.b64encode(image_data).decode(\"utf-8\")\n\ndisplay_base64_image(img_base64)\n\n# --- Step 2: Create and execute the completion request ---\ncompletion \u003d llm_model.new_completion()\nmp_message \u003d completion.new_multipart_message()\nprompt_text \u003d (\n    \"\"\"First convert the base64 image into original image and then Extract all the metrics and text present \n    in the provided image and create a detailed summary of the image \n    even if its blurry i want you to just extract every character present in image.\"\"\"\n)\nmp_message.with_text(prompt_text)\n# mp_message.with_text(f\"Here is the image in base64 format:\\n{img_base64}\") \n\n# adding the image using with_inline_image method.\nmp_message.with_inline_image(image_data)\n# mp_message.with_inline_image(img_base64)\n\n# Add the message to the completion request.\nmp_message.add()\n\n# Execute the completion request\nprint(\"Executing LLM request...\")\nresp \u003d completion.execute()\n\n# Debugging: Print response structure\nprint(\"Raw response object type:\", type(resp))\nprint(\"Success:\", resp.success)\n\n# Extract response text correctly\nif hasattr(resp, \"text\"):\n    print(\"Response from LLM:\", resp.text)\nelse:\n    print(\"Response object does not have \u0027text\u0027. Full response:\", resp)\n\n# Check full JSON response\ntry:\n    response_json \u003d resp.json()\n    print(\"Full JSON response:\", response_json)\nexcept Exception as e:\n    print(\"Error extracting JSON response:\", e)\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        "help(mp_message)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        "# # Initialize the processor\n# folder_id \u003d \"Knowledge_bank\"\n# output_folder_id \u003d \"images_extracted\"\n# processor \u003d FileProcessor(folder_id, output_folder_id)\n\n# # Get list of files\n# folder \u003d dataiku.Folder(folder_id)\n# file_list \u003d folder.list_paths_in_partition()\n\n# # Process files\n# results_df \u003d processor.process_all_files(file_list)\n    "
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        "import dataiku\nimport os\nimport base64\nfrom PIL import Image\nfrom io import BytesIO\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image as IPImage, display\n\n# Create a handle for the LLM model\nclient \u003d dataiku.api_client()\nproject \u003d client.get_default_project()\nLLM_MODEL_ID \u003d \"custom:iliad-plugin-conn-prod:Claude_3_5_Sonnet\"\nllm_model \u003d project.get_llm(LLM_MODEL_ID)\n\n# Read image from a Dataiku managed folder\nfolder \u003d dataiku.Folder(\"input_images_extracted_custom\")\nimage_filename \u003d \"Phase 1 Requirements - PSIT Patient Journey AI Model_image1.jpeg\"\n\n# Read the image data\nwith folder.get_download_stream(image_filename) as stream:\n    image_data \u003d stream.read()\n\n# Convert to base64 for display purposes and for use in the API\nimg_base64 \u003d base64.b64encode(image_data).decode(\"utf-8\")\nprint(f\"Image {image_filename} loaded successfully\")\n\n# Convert to Langchain LLM\nprint(\"Converting to Langchain LLM...\")\nllm \u003d llm_model.as_langchain_llm()\nprint(f\"LLM type: {type(llm)}\")\n\n# Import Langchain components for message handling\nfrom langchain.schema import HumanMessage, SystemMessage\n\n# Create a more specific system message\nsystem_message \u003d SystemMessage(content\u003d\"\"\"You are a precise document analysis assistant specializing in text extraction from images.\nYour task is to extract ALL text content visible in the provided image with 100% accuracy.\n- Include ALL text, even if partially visible, small, or in different orientations\n- Maintain the original structure, formatting, and layout as much as possible\n- Extract all numbers, metrics, and values exactly as they appear\n- If text is in tables, preserve the table structure\n- Be comprehensive and detailed - do not summarize or omit any text\n- If you\u0027re uncertain about any text, indicate this with [unclear text] but make your best guess\n\"\"\")\n\n# Create a more detailed human message\nhuman_message \u003d HumanMessage(\n    content\u003d[\n        {\"type\": \"text\", \"text\": \"\"\"Extract ALL text content from this document image. \nBe extremely thorough and extract every single character, number, heading, and label visible in the image.\nFormat your response to preserve the original document layout as much as possible.\nUse formatting like headings, bullet points, and indentation to maintain the structure.\nDO NOT summarize or generalize - I need the EXACT text content as written in the document.\nThis appears to be a technical requirements document - please extract all requirements, metrics, and technical details completely.\"\"\"},\n        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"}}\n    ]\n)\n\nprint(\"Sending request to Claude via Langchain...\")\ntry:\n    # Send the request with temperature\u003d0 for more precise extraction\n    response \u003d llm.invoke([system_message, human_message], config\u003d{\"temperature\": 0})\n    \n    # Print the response\n    print(\"\\n\u003d\u003d\u003d\u003d\u003d CLAUDE\u0027S ANALYSIS \u003d\u003d\u003d\u003d\u003d\")\n    print(response)  # The response is already a string, no need for .content\n    print(\"\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\\n\")\n    \n    # Save the response to a dataset\n    try:\n        output_dataset \u003d dataiku.Dataset(\"extracted_text_claude\")\n        with output_dataset.get_writer() as writer:\n            writer.write_row_dict({\n                \"filename\": image_filename,\n                \"extracted_text\": response,  # The response is already a string\n                \"extraction_method\": \"Claude with Enhanced Prompting\"\n            })\n        print(\"Results saved to dataset \u0027extracted_text_claude\u0027\")\n    except Exception as e:\n        print(f\"Error saving to dataset: {e}\")\n        \n    # Optionally process multiple images\n    process_all_images \u003d False  # Set to True to process all images in the folder\n    \n    if process_all_images:\n        print(\"\\nProcessing all images in the folder...\")\n        all_files \u003d folder.list_paths_in_partition()\n        image_files \u003d [f for f in all_files if f.lower().endswith((\u0027.png\u0027, \u0027.jpg\u0027, \u0027.jpeg\u0027))]\n        \n        for img_file in image_files:\n            if img_file \u003d\u003d image_filename:\n                continue  # Skip the one we already processed\n                \n            print(f\"Processing {img_file}...\")\n            with folder.get_download_stream(img_file) as stream:\n                img_data \u003d stream.read()\n            \n            img_b64 \u003d base64.b64encode(img_data).decode(\"utf-8\")\n            \n            human_msg \u003d HumanMessage(\n                content\u003d[\n                    {\"type\": \"text\", \"text\": f\"Extract ALL text from this image ({img_file}). Be comprehensive and extract every single character, including numbers, headings, and labels.\"},\n                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_b64}\"}}\n                ]\n            )\n            \n            try:\n                img_response \u003d llm.invoke([system_message, human_msg], config\u003d{\"temperature\": 0})\n                \n                # Save to dataset\n                with output_dataset.get_writer() as writer:\n                    writer.write_row_dict({\n                        \"filename\": img_file,\n                        \"extracted_text\": img_response,  # Already a string\n                        \"extraction_method\": \"Claude with Enhanced Prompting\"\n                    })\n                print(f\"Successfully processed {img_file}\")\n            except Exception as e:\n                print(f\"Error processing {img_file}: {e}\")\n    \nexcept Exception as e:\n    print(f\"Error sending request: {e}\")\n    \nprint(\"\\nProcessing complete!\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        "help(llm)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true
      },
      "source": [
        "Testing - VS"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        "connection_name \u003d \"iliad-plugin-conn-prod\" \nconnection \u003d client.get_connection(connection_name)\nconnection_info \u003d connection.get_info()\nconnection_params \u003d connection_info[\"params\"]\nmodels \u003d connection_params[\u0027models\u0027]\nfor model in models:\n    print(f\"Model ID: {model.get(\u0027id\u0027)}\")\n    print(f\"Capability: {model.get(\u0027capability\u0027)}\")\n    print(f\"Type: {model.get(\u0027type\u0027)}\")\n    print()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        "import dataikuapi\n\nclient \u003d dataikuapi.APINodeClient(\"https://cdl-dku-genai01.commercial-datalake-prod.awscloud.abbvienet.com/\", \"mlopsChatbot\")\n\nresult \u003d client.run_function(\"generate\",\n        user_query \u003d \"What are support programs currently available under each drug brand?\")\nprint(\"Function result: %s\" % result.get(\"response\"))\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        ""
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        "import requests\nimport json\n\n# Replace with your actual Dataiku webapp URL LmdRX0E_qaapi\napi_url \u003d \"https://cdl-dku-desi-p.commercial-datalake-prod.awscloud.abbvienet.com/projects/GENAIPOC/webapps/qa_api\"\n\n# Health check\nhealth_response \u003d requests.get(f\"{api_url}/health\")\nprint(\"Health check:\", health_response.json())\n\n# Query the system\nquery_payload \u003d {\"query\": \"Who to contact for rinvoq reactions?\"}\nquery_response \u003d requests.post(\n    f\"{api_url}/query\",\n    headers\u003d{\"Content-Type\": \"application/json\"},\n    data\u003djson.dumps(query_payload)\n)\nprint(\"Query response:\", query_response.json())"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        "import dataiku\nimport json\nimport traceback\n\n# Import the DataikuQASystem class - if this is a custom class you\u0027ve created\nfrom DataikuQASystem import DataikuQASystem\n\n# Environment settings\nPROJECT_KEY \u003d \"GENAIPOC\"\nWEBAPP_ID \u003d \"LmdRX0E_qaapi\"  # Your original QA API webapp ID\n\n# Hardcoded configuration values for QA system\nKB_ID \u003d \"dV3dIQCo\"  # Your knowledge bank ID\nEMBEDDING_MODEL \u003d \"custom:iliad-plugin-conn-prod:text-embedding-ada-002\"\nLLM_MODEL \u003d \"custom:iliad-plugin-conn-prod:gpt-4o\"\nNUM_DOCS \u003d 5  # Number of documents to retrieve\n\n# Initialize the QA system\ntry:\n    qa_system \u003d DataikuQASystem(\n        kb_id\u003dKB_ID,\n        embedding_model_name\u003dEMBEDDING_MODEL,\n        llm_model_name\u003dLLM_MODEL,\n        k\u003dNUM_DOCS\n    )\n    print(f\"QA System successfully initialized with KB: {KB_ID}\")\n    system_ready \u003d True\nexcept Exception as e:\n    print(f\"Error initializing QA system: {str(e)}\")\n    traceback.print_exc()\n    system_ready \u003d False\n\n# Connect to Dataiku API client using the working pattern\ntry:\n    # Use the client that works in your environment\n    client \u003d dataiku.api_client()\n    project \u003d client.get_project(PROJECT_KEY)\n    print(f\"Successfully connected to project {PROJECT_KEY}\")\nexcept Exception as e:\n    print(f\"Error initializing project: {str(e)}\")\n    traceback.print_exc()\n    exit(1)\n\ntry:\n    webapp \u003d project.get_webapp(WEBAPP_ID)\n    print(f\"Successfully connected to webapp {WEBAPP_ID}\")\nexcept Exception as e:\n    print(f\"Error initializing webapp: {str(e)}\")\n    traceback.print_exc()\n    exit(1)\n\ntry:\n    backend \u003d webapp.get_backend_client()\n    backend.session.headers[\u0027Content-Type\u0027] \u003d \u0027application/json\u0027\n    print(\"Backend client successfully initialized\")\nexcept Exception as e:\n    print(f\"Error initializing backend: {str(e)}\")\n    traceback.print_exc()\n    exit(1)\n\ndef check_health():\n    \"\"\"Check the health of the QA system\"\"\"\n    try:\n        health_response \u003d backend.session.get(backend.base_url + \u0027/health\u0027)\n        print(f\"Health check response: {health_response.text}\")\n        return health_response.text\n    except Exception as e:\n        print(f\"Health check failed: {str(e)}\")\n        traceback.print_exc()\n        return json.dumps({\"status\": \"error\", \"message\": str(e)})\n\ndef query_qa_system(question):\n    \"\"\"Query the QA system with a question\"\"\"\n    try:\n        response \u003d backend.session.post(\n            backend.base_url + \u0027/query\u0027, \n            json\u003d{\u0027message\u0027: question}\n        )\n        print(f\"Query response: {response.text}\")\n        return response.text\n    except Exception as e:\n        print(f\"Query failed: {str(e)}\")\n        traceback.print_exc()\n        return json.dumps({\"status\": \"error\", \"message\": str(e)})\n\n# Example usage\nif __name__ \u003d\u003d \"__main__\":\n    # Test health check\n    print(\"\\n--- Health Check ---\")\n    health_result \u003d check_health()\n    \n    # Test query\n    print(\"\\n--- Test Query ---\")\n    question \u003d \"Who to contact for rinvoq reactions?\"\n    answer \u003d query_qa_system(question)\n    \n    # Try to parse the response\n    try:\n        answer_json \u003d json.loads(answer)\n        if \"response\" in answer_json:\n            print(f\"\\nFormatted Answer: {answer_json[\u0027response\u0027]}\")\n    except json.JSONDecodeError:\n        print(\"Could not parse response as JSON\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        ""
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        "import dataiku, dataikuapi\n\n\n# Environment settings\nDSS_LOCATION \u003d \"https://cdl-dku-desi-p.commercial-datalake-prod.awscloud.abbvienet.com\"\nAPI_KEY \u003d \"dkuaps-9ALuuZLhFJg9dTcrSgPMcsdtfP8bpPXC\"\nPROJECT_KEY \u003d \"GENAIPOC\"\nWEBAPP_ID \u003d \"LmdRX0E\" \n\n# Depending on your case, use one of the following\n\n#client \u003d dataikuapi.DSSClient(DSS_LOCATION, API_KEY)\nclient \u003d dataiku.api_client()\n\ntry:\n    project \u003d client.get_project(PROJECT_KEY)\nexcept:\n    print(\"error initializing project\")\ntry:\n    webapp \u003d project.get_webapp(WEBAPP_ID)\nexcept:\n    print(\"error initializing webapp\")\ntry:\n    backend \u003d webapp.get_backend_client()\nexcept:\n    print(\"error initializing backend\")\n\nbackend.session.headers[\u0027Content-Type\u0027] \u003d \u0027application/json\u0027\n\nresp \u003d backend.session.post(backend.base_url + \u0027/query\u0027, json\u003d{\u0027message\u0027:\u0027Who to contact for rinvoq reactions?\u0027})\n\nprint(backend.base_url + \u0027/query\u0027)\n# Query the LLM\nprint(resp.text)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# test-headless"
      ]
    },
    {
      "execution_count": 5,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku, dataikuapi\n\n\n# Environment settings\nDSS_LOCATION \u003d \"https://cdl-dku-desi-p.commercial-datalake-prod.awscloud.abbvienet.com\"\nAPI_KEY \u003d \"dkuaps-9ALuuZLhFJg9dTcrSgPMcsdtfP8bpPXC\"\nPROJECT_KEY \u003d \"GENAIPOC\"\nWEBAPP_ID \u003d \"gq110S2\" \n\n# Depending on your case, use one of the following\n\n#client \u003d dataikuapi.DSSClient(DSS_LOCATION, API_KEY)\nclient \u003d dataiku.api_client()\n\ntry:\n    project \u003d client.get_project(PROJECT_KEY)\nexcept:\n    print(\"error initializing project\")\ntry:\n    webapp \u003d project.get_webapp(WEBAPP_ID)\nexcept:\n    print(\"error initializing webapp\")\ntry:\n    backend \u003d webapp.get_backend_client()\nexcept:\n    print(\"error initializing backend\")\n\nbackend.session.headers[\u0027Content-Type\u0027] \u003d \u0027application/json\u0027\n\n\n# Prepare and send payload to /query_on_docs\npayload \u003d {\n   \"files\": [\n      {\n         \"filename\": \"file1.txt\",\n         \"content\": \"SGVsbG8gd29ybGQh\"\n      },\n      {\n         \"filename\": \"file2.txt\",\n         \"content\": \"VGhpcyBpcyBhIHRlc3QgZmlsZS4\u003d\"\n      }\n   ]\n}\nresponse \u003d backend.session.post(backend.base_url + \u0027/update_kb\u0027, json\u003dpayload)\n\n# Query the LLM\nprint(response.text)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "{\"combined_text\":\"--- file1.txt ---\\nHello world!\\n\\n--- file2.txt ---\\nThis is a test file.\\n\",\"file_count\":2,\"result\":{\"chunks_added\":0,\"message\":\"No new data was added because all files already exist.\",\"status\":\"no_new_chunks\",\"total_rows\":1272}}\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "execution_count": 3,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "backend.base_url"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "\u0027http://10.242.92.241:10000/web-apps-backends/GENAIPOC/gq110S2/\u0027"
          },
          "metadata": {}
        }
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    }
  ]
}