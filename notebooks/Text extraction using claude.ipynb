{
  "metadata": {
    "kernelspec": {
      "name": "py-dku-venv-cdl-p-diku-psbts-py39-llm-env",
      "display_name": "Python (env cdl-p-diku-psbts-py39-llm-env)",
      "language": "python"
    },
    "hide_input": false,
    "language_info": {
      "name": "python",
      "version": "3.9.20",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "modifiedBy": "tomarsx1",
    "customFields": {},
    "creator": "chaudpx2",
    "tags": [],
    "createdOn": 1744700305916
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%pylab inline"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true
      },
      "source": [
        "# usual code"
      ]
    },
    {
      "execution_count": 1,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        "import dataiku\nfrom dataiku import pandasutils as pdu\nimport pandas as pd"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        "# Example: load a DSS dataset as a Pandas dataframe\nmydataset \u003d dataiku.Dataset(\"mydataset\")\nmydataset_df \u003d mydataset.get_dataframe()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        "import os\nimport shutil\nimport zipfile\nimport tempfile\nimport logging\nimport base64\nfrom io import BytesIO\nimport pandas as pd\nimport concurrent.futures\n\nimport dataiku\nfrom docx import Document\nimport pdfplumber\nfrom pptx import Presentation\nfrom PIL import Image\n\n\n# Configure logging\nlogging.basicConfig(level\u003dlogging.INFO, format\u003d\u0027%(asctime)s - %(levelname)s - %(message)s\u0027)\nlogger \u003d logging.getLogger(__name__)\n\n\nclass VisionLLMExtractor:\n    \"\"\"\n    Uses Vision LLM to extract text from images.\n    \"\"\"\n    def __init__(self, llm_model_id\u003d\"custom:iliad-plugin-conn-prod:Claude_3_5_Sonnet\"):\n        \"\"\"\n        Initialize the vision LLM extractor with the specified model.\n        \n        Args:\n            llm_model_id: ID of the LLM model to use\n        \"\"\"\n        try:\n            client \u003d dataiku.api_client()\n            project \u003d client.get_default_project()\n            self.llm_model \u003d project.get_llm(llm_model_id)\n            logger.info(f\"Initialized VisionLLMExtractor with model: {llm_model_id}\")\n        except Exception as e:\n            logger.error(f\"Error initializing VisionLLMExtractor: {e}\")\n            self.llm_model \u003d None\n    \n    def extract_text_from_image(self, image_data):\n        \"\"\"\n        Extract text from an image using the vision LLM.\n        \n        Args:\n            image_data: Raw image data bytes\n            \n        Returns:\n            str: Extracted text from the image\n        \"\"\"\n        if self.llm_model is None:\n            return \"Error: LLM model not initialized\"\n        \n        try:\n            # Convert image to base64\n            img_base64 \u003d base64.b64encode(image_data).decode(\"utf-8\")\n            \n            # Create and execute the completion request\n            completion \u003d self.llm_model.new_completion()\n            mp_message \u003d completion.new_multipart_message()\n            mp_message.with_text(\"Extract all text content visible in this image. Return only the extracted text without any additional commentary.\")\n            mp_message.with_text(f\"Here is the image in base64 format:\\n{img_base64}\")\n            mp_message.add()\n            \n            # Execute the completion request\n            logger.info(\"Executing LLM request for image text extraction...\")\n            resp \u003d completion.execute()\n            \n            # Extract response text\n            if hasattr(resp, \"text\"):\n                extracted_text \u003d resp.text\n                logger.info(f\"Successfully extracted text from image: {extracted_text[:50]}...\")\n                return extracted_text\n            else:\n                logger.error(\"Response object does not have \u0027text\u0027 attribute\")\n                return \"Error: Failed to extract text from image\"\n                \n        except Exception as e:\n            logger.error(f\"Error extracting text from image: {e}\")\n            return f\"Error extracting text from image: {e}\"\n\n\nclass BaseExtractor:\n    \"\"\"\n    Base extractor that handles file reading from a Dataiku Folder.\n    \"\"\"\n    def __init__(self, folder_id):\n        self.data_source \u003d dataiku.Folder(folder_id)\n\n    def get_file_data(self, file_path):\n        \"\"\"\n        Reads file data from the Dataiku Folder.\n        \"\"\"\n        with self.data_source.get_download_stream(file_path) as f:\n            return f.read()\n\n\nclass WordExtractor(BaseExtractor):\n    \"\"\"\n    Extracts text, tables, and images from Word documents.\n    \"\"\"\n    def __init__(self, folder_id, vision_llm\u003dNone):\n        super().__init__(folder_id)\n        self.vision_llm \u003d vision_llm\n        \n    def extract_text_and_tables(self, file_path):\n        \"\"\"\n        Extracts text and table content from a Word document.\n        \"\"\"\n        try:\n            file_data \u003d self.get_file_data(file_path)\n            doc_stream \u003d BytesIO(file_data)\n            doc \u003d Document(doc_stream)\n\n            # Extract text from paragraphs.\n            text_content \u003d [para.text.strip() for para in doc.paragraphs if para.text.strip()]\n\n            # Extract table data.\n            table_data \u003d []\n            for table in doc.tables:\n                table_content \u003d []\n                for row in table.rows:\n                    row_data \u003d [cell.text.strip() for cell in row.cells]\n                    table_content.append(row_data)\n                table_data.append(table_content)\n\n            return {\"text\": \"\\n\".join(text_content), \"tables\": table_data}\n        except Exception as e:\n            logger.error(f\"Error reading Word document {file_path}: {e}\")\n            return {\"error\": f\"Error reading Word document: {e}\"}\n\n    def extract_images(self, file_path, output_folder_id):\n        \"\"\"\n        Extracts images from a Word document and saves them to a managed folder.\n        Uses vision LLM to extract text from images if available.\n        \n        Returns a dict with image paths and extracted text.\n        \"\"\"\n        try:\n            file_data \u003d self.get_file_data(file_path)\n            file_name \u003d os.path.basename(file_path)\n            base_name \u003d os.path.splitext(file_name)[0]\n\n            # Get the output folder for images\n            image_folder \u003d dataiku.Folder(output_folder_id)\n            \n            # List existing files in the output folder\n            existing_images \u003d set(image_folder.list_paths_in_partition())\n\n            # Create a temporary directory for extraction\n            temp_dir \u003d tempfile.mkdtemp()\n            temp_docx_path \u003d os.path.join(temp_dir, file_name)\n            with open(temp_docx_path, \"wb\") as temp_file:\n                temp_file.write(file_data)\n\n            # Dictionary to store image paths and extracted text\n            image_results \u003d {}\n            \n            with zipfile.ZipFile(temp_docx_path, \"r\") as docx_zip:\n                for file_info in docx_zip.infolist():\n                    if file_info.filename.startswith(\"word/media/\"):\n                        image_name \u003d os.path.basename(file_info.filename)\n                        image_path \u003d f\"{base_name}_{image_name}\"\n                        \n                        # Get image data\n                        image_data \u003d docx_zip.read(file_info.filename)\n                        \n                        # Process with vision LLM if available\n                        extracted_text \u003d \"\"\n                        if self.vision_llm:\n                            extracted_text \u003d self.vision_llm.extract_text_from_image(image_data)\n                        \n                        # Check if the image already exists\n                        if image_path not in existing_images:\n                            with image_folder.get_writer(image_path) as writer:\n                                writer.write(image_data)\n                        \n                        # Store result\n                        image_results[image_path] \u003d extracted_text\n\n            shutil.rmtree(temp_dir, ignore_errors\u003dTrue)\n            return image_results\n        except Exception as e:\n            logger.error(f\"Error extracting images from {file_path}: {e}\")\n            return {\"error\": f\"Error extracting images: {e}\"}\n\n\nclass PDFExtractor(BaseExtractor):\n    \"\"\"\n    Extracts text and images from PDF documents.\n    \"\"\"\n    def __init__(self, folder_id, vision_llm\u003dNone):\n        super().__init__(folder_id)\n        self.vision_llm \u003d vision_llm\n    \n    def extract_text(self, file_path):\n        \"\"\"\n        Extracts text content from a PDF file.\n        \"\"\"\n        try:\n            file_data \u003d self.get_file_data(file_path)\n            pdf_stream \u003d BytesIO(file_data)\n            text_content \u003d []\n            with pdfplumber.open(pdf_stream) as pdf:\n                for page in pdf.pages:\n                    extracted \u003d page.extract_text()\n                    if extracted:\n                        text_content.append(extracted)\n            return \"\\n\".join(text_content)\n        except Exception as e:\n            logger.error(f\"Error reading PDF {file_path}: {e}\")\n            return f\"Error reading PDF: {e}\"\n    \n    def extract_images(self, file_path, output_folder_id):\n        \"\"\"\n        Extracts images from a PDF file and uses vision LLM to extract text.\n        \"\"\"\n        try:\n            file_data \u003d self.get_file_data(file_path)\n            pdf_stream \u003d BytesIO(file_data)\n            file_name \u003d os.path.basename(file_path)\n            base_name \u003d os.path.splitext(file_name)[0]\n            \n            # Get the output folder for images\n            image_folder \u003d dataiku.Folder(output_folder_id)\n            \n            # Dictionary to store image paths and extracted text\n            image_results \u003d {}\n            \n            with pdfplumber.open(pdf_stream) as pdf:\n                for i, page in enumerate(pdf.pages):\n                    # Extract images from the page\n                    image_objects \u003d page.images\n                    for j, img in enumerate(image_objects):\n                        # Extract image data\n                        image_data \u003d img[\"stream\"].get_data()\n                        image_path \u003d f\"{base_name}_page{i+1}_image{j+1}.png\"\n                        \n                        # Write image to output folder\n                        with image_folder.get_writer(image_path) as writer:\n                            writer.write(image_data)\n                        \n                        # Process with vision LLM if available\n                        extracted_text \u003d \"\"\n                        if self.vision_llm:\n                            extracted_text \u003d self.vision_llm.extract_text_from_image(image_data)\n                        \n                        # Store result\n                        image_results[image_path] \u003d extracted_text\n            \n            return image_results\n        except Exception as e:\n            logger.error(f\"Error extracting images from PDF {file_path}: {e}\")\n            return {\"error\": f\"Error extracting images from PDF: {e}\"}\n\n\nclass PowerPointExtractor(BaseExtractor):\n    \"\"\"\n    Extracts text and images from PowerPoint presentations.\n    \"\"\"\n    def __init__(self, folder_id, vision_llm\u003dNone):\n        super().__init__(folder_id)\n        self.vision_llm \u003d vision_llm\n    \n    def extract_text(self, file_path):\n        \"\"\"\n        Extracts text from each slide in a PowerPoint file.\n        \"\"\"\n        try:\n            file_data \u003d self.get_file_data(file_path)\n            ppt_stream \u003d BytesIO(file_data)\n            prs \u003d Presentation(ppt_stream)\n            text_content \u003d []\n            for slide in prs.slides:\n                for shape in slide.shapes:\n                    if hasattr(shape, \"text\") and shape.text:\n                        text_content.append(shape.text.strip())\n            return \"\\n\".join(text_content)\n        except Exception as e:\n            logger.error(f\"Error reading PowerPoint {file_path}: {e}\")\n            return f\"Error reading PowerPoint: {e}\"\n    \n    def extract_images(self, file_path, output_folder_id):\n        \"\"\"\n        Extracts images from a PowerPoint file and uses vision LLM to extract text.\n        \"\"\"\n        try:\n            file_data \u003d self.get_file_data(file_path)\n            ppt_stream \u003d BytesIO(file_data)\n            file_name \u003d os.path.basename(file_path)\n            base_name \u003d os.path.splitext(file_name)[0]\n            \n            # Get the output folder for images\n            image_folder \u003d dataiku.Folder(output_folder_id)\n            \n            # Dictionary to store image paths and extracted text\n            image_results \u003d {}\n            \n            prs \u003d Presentation(ppt_stream)\n            for i, slide in enumerate(prs.slides):\n                for j, shape in enumerate(slide.shapes):\n                    if hasattr(shape, \"image\"):\n                        # Extract image data\n                        image_data \u003d shape.image.blob\n                        image_path \u003d f\"{base_name}_slide{i+1}_image{j+1}.png\"\n                        \n                        # Write image to output folder\n                        with image_folder.get_writer(image_path) as writer:\n                            writer.write(image_data)\n                        \n                        # Process with vision LLM if available\n                        extracted_text \u003d \"\"\n                        if self.vision_llm:\n                            extracted_text \u003d self.vision_llm.extract_text_from_image(image_data)\n                        \n                        # Store result\n                        image_results[image_path] \u003d extracted_text\n            \n            return image_results\n        except Exception as e:\n            logger.error(f\"Error extracting images from PowerPoint {file_path}: {e}\")\n            return {\"error\": f\"Error extracting images from PowerPoint: {e}\"}\n\n\nclass SpreadsheetExtractor(BaseExtractor):\n    \"\"\"\n    Extracts data from Excel and CSV files, including multiple sheets in Excel.\n    \"\"\"\n    def extract_data(self, file_path):\n        try:\n            file_data \u003d self.get_file_data(file_path)\n            file_ext \u003d os.path.splitext(file_path)[1].lower()\n\n            if file_ext in [\u0027.xls\u0027, \u0027.xlsx\u0027]:\n                # For Excel files: extract all sheets\n                excel_data \u003d pd.read_excel(BytesIO(file_data), sheet_name\u003dNone)\n                # Convert each sheet to a list of dicts\n                return {sheet_name: df.to_dict(orient\u003d\u0027records\u0027) for sheet_name, df in excel_data.items()}\n            elif file_ext \u003d\u003d \u0027.csv\u0027:\n                # For CSV: read as a single sheet\n                df \u003d pd.read_csv(BytesIO(file_data))\n                return {\"Sheet1\": df.to_dict(orient\u003d\u0027records\u0027)}\n            else:\n                return {\"error\": f\"Unsupported spreadsheet file type: {file_ext}\"}\n        except Exception as e:\n            logger.error(f\"Error reading spreadsheet {file_path}: {e}\")\n            return {\"error\": f\"Error reading spreadsheet: {e}\"}\n\n\nclass FileProcessor:\n    \"\"\"\n    Processes files in a folder by dispatching them to the appropriate extractor\n    based on file extension.\n    \"\"\"\n    def __init__(self, folder_id, image_output_folder_id\u003dNone, max_workers\u003d10, \n                 llm_model_id\u003d\"custom:iliad-plugin-conn-prod:Claude_3_5_Sonnet\"):\n        self.folder_id \u003d folder_id\n        self.image_output_folder_id \u003d image_output_folder_id\n        self.max_workers \u003d int(max_workers) #or os.cpu_count()\n        # Initialize vision LLM extractor\n        self.vision_llm \u003d VisionLLMExtractor(llm_model_id)\n        logger.info(f\"Initialized FileProcessor with {self.max_workers} workers and LLM model {llm_model_id}\")\n\n    def process_file(self, file_path):\n        \"\"\"Process a single file and return the extracted data.\"\"\"\n        file_name \u003d os.path.basename(file_path)\n        ext \u003d os.path.splitext(file_name)[1].lower()\n        result \u003d {\"file_name\": file_name, \"file_path\": file_path}\n        \n        logger.info(f\"Processing file: {file_name}\")\n        \n        try:\n            if ext \u003d\u003d \".docx\":\n                word_extractor \u003d WordExtractor(self.folder_id, self.vision_llm)\n                content \u003d word_extractor.extract_text_and_tables(file_path)\n                result[\"text_content\"] \u003d content.get(\"text\", content.get(\"error\", \"\"))\n                result[\"table_content\"] \u003d str(content.get(\"tables\", \"\"))\n                \n                if self.image_output_folder_id:\n                    images_result \u003d word_extractor.extract_images(file_path, self.image_output_folder_id)\n                    # Format the output with image paths and extracted text\n                    formatted_image_results \u003d []\n                    for img_path, text in images_result.items():\n                        formatted_image_results.append(f\"Image: {img_path}, Text: {text}\")\n                    result[\"images_extracted\"] \u003d \"\\n\".join(formatted_image_results)\n                else:\n                    result[\"images_extracted\"] \u003d \"\"\n                    \n            elif ext \u003d\u003d \".pdf\":\n                pdf_extractor \u003d PDFExtractor(self.folder_id, self.vision_llm)\n                result[\"text_content\"] \u003d pdf_extractor.extract_text(file_path)\n                result[\"table_content\"] \u003d \"\"\n                \n                if self.image_output_folder_id:\n                    images_result \u003d pdf_extractor.extract_images(file_path, self.image_output_folder_id)\n                    # Format the output with image paths and extracted text\n                    if isinstance(images_result, dict) and \"error\" not in images_result:\n                        formatted_image_results \u003d []\n                        for img_path, text in images_result.items():\n                            formatted_image_results.append(f\"Image: {img_path}, Text: {text}\")\n                        result[\"images_extracted\"] \u003d \"\\n\".join(formatted_image_results)\n                    else:\n                        result[\"images_extracted\"] \u003d str(images_result)\n                else:\n                    result[\"images_extracted\"] \u003d \"\"\n                    \n            elif ext in [\".ppt\", \".pptx\"]:\n                ppt_extractor \u003d PowerPointExtractor(self.folder_id, self.vision_llm)\n                result[\"text_content\"] \u003d ppt_extractor.extract_text(file_path)\n                result[\"table_content\"] \u003d \"\"\n                \n                if self.image_output_folder_id:\n                    images_result \u003d ppt_extractor.extract_images(file_path, self.image_output_folder_id)\n                    # Format the output with image paths and extracted text\n                    if isinstance(images_result, dict) and \"error\" not in images_result:\n                        formatted_image_results \u003d []\n                        for img_path, text in images_result.items():\n                            formatted_image_results.append(f\"Image: {img_path}, Text: {text}\")\n                        result[\"images_extracted\"] \u003d \"\\n\".join(formatted_image_results)\n                    else:\n                        result[\"images_extracted\"] \u003d str(images_result)\n                else:\n                    result[\"images_extracted\"] \u003d \"\"\n                \n            elif ext in [\".xls\", \".xlsx\", \".csv\"]:\n                spreadsheet_extractor \u003d SpreadsheetExtractor(self.folder_id)\n                data \u003d spreadsheet_extractor.extract_data(file_path)\n                if \"error\" in data:\n                    result[\"text_content\"] \u003d data[\"error\"]\n                    result[\"table_content\"] \u003d \"\"\n                else:\n                    result[\"text_content\"] \u003d \"\"\n                    result[\"table_content\"] \u003d str(data)\n                result[\"images_extracted\"] \u003d \"\"\n                \n            else:\n                result[\"text_content\"] \u003d f\"Unsupported file type: {file_name}\"\n                result[\"table_content\"] \u003d \"\"\n                result[\"images_extracted\"] \u003d \"\"\n            \n            logger.info(f\"Completed processing file: {file_name}\")\n            return result\n            \n        except Exception as e:\n            logger.error(f\"Error processing file {file_name}: {e}\")\n            result[\"text_content\"] \u003d f\"Error processing file: {e}\"\n            result[\"table_content\"] \u003d \"\"\n            result[\"images_extracted\"] \u003d \"\"\n            return result\n\n    def process_all_files(self, file_list):\n        \"\"\"\n        Processes all files in parallel and returns a Pandas DataFrame.\n        \n        Args:\n            file_list: List of file paths to process\n            \n        Returns:\n            pd.DataFrame: DataFrame containing the extracted data\n        \"\"\"\n        logger.info(f\"Starting parallel processing of {len(file_list)} files with {self.max_workers} workers\")\n        extracted_data \u003d []\n        \n        # Use ThreadPoolExecutor for I/O bound operations\n        with concurrent.futures.ThreadPoolExecutor(max_workers\u003dself.max_workers) as executor:\n            # Submit all file processing tasks\n            future_to_file \u003d {executor.submit(self.process_file, file_path): file_path \n                             for file_path in file_list}\n            \n            # Process results as they complete\n            for future in concurrent.futures.as_completed(future_to_file):\n                file_path \u003d future_to_file[future]\n                try:\n                    data \u003d future.result()\n                    extracted_data.append(data)\n                    logger.info(f\"Added results for {os.path.basename(file_path)}\")\n                except Exception as e:\n                    logger.error(f\"Exception processing {file_path}: {e}\")\n                    # Add error information to the results\n                    extracted_data.append({\n                        \"file_name\": os.path.basename(file_path),\n                        \"file_path\": file_path,\n                        \"text_content\": f\"Error in parallel processing: {e}\",\n                        \"table_content\": \"\",\n                        \"images_extracted\": \"\"\n                    })\n        \n        logger.info(f\"Completed processing all {len(file_list)} files\")\n        return pd.DataFrame(extracted_data)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        "def main():\n    \"\"\"\n    Main function to run the document digitization pipeline.\n    Example usage in a Dataiku recipe.\n    \"\"\"\n    # Get input and output datasets from Dataiku\n    input_folder_id \u003d \"Input\" #dataiku.get_custom_variables().get(\"input_folder\", \"input_documents\")\n    output_dataset \u003d \"tripadvisor_hotel_reviews_summarized\" #dataiku.get_custom_variables().get(\"output_dataset\", \"extracted_document_data\")\n    \n    # Get LLM model ID from custom variables or use default\n    llm_model_id \u003d \"custom:iliad-plugin-conn-prod:Claude_3_5_Sonnet\" #dataiku.get_custom_variables().get(\"llm_model_id\", \"custom:iliad-plugin-conn-prod:Claude_3_5_Sonnet\")\n    \n    # Configure parallel processing\n    max_workers \u003d 10 #int(dataiku.get_custom_variables().get(\"max_workers\", 10))\n    \n    image_output_folder_id\u003d\"input_images_extracted_custom\"  \n    \n    input_folder \u003d dataiku.Folder(input_folder_id)\n    # List all files in the input folder\n    file_list \u003d input_folder.list_paths_in_partition()\n    \n    # Initialize the document processor\n    processor \u003d FileProcessor(\n        input_folder_id,\n        image_output_folder_id,\n        10,\n        llm_model_id\n        \n    )\n    \n    # Process all documents\n    results_df \u003d processor.process_all_files(file_list)\n    print(results_df)\n    \n    # Write results to the output dataset\n    output \u003d dataiku.Dataset(output_dataset)\n    output.write_with_schema(results_df)\n    \n    logger.info(f\"Document digitization pipeline completed. Processed {len(results_df)} files.\")\n\n\nif __name__ \u003d\u003d \"__main__\":\n    main()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Using unstructured library"
      ]
    },
    {
      "execution_count": 1,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\nimport logging\nimport pandas as pd\nfrom io import BytesIO\nfrom typing import Dict, List, Any, Optional\nimport traceback\n\n# Configure logging\nlogger \u003d logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\n\nclass ExcelExtractor:\n    \"\"\"\n    Extracts content from Excel files (xlsx, xls, xlsm) using pandas and openpyxl.\n    Handles large files and multiple sheets effectively.\n    \"\"\"\n    \n    def __init__(self):\n        self.max_rows_per_sheet \u003d 500  # Limit rows to prevent memory issues\n        self.max_cols_per_sheet \u003d 20   # Limit columns to prevent memory issues\n        \n    def extract_excel_content(self, file_data: bytes, file_path: str) -\u003e Dict[str, Any]:\n        \"\"\"\n        Extract content from Excel files.\n        \n        Args:\n            file_data: Raw bytes of the Excel file\n            file_path: Path to the Excel file (used for logging and metadata)\n            \n        Returns:\n            Dict with extracted content:\n                - text: Extracted text representation\n                - sheets: List of sheet data\n                - metadata: File metadata\n        \"\"\"\n        try:\n            file_name \u003d os.path.basename(file_path)\n            logger.info(f\"Processing Excel file: {file_name}\")\n            \n            # Create a BytesIO object from file data\n            excel_stream \u003d BytesIO(file_data)\n            \n            # Read all sheets from the Excel file using openpyxl\n            excel_file \u003d pd.ExcelFile(excel_stream, engine\u003d\u0027openpyxl\u0027)\n            sheet_names \u003d excel_file.sheet_names\n            \n            sheets_data \u003d []\n            all_text_content \u003d []\n            \n            for sheet_name in sheet_names:\n                # Read the sheet into a DataFrame\n                try:\n                    df \u003d pd.read_excel(excel_file, sheet_name\u003dsheet_name, engine\u003d\u0027openpyxl\u0027)\n                    \n                    # Handle empty sheets\n                    if df.empty:\n                        sheets_data.append({\n                            \"sheet_name\": sheet_name,\n                            \"is_empty\": True,\n                            \"text\": f\"[Sheet \u0027{sheet_name}\u0027 is empty]\"\n                        })\n                        all_text_content.append(f\"\\n--- Sheet: {sheet_name} ---\\n[Empty sheet]\")\n                        continue\n                    \n                    # Convert DataFrame to text representation\n                    text_representation \u003d self._dataframe_to_text(df, sheet_name)\n                    all_text_content.append(text_representation)\n                    \n                    # Store sheet data\n                    sheets_data.append({\n                        \"sheet_name\": sheet_name,\n                        \"is_empty\": False,\n                        \"row_count\": len(df),\n                        \"column_count\": len(df.columns),\n                        \"columns\": df.columns.tolist(),\n                        \"text\": text_representation\n                    })\n                except Exception as e:\n                    logger.error(f\"Error processing sheet \u0027{sheet_name}\u0027 in {file_name}: {str(e)}\")\n                    sheets_data.append({\n                        \"sheet_name\": sheet_name,\n                        \"is_empty\": True,\n                        \"text\": f\"[Error processing sheet \u0027{sheet_name}\u0027: {str(e)}]\"\n                    })\n                    all_text_content.append(f\"\\n--- Sheet: {sheet_name} ---\\n[Error processing sheet: {str(e)}]\")\n            \n            # Create metadata\n            metadata \u003d {\n                \"file_name\": file_name,\n                \"file_path\": file_path,\n                \"file_type\": os.path.splitext(file_name)[1].lower(),\n                \"sheet_count\": len(sheet_names),\n                \"sheet_names\": sheet_names\n            }\n            \n            # Combine all content\n            full_text_content \u003d \"\\n\\n\".join(all_text_content)\n            \n            result \u003d {\n                \"text\": full_text_content,\n                \"sheets\": sheets_data,\n                \"metadata\": metadata\n            }\n            \n            logger.info(f\"Successfully processed Excel file: {file_name}\")\n            return result\n            \n        except Exception as e:\n            logger.error(f\"Error extracting content from Excel file {file_path}: {str(e)}\\n{traceback.format_exc()}\")\n            return {\n                \"text\": \"Error extracting Excel content: File could not be processed.\",\n                \"sheets\": [],\n                \"metadata\": {\"file_name\": file_name, \"file_path\": file_path, \"error\": str(e)}\n            }\n    \n    def _dataframe_to_text(self, df: pd.DataFrame, sheet_name: str) -\u003e str:\n        \"\"\"\n        Convert a DataFrame to a text representation.\n        \n        Args:\n            df: The pandas DataFrame to convert\n            sheet_name: Name of the sheet\n            \n        Returns:\n            str: Text representation of the DataFrame\n        \"\"\"\n        try:\n            rows, cols \u003d df.shape\n            text_lines \u003d [f\"--- Sheet: {sheet_name} ---\"]\n            text_lines.append(f\"Rows: {rows}, Columns: {cols}\")\n            text_lines.append(\"\")\n            \n            max_rows \u003d min(self.max_rows_per_sheet, rows)\n            max_cols \u003d min(self.max_cols_per_sheet, cols)\n            \n            # Show sample data (first `max_rows` rows and `max_cols` columns)\n            sample_df \u003d df.iloc[:max_rows, :max_cols].fillna(\"\")\n            sample_text \u003d sample_df.to_string(index\u003dFalse)\n            text_lines.append(sample_text)\n            \n            return \"\\n\".join(text_lines)\n            \n        except Exception as e:\n            logger.error(f\"Error converting DataFrame to text for sheet {sheet_name}: {str(e)}\")\n            return f\"[Error converting DataFrame to text: {str(e)}]\"\n\n# Handler function for the document processor\ndef handle_excel_file(file_data: bytes, file_path: str) -\u003e Dict[str, Any]:\n    \"\"\"\n    Handler function for Excel files that you can call from your document processor.\n    \n    Args:\n        file_data: Raw bytes of the Excel file\n        file_path: Path to the Excel file\n        \n    Returns:\n        Dict with extracted content\n    \"\"\"\n    extractor \u003d ExcelExtractor()\n    extracted_data \u003d extractor.extract_excel_content(file_data, file_path)\n    \n    # Convert extracted data to the desired format\n    combined_data \u003d {\n        \"text\": extracted_data.get(\"text\", \"\"),\n        \"tables\": [\n            {\n                \"text\": sheet.get(\"text\", \"\"), \n                \"metadata\": {\"sheet_name\": sheet.get(\"sheet_name\", \"\")}\n            }\n            for sheet in extracted_data.get(\"sheets\", [])\n        ],\n        \"images\": [],\n        \"metadata\": extracted_data.get(\"metadata\", {}),\n    }\n    \n    return combined_data\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    },
    {
      "execution_count": 2,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\nimport logging\nfrom io import BytesIO\nimport base64\nimport pandas as pd\nimport concurrent.futures\nfrom typing import Dict, List, Any, Optional, Tuple\n\nimport dataiku\nfrom unstructured.partition.auto import partition\nfrom unstructured.chunking.title import chunk_by_title\nfrom unstructured.staging.base import elements_to_json\nfrom unstructured.partition.xlsx import partition_xlsx\n\n\n\n# Configure logging\nlogging.basicConfig(level\u003dlogging.INFO, format\u003d\u0027%(asctime)s - %(levelname)s - %(message)s\u0027)\nlogger \u003d logging.getLogger(__name__)\n\nclass BaseDigitizer:\n    \"\"\"\n    Base class for document digitization that handles file reading from a Dataiku Folder.\n    \"\"\"\n    def __init__(self, folder_id: str):\n        \"\"\"\n        Initialize the digitizer with a Dataiku folder.\n        \n        Args:\n            folder_id: The ID of the Dataiku folder containing the documents\n        \"\"\"\n        self.data_source \u003d dataiku.Folder(folder_id)\n    \n    def get_file_data(self, file_path: str) -\u003e bytes:\n        \"\"\"\n        Reads file data from the Dataiku Folder.\n        \n        Args:\n            file_path: Path to the file within the Dataiku folder\n            \n        Returns:\n            bytes: The file content as bytes\n        \"\"\"\n        with self.data_source.get_download_stream(file_path) as f:\n            return f.read()\n\nclass UnstructuredDigitizer(BaseDigitizer):\n    \"\"\"\n    Digitizes documents using the Unstructured library to extract content.\n    \"\"\"\n    def __init__(self, folder_id: str, llm_model_id: str \u003d \"custom:iliad-plugin-conn-prod:Claude_3_5_Sonnet\"):\n        \"\"\"\n        Initialize the Unstructured digitizer.\n        \n        Args:\n            folder_id: The ID of the Dataiku folder containing the documents\n            llm_model_id: The ID of the LLM model to use for image description\n        \"\"\"\n        super().__init__(folder_id)\n        self.llm_model_id \u003d llm_model_id\n        \n        # Initialize Dataiku LLM client\n        self.client \u003d dataiku.api_client()\n        self.project \u003d self.client.get_default_project()\n        try:\n            self.llm_model \u003d self.project.get_llm(self.llm_model_id)\n            logger.info(f\"Successfully initialized LLM model: {self.llm_model_id}\")\n        except Exception as e:\n            logger.warning(f\"Failed to initialize LLM model: {e}\")\n            self.llm_model \u003d None\n    \n    def extract_content(self, file_path: str) -\u003e Dict[str, Any]:\n        \"\"\"\n        Extract all content from a document using Unstructured.\n        \n        Args:\n            file_path: Path to the file within the Dataiku folder\n            \n        Returns:\n            Dict with keys:\n                - text: Extracted text content\n                - tables: Extracted tables\n                - images: List of extracted images with descriptions\n                - metadata: Document metadata\n        \"\"\"\n        try:\n            file_data \u003d self.get_file_data(file_path)\n            file_name \u003d os.path.basename(file_path)\n            file_ext \u003d os.path.splitext(file_name)[1].lower()\n            \n            # Handle Excel files separately\n            if file_ext in [\u0027.xlsx\u0027, \u0027.xls\u0027]:\n                return handle_excel_file(file_data, file_path)\n            \n            # Create a BytesIO object from the file data\n            file_stream \u003d BytesIO(file_data)\n            \n            # Extract elements using Unstructured\n            elements \u003d partition(\n                file\u003dfile_stream,\n                file_filename\u003dfile_name,\n                strategy\u003d\"auto\",\n                include_metadata\u003dTrue,\n                extract_images_in_pdf\u003dTrue,\n                extract_image_block_types\u003d[\"Image\"],\n                extract_tables\u003dTrue\n            )\n            \n            # Process extracted elements\n            text_elements \u003d []\n            table_elements \u003d []\n            image_elements \u003d []\n            \n            for element in elements:\n                element_type \u003d element.category\n                \n                if element_type \u003d\u003d \"Table\":\n                    table_elements.append(element)\n                elif element_type \u003d\u003d \"Image\":\n                    image_elements.append(element)\n                elif element_type in [\"Title\", \"NarrativeText\", \"Text\", \"ListItem\", \"Header\"]:\n                    text_elements.append(element)\n            \n            # Process text content\n            text_content \u003d \"\\n\".join([element.text for element in text_elements])\n            \n            # Process tables\n            tables \u003d []\n            for table_element in table_elements:\n                tables.append({\n                    \"text\": table_element.text,\n                    \"metadata\": table_element.metadata.to_dict() if hasattr(table_element, \"metadata\") else {}\n                })\n            \n            # Process images and get descriptions using Dataiku LLM\n            images \u003d []\n            for image_element in image_elements:\n                image_data \u003d {}\n                if hasattr(image_element, \"metadata\") and hasattr(image_element.metadata, \"image_base64\"):\n                    image_data[\"image_base64\"] \u003d image_element.metadata.image_base64\n                    if self.llm_model:\n                        image_data[\"description\"] \u003d self._describe_image(image_element.metadata.image_base64)\n                    else:\n                        image_data[\"description\"] \u003d \"Image description not available (LLM model not configured)\"\n                images.append(image_data)\n            \n            # Get document metadata\n            metadata \u003d {\n                \"file_name\": file_name,\n                \"file_path\": file_path,\n                \"file_type\": file_ext,\n                \"page_count\": len(set([e.metadata.page_number for e in elements if hasattr(e, \"metadata\") and hasattr(e.metadata, \"page_number\")])),\n            }\n            \n            # Create structured output\n            result \u003d {\n                \"text\": text_content,\n                \"tables\": tables,\n                \"images\": images,\n                \"metadata\": metadata\n            }\n            \n            return result\n        except Exception as e:\n            logger.error(f\"Error extracting content from {file_path}: {e}\")\n            return {\n                \"text\": f\"Error extracting content: {str(e)}\",\n                \"tables\": [],\n                \"images\": [],\n                \"metadata\": {\"file_name\": os.path.basename(file_path), \"file_path\": file_path, \"error\": str(e)}\n            }\n    \n    def _describe_image(self, image_base64: str) -\u003e str:\n        \"\"\"\n        Generate a description for an image using Dataiku\u0027s LLM integration.\n        \n        Args:\n            image_base64: Base64 encoded image\n            \n        Returns:\n            str: Description of the image\n        \"\"\"\n        try:\n            if not self.llm_model:\n                return \"Image description not available (LLM model not configured)\"\n            \n            # Create a completion request\n            completion \u003d self.llm_model.new_completion()\n            mp_message \u003d completion.new_multipart_message()\n            \n            # Add text instructions and image data\n            mp_message.with_text(\"Please provide a detailed description of this image, including all visible text, elements, and context.\")\n            mp_message.with_text(f\"Here is the image in base64 format:\\n{image_base64}\")\n            mp_message.add()\n            \n            # Execute the completion request\n            logger.info(\"Executing LLM request for image description...\")\n            resp \u003d completion.execute()\n            \n            # Extract response text\n            if resp.success and hasattr(resp, \"text\"):\n                return resp.text\n            else:\n                logger.warning(f\"LLM request failed or unexpected response format: {resp}\")\n                return \"Unable to generate image description\"\n        except Exception as e:\n            logger.error(f\"Error describing image: {e}\")\n            return f\"Error generating image description: {str(e)}\"\n    \n    def combine_extracted_data(self, extracted_data: Dict[str, Any]) -\u003e str:\n        \"\"\"\n        Combine all extracted data into a single text column.\n        \n        Args:\n            extracted_data: Dictionary containing text, tables, and images\n            \n        Returns:\n            str: Combined extracted data as a single text string\n        \"\"\"\n        combined \u003d []\n        \n        # Add metadata\n        metadata \u003d extracted_data.get(\"metadata\", {})\n        combined.append(f\"DOCUMENT METADATA:\")\n        for key, value in metadata.items():\n            combined.append(f\"{key}: {value}\")\n        combined.append(\"\\n\")\n        \n        # Add text content\n        text_content \u003d extracted_data.get(\"text\", \"\")\n        if text_content:\n            combined.append(\"TEXT CONTENT:\")\n            combined.append(text_content)\n            combined.append(\"\\n\")\n        \n        # Add tables\n        tables \u003d extracted_data.get(\"tables\", [])\n        if tables:\n            combined.append(\"TABLE CONTENT:\")\n            for i, table in enumerate(tables):\n                combined.append(f\"Table {i+1}:\")\n                combined.append(table.get(\"text\", \"\"))\n                combined.append(\"\")\n            combined.append(\"\\n\")\n        \n        # Add images with descriptions\n        images \u003d extracted_data.get(\"images\", [])\n        if images:\n            combined.append(\"IMAGE CONTENT:\")\n            for i, image in enumerate(images):\n                combined.append(f\"Image {i+1} Description:\")\n                combined.append(image.get(\"description\", \"No description available\"))\n                combined.append(\"\")\n        \n        return \"\\n\".join(combined)\n\n\nclass DocumentProcessor:\n    \"\"\"\n    Processes documents in a folder by extracting content using the UnstructuredDigitizer.\n    \"\"\"\n    def __init__(self, input_folder_id: str, llm_model_id: str \u003d \"custom:iliad-plugin-conn-prod:Claude_3_5_Sonnet\", \n                 max_workers: int \u003d 1):\n        \"\"\"\n        Initialize the document processor.\n        \n        Args:\n            input_folder_id: The ID of the Dataiku folder containing the documents\n            llm_model_id: The ID of the LLM model to use for image description\n            max_workers: Maximum number of worker threads for parallel processing\n        \"\"\"\n        self.input_folder_id \u003d input_folder_id\n        self.digitizer \u003d UnstructuredDigitizer(input_folder_id, llm_model_id)\n        self.max_workers \u003d max_workers or os.cpu_count()\n        logger.info(f\"Initialized DocumentProcessor with {self.max_workers} workers\")\n    \n    def process_file(self, file_path: str) -\u003e Dict[str, Any]:\n        \"\"\"\n        Process a single document file.\n        \n        Args:\n            file_path: Path to the file within the Dataiku folder\n            \n        Returns:\n            Dict: Processed document data\n        \"\"\"\n        try:\n            file_name \u003d os.path.basename(file_path)\n            logger.info(f\"Processing file: {file_name}\")\n            \n            # Extract content using the digitizer\n            extracted_data \u003d self.digitizer.extract_content(file_path)\n            \n            # Combine all extracted data into a single text field\n            combined_data \u003d self.digitizer.combine_extracted_data(extracted_data)\n            \n            # Prepare result\n            result \u003d {\n                \"file_name\": file_name,\n                \"file_path\": file_path,\n                \"extracted_text\": str(extracted_data.get(\"text\", {})),\n                \"extracted_tables\": str(extracted_data.get(\"tables\", {})),\n                \"extracted_images\": str(extracted_data.get(\"images\", {})),\n                \"extracted_data\": combined_data,\n                \"metadata\": str(extracted_data.get(\"metadata\", {})),\n                \"processing_status\": \"success\"\n            }\n            \n            logger.info(f\"Completed processing file: {file_name}\")\n            return result\n        except Exception as e:\n            logger.error(f\"Error processing file {file_path}: {e}\")\n            return {\n                \"file_name\": os.path.basename(file_path),\n                \"file_path\": file_path,\n                \"extracted_data\": f\"Error processing file: {str(e)}\",\n                \"metadata\": \"{}\",\n                \"processing_status\": \"error\"\n            }\n    \n    def process_all_files(self, file_list: Optional[List[str]] \u003d None) -\u003e pd.DataFrame:\n        \"\"\"\n        Process all files in the input folder in parallel.\n        \n        Args:\n            file_list: List of file paths to process. If None, all files in the folder are processed.\n            \n        Returns:\n            pd.DataFrame: DataFrame containing the extracted data\n        \"\"\"\n        # If no file list is provided, get all files from the folder\n        if file_list is None:\n            data_source \u003d dataiku.Folder(self.input_folder_id)\n            file_list \u003d data_source.list_paths_in_partition()\n        \n        logger.info(f\"Starting parallel processing of {len(file_list)} files with {self.max_workers} workers\")\n        processed_data \u003d []\n        \n        # Use ThreadPoolExecutor for parallel processing\n        with concurrent.futures.ThreadPoolExecutor(max_workers\u003dself.max_workers) as executor:\n            # Submit all file processing tasks\n            future_to_file \u003d {executor.submit(self.process_file, file_path): file_path \n                             for file_path in file_list}\n            \n            # Process results as they complete\n            for future in concurrent.futures.as_completed(future_to_file):\n                file_path \u003d future_to_file[future]\n                try:\n                    data \u003d future.result()\n                    processed_data.append(data)\n                    logger.info(f\"Added results for {os.path.basename(file_path)}\")\n                except Exception as e:\n                    logger.error(f\"Exception processing {file_path}: {e}\")\n                    # Add error information to the results\n                    processed_data.append({\n                        \"file_name\": os.path.basename(file_path),\n                        \"file_path\": file_path,\n                        \"extracted_data\": f\"Error in parallel processing: {str(e)}\",\n                        \"metadata\": \"{}\",\n                        \"processing_status\": \"error\"\n                    })\n        \n        logger.info(f\"Completed processing all {len(file_list)} files\")\n        return pd.DataFrame(processed_data)\n\n\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def main():\n    \"\"\"\n    Main function to run the document digitization pipeline.\n    Example usage in a Dataiku recipe.\n    \"\"\"\n    logging.basicConfig(level\u003dlogging.INFO)\n    # Get input and output datasets from Dataiku\n    input_folder \u003d \"Input\" #dataiku.get_custom_variables().get(\"input_folder\", \"input_documents\")\n    output_dataset \u003d \"tripadvisor_hotel_reviews_summarized\" #dataiku.get_custom_variables().get(\"output_dataset\", \"extracted_document_data\")\n    \n    # Get LLM model ID from custom variables or use default\n    llm_model_id \u003d \"custom:iliad-plugin-conn-prod:Claude_3_5_Sonnet\" #dataiku.get_custom_variables().get(\"llm_model_id\", \"custom:iliad-plugin-conn-prod:Claude_3_5_Sonnet\")\n    \n    # Configure parallel processing\n    max_workers \u003d 1 #int(dataiku.get_custom_variables().get(\"max_workers\", 10))\n    \n    # Initialize the document processor\n    processor \u003d DocumentProcessor(\n        input_folder_id\u003dinput_folder,\n        llm_model_id\u003dllm_model_id,\n        max_workers\u003dmax_workers\n    )\n    \n    # Process all documents\n    results_df \u003d processor.process_all_files()\n    print(results_df)\n    # Write results to the output dataset\n    output \u003d dataiku.Dataset(output_dataset)\n    output.write_with_schema(results_df)\n    \n    logger.info(f\"Document digitization pipeline completed. Processed {len(results_df)} files.\")\n\n\nif __name__ \u003d\u003d \"__main__\":\n    main()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "2025-04-29 02:07:39,888 - INFO - Successfully initialized LLM model: custom:iliad-plugin-conn-prod:Claude_3_5_Sonnet\n2025-04-29 02:07:39,889 - INFO - Initialized DocumentProcessor with 1 workers\n2025-04-29 02:07:39,925 - INFO - Starting parallel processing of 71 files with 1 workers\n2025-04-29 02:07:39,926 - INFO - Processing file: RC Cover Email - Peds.pdf\n/data/dataiku/dss_data/code-envs/python/cdl-p-diku-psbts-py39-llm-env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n2025-04-29 02:07:42,832 - INFO - pikepdf C++ to Python logger bridge initialized\n2025-04-29 02:07:45,028 - INFO - Reading PDF for file: /tmp/tmppphrnrsa/document.pdf ...\n2025-04-29 02:08:14,685 - INFO - Completed processing file: RC Cover Email - Peds.pdf\n2025-04-29 02:08:14,686 - INFO - Processing file: Phase 1 Requirements - PSIT Patient Journey AI Model.docx\n2025-04-29 02:08:14,686 - INFO - Added results for RC Cover Email - Peds.pdf\n2025-04-29 02:08:15,482 - INFO - Completed processing file: Phase 1 Requirements - PSIT Patient Journey AI Model.docx\n2025-04-29 02:08:15,483 - INFO - Processing file: RComplete Bridge Optimization Tactical brief1.23.25.xlsx\n2025-04-29 02:08:15,483 - INFO - Added results for Phase 1 Requirements - PSIT Patient Journey AI Model.docx\n2025-04-29 02:08:15,549 - INFO - Processing Excel file: RComplete Bridge Optimization Tactical brief1.23.25.xlsx\n/data/dataiku/dss_data/code-envs/python/cdl-p-diku-psbts-py39-llm-env/lib64/python3.9/site-packages/pandas/compat/_optional.py:117: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  if distutils.version.LooseVersion(version) \u003c minimum_version:\n/data/dataiku/dss_data/code-envs/python/cdl-p-diku-psbts-py39-llm-env/lib/python3.9/site-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  other \u003d LooseVersion(other)\n2025-04-29 02:08:15,732 - INFO - Successfully processed Excel file: RComplete Bridge Optimization Tactical brief1.23.25.xlsx\n2025-04-29 02:08:15,735 - INFO - Completed processing file: RComplete Bridge Optimization Tactical brief1.23.25.xlsx\n2025-04-29 02:08:15,735 - INFO - Processing file: FAS Task Actions - Process - FAS Process-from 12-20-23 workshop.pdf\n2025-04-29 02:08:15,735 - INFO - Added results for RComplete Bridge Optimization Tactical brief1.23.25.xlsx\n2025-04-29 02:08:15,802 - WARNING - CropBox missing from /Page, defaulting to MediaBox\n2025-04-29 02:08:23,407 - INFO - Reading PDF for file: /tmp/tmpgt8xryfx/document.pdf ...\n2025-04-29 02:08:26,238 - WARNING - CropBox missing from /Page, defaulting to MediaBox\n2025-04-29 02:08:49,770 - INFO - Executing LLM request for image description...\n2025-04-29 02:08:49,783 - INFO - Executing LLM request for image description...\n2025-04-29 02:08:49,792 - INFO - Completed processing file: FAS Task Actions - Process - FAS Process-from 12-20-23 workshop.pdf\n2025-04-29 02:08:49,793 - INFO - Processing file: ARS RC Sorry I Missed You Email.pdf\n2025-04-29 02:08:49,793 - INFO - Added results for FAS Task Actions - Process - FAS Process-from 12-20-23 workshop.pdf\n2025-04-29 02:08:51,013 - INFO - Reading PDF for file: /tmp/tmpnkh5i7zu/document.pdf ...\n2025-04-29 02:09:30,734 - INFO - Completed processing file: ARS RC Sorry I Missed You Email.pdf\n2025-04-29 02:09:30,735 - INFO - Processing file: HCP Affordability Bridge Eligible Letter_B2H.docx\n2025-04-29 02:09:30,736 - INFO - Added results for ARS RC Sorry I Missed You Email.pdf\n2025-04-29 02:09:30,979 - INFO - Completed processing file: HCP Affordability Bridge Eligible Letter_B2H.docx\n2025-04-29 02:09:30,979 - INFO - Processing file: Draft_PSIT_Patient_Journey_AI_Model.pptx\n2025-04-29 02:09:30,980 - INFO - Added results for HCP Affordability Bridge Eligible Letter_B2H.docx\n2025-04-29 02:09:31,316 - INFO - Completed processing file: Draft_PSIT_Patient_Journey_AI_Model.pptx\n2025-04-29 02:09:31,317 - INFO - Processing file: RC Welcome Email.pdf\n2025-04-29 02:09:31,317 - INFO - Added results for Draft_PSIT_Patient_Journey_AI_Model.pptx\n2025-04-29 02:09:32,567 - INFO - Reading PDF for file: /tmp/tmpqocxs3nw/document.pdf ...\n2025-04-29 02:10:02,784 - INFO - Completed processing file: RC Welcome Email.pdf\n2025-04-29 02:10:02,786 - INFO - Processing file: RIN AD bridge measurement 2.11.25.pptx\n2025-04-29 02:10:02,786 - INFO - Added results for RC Welcome Email.pdf\n2025-04-29 02:10:03,304 - INFO - Completed processing file: RIN AD bridge measurement 2.11.25.pptx\n2025-04-29 02:10:03,305 - INFO - Processing file: RC Haven\u0027t Talked In A While Email.pdf\n2025-04-29 02:10:03,305 - INFO - Added results for RIN AD bridge measurement 2.11.25.pptx\n2025-04-29 02:10:04,206 - INFO - Reading PDF for file: /tmp/tmphzs6l5i5/document.pdf ...\n2025-04-29 02:10:43,660 - INFO - Completed processing file: RC Haven\u0027t Talked In A While Email.pdf\n2025-04-29 02:10:43,662 - INFO - Processing file: Rcomplete_Ambassador Services Brochure_Print_Savings Card T\u0026C Update.pdf\n2025-04-29 02:10:43,662 - INFO - Added results for RC Haven\u0027t Talked In A While Email.pdf\n2025-04-29 02:11:14,514 - INFO - Reading PDF for file: /tmp/tmpt8iw9rkn/document.pdf ...\n2025-04-29 02:12:41,819 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:41,833 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:41,844 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:41,855 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:41,865 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:41,876 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:41,886 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:41,895 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:41,904 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:41,914 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:41,923 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:41,933 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:41,943 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:41,952 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:41,961 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:41,971 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:41,981 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:41,990 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:41,999 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:42,008 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:42,018 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:42,027 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:42,037 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:42,047 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:42,056 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:42,065 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:42,075 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:42,084 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:42,093 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:42,102 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:42,111 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:42,119 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:42,128 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:42,136 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:42,145 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:42,153 - INFO - Executing LLM request for image description...\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "2025-04-29 02:12:42,162 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:42,171 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:42,179 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:42,188 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:42,196 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:42,204 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:42,212 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:42,221 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:42,229 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:42,237 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:42,245 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:42,253 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:42,261 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:42,270 - INFO - Executing LLM request for image description...\n2025-04-29 02:12:42,280 - INFO - Completed processing file: Rcomplete_Ambassador Services Brochure_Print_Savings Card T\u0026C Update.pdf\n2025-04-29 02:12:42,280 - INFO - Processing file: HCP Bridge Eligible Letter_B1U.pdf\n2025-04-29 02:12:42,280 - INFO - Added results for Rcomplete_Ambassador Services Brochure_Print_Savings Card T\u0026C Update.pdf\n2025-04-29 02:12:42,416 - WARNING - CropBox missing from /Page, defaulting to MediaBox\n2025-04-29 02:12:42,625 - WARNING - CropBox missing from /Page, defaulting to MediaBox\n2025-04-29 02:12:42,907 - WARNING - CropBox missing from /Page, defaulting to MediaBox\n2025-04-29 02:12:43,060 - INFO - Reading PDF for file: /tmp/tmp62y5kd_r/document.pdf ...\n2025-04-29 02:12:47,580 - WARNING - CropBox missing from /Page, defaulting to MediaBox\n2025-04-29 02:12:47,784 - WARNING - CropBox missing from /Page, defaulting to MediaBox\n2025-04-29 02:12:48,043 - WARNING - CropBox missing from /Page, defaulting to MediaBox\n2025-04-29 02:13:15,909 - INFO - Executing LLM request for image description...\n2025-04-29 02:13:15,921 - INFO - Executing LLM request for image description...\n2025-04-29 02:13:15,931 - INFO - Executing LLM request for image description...\n2025-04-29 02:13:15,941 - INFO - Executing LLM request for image description...\n2025-04-29 02:13:15,950 - INFO - Executing LLM request for image description...\n2025-04-29 02:13:15,959 - INFO - Executing LLM request for image description...\n2025-04-29 02:13:15,969 - INFO - Completed processing file: HCP Bridge Eligible Letter_B1U.pdf\n2025-04-29 02:13:15,970 - INFO - Processing file: RIN AD Bridge Program Optimization Starter Document.pptx\n2025-04-29 02:13:15,970 - INFO - Added results for HCP Bridge Eligible Letter_B1U.pdf\n2025-04-29 02:13:16,722 - INFO - Completed processing file: RIN AD Bridge Program Optimization Starter Document.pptx\n2025-04-29 02:13:16,723 - INFO - Processing file: RC Insurance Email - Peds.pdf\n2025-04-29 02:13:16,723 - INFO - Added results for RIN AD Bridge Program Optimization Starter Document.pptx\n2025-04-29 02:13:17,787 - INFO - Reading PDF for file: /tmp/tmp3tsfw3bv/document.pdf ...\n2025-04-29 02:13:47,266 - INFO - Completed processing file: RC Insurance Email - Peds.pdf\n2025-04-29 02:13:47,268 - INFO - Processing file: Patient Bridge Eligible Letter_B1O.docx\n2025-04-29 02:13:47,268 - INFO - Added results for RC Insurance Email - Peds.pdf\n2025-04-29 02:13:47,438 - INFO - Completed processing file: Patient Bridge Eligible Letter_B1O.docx\n2025-04-29 02:13:47,439 - INFO - Processing file: RC Welcome Email - Peds.pdf\n2025-04-29 02:13:47,439 - INFO - Added results for Patient Bridge Eligible Letter_B1O.docx\n2025-04-29 02:13:48,473 - INFO - Reading PDF for file: /tmp/tmp9f5em2t4/document.pdf ...\n2025-04-29 02:14:19,140 - INFO - Completed processing file: RC Welcome Email - Peds.pdf\n2025-04-29 02:14:19,141 - INFO - Processing file: RC Haven\u0027t Talked in a While Email - Peds.pdf\n2025-04-29 02:14:19,141 - INFO - Added results for RC Welcome Email - Peds.pdf\n2025-04-29 02:14:20,136 - INFO - Reading PDF for file: /tmp/tmpxhjxntg2/document.pdf ...\n2025-04-29 02:14:46,007 - INFO - Completed processing file: RC Haven\u0027t Talked in a While Email - Peds.pdf\n2025-04-29 02:14:46,008 - INFO - Processing file: Rcomplete Dr. Discussion Guide (Telemedicine Options) Print - CD LC.pdf\n2025-04-29 02:14:46,009 - INFO - Added results for RC Haven\u0027t Talked in a While Email - Peds.pdf\n2025-04-29 02:14:48,328 - INFO - Reading PDF for file: /tmp/tmphsks1891/document.pdf ...\n2025-04-29 02:15:20,973 - INFO - Executing LLM request for image description...\n2025-04-29 02:15:20,986 - INFO - Executing LLM request for image description...\n2025-04-29 02:15:20,996 - INFO - Executing LLM request for image description...\n2025-04-29 02:15:21,006 - INFO - Executing LLM request for image description...\n2025-04-29 02:15:21,016 - INFO - Executing LLM request for image description...\n2025-04-29 02:15:21,025 - INFO - Executing LLM request for image description...\n2025-04-29 02:15:21,034 - INFO - Executing LLM request for image description...\n2025-04-29 02:15:21,044 - INFO - Executing LLM request for image description...\n2025-04-29 02:15:21,054 - INFO - Executing LLM request for image description...\n2025-04-29 02:15:21,064 - INFO - Executing LLM request for image description...\n2025-04-29 02:15:21,075 - INFO - Completed processing file: Rcomplete Dr. Discussion Guide (Telemedicine Options) Print - CD LC.pdf\n2025-04-29 02:15:21,075 - INFO - Processing file: 2025-02-26-Disruption Workshop.pptx\n2025-04-29 02:15:21,076 - INFO - Added results for Rcomplete Dr. Discussion Guide (Telemedicine Options) Print - CD LC.pdf\n2025-04-29 02:15:22,185 - INFO - Completed processing file: 2025-02-26-Disruption Workshop.pptx\n2025-04-29 02:15:22,186 - INFO - Processing file: Pre Fill PA Summary 1.pptx\n2025-04-29 02:15:22,186 - INFO - Added results for 2025-02-26-Disruption Workshop.pptx\n2025-04-29 02:15:22,409 - INFO - Completed processing file: Pre Fill PA Summary 1.pptx\n2025-04-29 02:15:22,409 - INFO - Processing file: Rinvoq AD PE Tactics Business Design Executive Summaries.pptx\n2025-04-29 02:15:22,410 - INFO - Added results for Pre Fill PA Summary 1.pptx\n2025-04-29 02:15:23,992 - INFO - Completed processing file: Rinvoq AD PE Tactics Business Design Executive Summaries.pptx\n2025-04-29 02:15:23,993 - INFO - Processing file: RC Insurance Cover Email.pdf\n2025-04-29 02:15:23,993 - INFO - Added results for Rinvoq AD PE Tactics Business Design Executive Summaries.pptx\n2025-04-29 02:15:25,940 - INFO - Reading PDF for file: /tmp/tmp9ohgt7_0/document.pdf ...\n2025-04-29 02:15:57,530 - INFO - Completed processing file: RC Insurance Cover Email.pdf\n2025-04-29 02:15:57,531 - INFO - Processing file: RIN AD Patient Engagement System Use Case Overview.pptx\n2025-04-29 02:15:57,532 - INFO - Added results for RC Insurance Cover Email.pdf\n2025-04-29 02:15:57,758 - INFO - Completed processing file: RIN AD Patient Engagement System Use Case Overview.pptx\n2025-04-29 02:15:57,758 - INFO - Processing file: 2025-02-APS OS RIN AD.pptx\n2025-04-29 02:15:57,759 - INFO - Added results for RIN AD Patient Engagement System Use Case Overview.pptx\n2025-04-29 02:15:59,144 - INFO - Completed processing file: 2025-02-APS OS RIN AD.pptx\n2025-04-29 02:15:59,145 - INFO - Processing file: Gloria Notes.txt\n2025-04-29 02:15:59,145 - INFO - Added results for 2025-02-APS OS RIN AD.pptx\n2025-04-29 02:15:59,255 - INFO - Completed processing file: Gloria Notes.txt\n2025-04-29 02:15:59,256 - INFO - Processing file: ARS RC Appointment Reminder Email.pdf\n2025-04-29 02:15:59,256 - INFO - Added results for Gloria Notes.txt\n2025-04-29 02:16:00,145 - INFO - Reading PDF for file: /tmp/tmp0ny0ryb6/document.pdf ...\n2025-04-29 02:16:40,211 - INFO - Completed processing file: ARS RC Appointment Reminder Email.pdf\n2025-04-29 02:16:40,213 - INFO - Processing file: PSIT-Rinvoq-AD-Data Triggers and Personalization.xlsx\n2025-04-29 02:16:40,213 - INFO - Added results for ARS RC Appointment Reminder Email.pdf\n2025-04-29 02:16:40,340 - INFO - Processing Excel file: PSIT-Rinvoq-AD-Data Triggers and Personalization.xlsx\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "/data/dataiku/dss_data/code-envs/python/cdl-p-diku-psbts-py39-llm-env/lib64/python3.9/site-packages/pandas/compat/_optional.py:117: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  if distutils.version.LooseVersion(version) \u003c minimum_version:\n/data/dataiku/dss_data/code-envs/python/cdl-p-diku-psbts-py39-llm-env/lib/python3.9/site-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  other \u003d LooseVersion(other)\n2025-04-29 02:16:40,363 - INFO - Successfully processed Excel file: PSIT-Rinvoq-AD-Data Triggers and Personalization.xlsx\n2025-04-29 02:16:40,364 - INFO - Completed processing file: PSIT-Rinvoq-AD-Data Triggers and Personalization.xlsx\n2025-04-29 02:16:40,364 - INFO - Processing file: Pharmacy Services Call Flows.pdf\n2025-04-29 02:16:40,364 - INFO - Added results for PSIT-Rinvoq-AD-Data Triggers and Personalization.xlsx\n2025-04-29 02:16:40,441 - WARNING - CropBox missing from /Page, defaulting to MediaBox\n2025-04-29 02:16:40,515 - WARNING - CropBox missing from /Page, defaulting to MediaBox\n2025-04-29 02:16:40,561 - WARNING - CropBox missing from /Page, defaulting to MediaBox\n2025-04-29 02:16:40,637 - INFO - Reading PDF for file: /tmp/tmpc4uf4i23/document.pdf ...\n2025-04-29 02:16:43,544 - WARNING - CropBox missing from /Page, defaulting to MediaBox\n2025-04-29 02:16:43,620 - WARNING - CropBox missing from /Page, defaulting to MediaBox\n2025-04-29 02:16:43,666 - WARNING - CropBox missing from /Page, defaulting to MediaBox\n2025-04-29 02:16:51,070 - INFO - Executing LLM request for image description...\n2025-04-29 02:16:51,083 - INFO - Executing LLM request for image description...\n2025-04-29 02:16:51,093 - INFO - Executing LLM request for image description...\n2025-04-29 02:16:51,103 - INFO - Completed processing file: Pharmacy Services Call Flows.pdf\n2025-04-29 02:16:51,104 - INFO - Processing file: RC I Can\u0027t Reach You Email.pdf\n2025-04-29 02:16:51,104 - INFO - Added results for Pharmacy Services Call Flows.pdf\n2025-04-29 02:16:53,031 - INFO - Reading PDF for file: /tmp/tmpdfn1f3wg/document.pdf ...\n2025-04-29 02:17:24,734 - INFO - Executing LLM request for image description...\n2025-04-29 02:17:24,748 - INFO - Completed processing file: RC I Can\u0027t Reach You Email.pdf\n2025-04-29 02:17:24,749 - INFO - Processing file: Impacted Tactics 2.3.25.xlsx\n2025-04-29 02:17:24,749 - INFO - Added results for RC I Can\u0027t Reach You Email.pdf\n2025-04-29 02:17:24,819 - INFO - Processing Excel file: Impacted Tactics 2.3.25.xlsx\n/data/dataiku/dss_data/code-envs/python/cdl-p-diku-psbts-py39-llm-env/lib64/python3.9/site-packages/pandas/compat/_optional.py:117: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  if distutils.version.LooseVersion(version) \u003c minimum_version:\n/data/dataiku/dss_data/code-envs/python/cdl-p-diku-psbts-py39-llm-env/lib/python3.9/site-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  other \u003d LooseVersion(other)\n2025-04-29 02:17:24,857 - INFO - Successfully processed Excel file: Impacted Tactics 2.3.25.xlsx\n2025-04-29 02:17:24,858 - INFO - Completed processing file: Impacted Tactics 2.3.25.xlsx\n2025-04-29 02:17:24,859 - INFO - Processing file: Journey update 2025-02-19 at 2.34.51 PM.png\n2025-04-29 02:17:24,859 - INFO - Added results for Impacted Tactics 2.3.25.xlsx\n2025-04-29 02:17:24,923 - INFO - Reading image file: /tmp/tmpfbun03xz/document.pdf ...\n2025-04-29 02:17:32,051 - WARNING - Image Extraction Error: Skipping the failed image\nTraceback (most recent call last):\n  File \"/data/dataiku/dss_data/code-envs/python/cdl-p-diku-psbts-py39-llm-env/lib64/python3.9/site-packages/PIL/JpegImagePlugin.py\", line 666, in _save\n    rawmode \u003d RAWMODE[im.mode]\nKeyError: \u0027RGBA\u0027\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/data/dataiku/dss_data/code-envs/python/cdl-p-diku-psbts-py39-llm-env/lib/python3.9/site-packages/unstructured/partition/pdf_image/pdf_image_utils.py\", line 221, in save_elements\n    write_image(cropped_image, output_f_path)\n  File \"/data/dataiku/dss_data/code-envs/python/cdl-p-diku-psbts-py39-llm-env/lib/python3.9/site-packages/unstructured/partition/pdf_image/pdf_image_utils.py\", line 48, in write_image\n    image.save(output_image_path)\n  File \"/data/dataiku/dss_data/code-envs/python/cdl-p-diku-psbts-py39-llm-env/lib64/python3.9/site-packages/PIL/Image.py\", line 2581, in save\n    save_handler(self, fp, filename)\n  File \"/data/dataiku/dss_data/code-envs/python/cdl-p-diku-psbts-py39-llm-env/lib64/python3.9/site-packages/PIL/JpegImagePlugin.py\", line 669, in _save\n    raise OSError(msg) from e\nOSError: cannot write mode RGBA as JPEG\n2025-04-29 02:17:32,059 - INFO - Executing LLM request for image description...\n2025-04-29 02:17:32,070 - INFO - Completed processing file: Journey update 2025-02-19 at 2.34.51 PM.png\n2025-04-29 02:17:32,071 - INFO - Processing file: RComplete_Intro to AD Adol Brochure_Print_Savings Card T\u0026C Update (1).pdf\n2025-04-29 02:17:32,071 - INFO - Added results for Journey update 2025-02-19 at 2.34.51 PM.png\n2025-04-29 02:17:50,871 - INFO - Reading PDF for file: /tmp/tmp544ubxon/document.pdf ...\n2025-04-29 02:19:55,241 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,253 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,263 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,273 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,282 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,292 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,302 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,311 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,320 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,328 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,337 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,346 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,354 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,363 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,371 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,379 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,388 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,396 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,404 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,412 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,420 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,427 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,435 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,443 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,451 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,459 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,467 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,476 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,484 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,492 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,500 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,508 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,516 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,525 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,534 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,543 - INFO - Executing LLM request for image description...\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "2025-04-29 02:19:55,551 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,560 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,569 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,578 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,587 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,596 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,604 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,612 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,621 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,630 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,639 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,647 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,656 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,665 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,673 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,682 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,696 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,705 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,714 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,723 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,733 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,742 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,751 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,761 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,770 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,779 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,792 - INFO - Executing LLM request for image description...\n2025-04-29 02:19:55,802 - INFO - Completed processing file: RComplete_Intro to AD Adol Brochure_Print_Savings Card T\u0026C Update (1).pdf\n2025-04-29 02:19:55,802 - INFO - Processing file: ARS RC Transition to ARS Email - Peds.pdf\n2025-04-29 02:19:55,802 - INFO - Added results for RComplete_Intro to AD Adol Brochure_Print_Savings Card T\u0026C Update (1).pdf\n2025-04-29 02:19:56,665 - INFO - Reading PDF for file: /tmp/tmple0nilec/document.pdf ...\n2025-04-29 02:20:35,718 - INFO - Completed processing file: ARS RC Transition to ARS Email - Peds.pdf\n2025-04-29 02:20:35,720 - INFO - Processing file: 5_6_FAS CSS IS  Overview_Lunch \u0026 Learn.pptx\n2025-04-29 02:20:35,720 - INFO - Added results for ARS RC Transition to ARS Email - Peds.pdf\n2025-04-29 02:20:37,444 - INFO - Completed processing file: 5_6_FAS CSS IS  Overview_Lunch \u0026 Learn.pptx\n2025-04-29 02:20:37,445 - INFO - Processing file: Rinvoq AD PA Denied Discovery.pptx\n2025-04-29 02:20:37,445 - INFO - Added results for 5_6_FAS CSS IS  Overview_Lunch \u0026 Learn.pptx\n2025-04-29 02:20:39,075 - INFO - Completed processing file: Rinvoq AD PA Denied Discovery.pptx\n2025-04-29 02:20:39,075 - INFO - Processing file: RC myAbbVie Assist Interim Assistance Email.pdf\n2025-04-29 02:20:39,076 - INFO - Added results for Rinvoq AD PA Denied Discovery.pptx\n2025-04-29 02:20:41,102 - INFO - Reading PDF for file: /tmp/tmpzbz1vow0/document.pdf ...\n2025-04-29 02:21:22,101 - INFO - Completed processing file: RC myAbbVie Assist Interim Assistance Email.pdf\n2025-04-29 02:21:22,103 - INFO - Processing file: Treatment Plan Journey _ SIs with Nodes.pptx\n2025-04-29 02:21:22,103 - INFO - Added results for RC myAbbVie Assist Interim Assistance Email.pdf\n2025-04-29 02:21:22,613 - INFO - Completed processing file: Treatment Plan Journey _ SIs with Nodes.pptx\n2025-04-29 02:21:22,613 - INFO - Processing file: Complete App User in 2024 by Brand-Indication 01-24-25.docx\n2025-04-29 02:21:22,614 - INFO - Added results for Treatment Plan Journey _ SIs with Nodes.pptx\n2025-04-29 02:21:22,737 - INFO - Completed processing file: Complete App User in 2024 by Brand-Indication 01-24-25.docx\n2025-04-29 02:21:22,738 - INFO - Processing file: Patient Bridge - Commercial Coverage Letter_B2O.docx\n2025-04-29 02:21:22,738 - INFO - Added results for Complete App User in 2024 by Brand-Indication 01-24-25.docx\n2025-04-29 02:21:22,840 - INFO - Completed processing file: Patient Bridge - Commercial Coverage Letter_B2O.docx\n2025-04-29 02:21:22,840 - INFO - Processing file: Bridge Status field in iEngage-Quick Reference Guide.pptx\n2025-04-29 02:21:22,840 - INFO - Added results for Patient Bridge - Commercial Coverage Letter_B2O.docx\n2025-04-29 02:21:22,969 - INFO - Completed processing file: Bridge Status field in iEngage-Quick Reference Guide.pptx\n2025-04-29 02:21:22,970 - INFO - Processing file: Patient Bridge Eligible Letter_B1V.pdf\n2025-04-29 02:21:22,970 - INFO - Added results for Bridge Status field in iEngage-Quick Reference Guide.pptx\n2025-04-29 02:21:23,040 - WARNING - CropBox missing from /Page, defaulting to MediaBox\n2025-04-29 02:21:23,167 - INFO - Reading PDF for file: /tmp/tmpzfcer4i4/document.pdf ...\n2025-04-29 02:21:24,799 - WARNING - CropBox missing from /Page, defaulting to MediaBox\n2025-04-29 02:21:30,441 - INFO - Completed processing file: Patient Bridge Eligible Letter_B1V.pdf\n2025-04-29 02:21:30,442 - INFO - Processing file: RComplete_Kaiser Rebate Brochure_Print_Savings Card T\u0026C Update (1).pdf\n2025-04-29 02:21:30,443 - INFO - Added results for Patient Bridge Eligible Letter_B1V.pdf\n2025-04-29 02:21:30,519 - WARNING - CropBox missing from /Page, defaulting to MediaBox\n2025-04-29 02:21:31,601 - WARNING - CropBox missing from /Page, defaulting to MediaBox\n2025-04-29 02:21:31,847 - WARNING - CropBox missing from /Page, defaulting to MediaBox\n2025-04-29 02:21:32,376 - WARNING - CropBox missing from /Page, defaulting to MediaBox\n2025-04-29 02:21:33,736 - WARNING - CropBox missing from /Page, defaulting to MediaBox\n2025-04-29 02:21:34,206 - WARNING - CropBox missing from /Page, defaulting to MediaBox\n2025-04-29 02:21:34,686 - WARNING - CropBox missing from /Page, defaulting to MediaBox\n2025-04-29 02:21:34,768 - WARNING - CropBox missing from /Page, defaulting to MediaBox\n2025-04-29 02:21:34,784 - INFO - Reading PDF for file: /tmp/tmpg_h89p99/document.pdf ...\n2025-04-29 02:21:44,303 - WARNING - CropBox missing from /Page, defaulting to MediaBox\n2025-04-29 02:21:45,371 - WARNING - CropBox missing from /Page, defaulting to MediaBox\n2025-04-29 02:21:45,615 - WARNING - CropBox missing from /Page, defaulting to MediaBox\n2025-04-29 02:21:46,141 - WARNING - CropBox missing from /Page, defaulting to MediaBox\n2025-04-29 02:21:47,503 - WARNING - CropBox missing from /Page, defaulting to MediaBox\n2025-04-29 02:21:47,971 - WARNING - CropBox missing from /Page, defaulting to MediaBox\n2025-04-29 02:21:48,443 - WARNING - CropBox missing from /Page, defaulting to MediaBox\n2025-04-29 02:21:48,523 - WARNING - CropBox missing from /Page, defaulting to MediaBox\n2025-04-29 02:22:17,746 - INFO - Executing LLM request for image description...\n2025-04-29 02:22:17,759 - INFO - Executing LLM request for image description...\n2025-04-29 02:22:17,769 - INFO - Executing LLM request for image description...\n2025-04-29 02:22:17,778 - INFO - Executing LLM request for image description...\n2025-04-29 02:22:17,787 - INFO - Executing LLM request for image description...\n2025-04-29 02:22:17,797 - INFO - Executing LLM request for image description...\n2025-04-29 02:22:17,807 - INFO - Executing LLM request for image description...\n2025-04-29 02:22:17,817 - INFO - Executing LLM request for image description...\n2025-04-29 02:22:17,826 - INFO - Executing LLM request for image description...\n2025-04-29 02:22:17,835 - INFO - Executing LLM request for image description...\n2025-04-29 02:22:17,845 - INFO - Executing LLM request for image description...\n2025-04-29 02:22:17,854 - INFO - Executing LLM request for image description...\n2025-04-29 02:22:17,864 - INFO - Completed processing file: RComplete_Kaiser Rebate Brochure_Print_Savings Card T\u0026C Update (1).pdf\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "2025-04-29 02:22:17,865 - INFO - Processing file: Pharmacy Solutions Delivery Text Messages.xlsx\n2025-04-29 02:22:17,865 - INFO - Added results for RComplete_Kaiser Rebate Brochure_Print_Savings Card T\u0026C Update (1).pdf\n2025-04-29 02:22:17,934 - INFO - Processing Excel file: Pharmacy Solutions Delivery Text Messages.xlsx\n/data/dataiku/dss_data/code-envs/python/cdl-p-diku-psbts-py39-llm-env/lib64/python3.9/site-packages/pandas/compat/_optional.py:117: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  if distutils.version.LooseVersion(version) \u003c minimum_version:\n/data/dataiku/dss_data/code-envs/python/cdl-p-diku-psbts-py39-llm-env/lib/python3.9/site-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  other \u003d LooseVersion(other)\n2025-04-29 02:22:17,951 - INFO - Successfully processed Excel file: Pharmacy Solutions Delivery Text Messages.xlsx\n2025-04-29 02:22:17,951 - INFO - Completed processing file: Pharmacy Solutions Delivery Text Messages.xlsx\n2025-04-29 02:22:17,952 - INFO - Processing file: RINVOQ Complete SMS Wave 1 Doctor Follow-Up Use Case.pdf\n2025-04-29 02:22:17,952 - INFO - Added results for Pharmacy Solutions Delivery Text Messages.xlsx\n2025-04-29 02:22:18,420 - INFO - Reading PDF for file: /tmp/tmpjrn0oz1r/document.pdf ...\n2025-04-29 02:22:28,180 - INFO - Executing LLM request for image description...\n2025-04-29 02:22:28,193 - INFO - Executing LLM request for image description...\n2025-04-29 02:22:28,203 - INFO - Completed processing file: RINVOQ Complete SMS Wave 1 Doctor Follow-Up Use Case.pdf\n2025-04-29 02:22:28,204 - INFO - Processing file: 2024-12-2 Bridge Workshop Agenda and Insights.pptx\n2025-04-29 02:22:28,204 - INFO - Added results for RINVOQ Complete SMS Wave 1 Doctor Follow-Up Use Case.pdf\n2025-04-29 02:22:30,618 - INFO - Completed processing file: 2024-12-2 Bridge Workshop Agenda and Insights.pptx\n2025-04-29 02:22:30,619 - INFO - Processing file: ARTS RC I Can\u0027t Reach You Email.pdf\n2025-04-29 02:22:30,619 - INFO - Added results for 2024-12-2 Bridge Workshop Agenda and Insights.pptx\n2025-04-29 02:22:32,851 - INFO - Reading PDF for file: /tmp/tmp1e0z87a1/document.pdf ...\n2025-04-29 02:23:17,994 - INFO - Completed processing file: ARTS RC I Can\u0027t Reach You Email.pdf\n2025-04-29 02:23:17,996 - INFO - Processing file: Examples of Complete App Push Notifications and Pop-ups.pptx\n2025-04-29 02:23:17,996 - INFO - Added results for ARTS RC I Can\u0027t Reach You Email.pdf\n2025-04-29 02:23:18,215 - INFO - Completed processing file: Examples of Complete App Push Notifications and Pop-ups.pptx\n2025-04-29 02:23:18,215 - INFO - Processing file: RC Staying on Track Email (1).pdf\n2025-04-29 02:23:18,216 - INFO - Added results for Examples of Complete App Push Notifications and Pop-ups.pptx\n2025-04-29 02:23:19,463 - INFO - Reading PDF for file: /tmp/tmpmacxf18l/document.pdf ...\n2025-04-29 02:23:51,192 - INFO - Completed processing file: RC Staying on Track Email (1).pdf\n2025-04-29 02:23:51,194 - INFO - Processing file: 65J-1910403 HUMIRA Complete Symptom Tracker.pdf\n2025-04-29 02:23:51,194 - INFO - Added results for RC Staying on Track Email (1).pdf\n2025-04-29 02:23:55,196 - INFO - Reading PDF for file: /tmp/tmp2yr5ilzc/document.pdf ...\n2025-04-29 02:24:08,885 - INFO - Executing LLM request for image description...\n2025-04-29 02:24:08,897 - INFO - Executing LLM request for image description...\n2025-04-29 02:24:08,907 - INFO - Executing LLM request for image description...\n2025-04-29 02:24:08,917 - INFO - Completed processing file: 65J-1910403 HUMIRA Complete Symptom Tracker.pdf\n2025-04-29 02:24:08,917 - INFO - Processing file: Rcomplete_App Leave Behind_Print_Savings Card T\u0026C Update.pdf\n2025-04-29 02:24:08,918 - INFO - Added results for 65J-1910403 HUMIRA Complete Symptom Tracker.pdf\n2025-04-29 02:24:08,998 - WARNING - CropBox missing from /Page, defaulting to MediaBox\n2025-04-29 02:24:10,470 - WARNING - CropBox missing from /Page, defaulting to MediaBox\n2025-04-29 02:24:11,703 - WARNING - CropBox missing from /Page, defaulting to MediaBox\n2025-04-29 02:24:22,655 - INFO - Reading PDF for file: /tmp/tmpo77zjsgd/document.pdf ...\n2025-04-29 02:24:30,869 - WARNING - CropBox missing from /Page, defaulting to MediaBox\n2025-04-29 02:24:32,328 - WARNING - CropBox missing from /Page, defaulting to MediaBox\n2025-04-29 02:24:33,545 - WARNING - CropBox missing from /Page, defaulting to MediaBox\n2025-04-29 02:25:46,749 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:46,762 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:46,772 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:46,781 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:46,791 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:46,800 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:46,809 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:46,818 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:46,828 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:46,837 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:46,846 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:46,856 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:46,864 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:46,873 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:46,882 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:46,892 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:46,902 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:46,912 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:46,921 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:46,930 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:46,941 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:46,951 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:46,961 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:46,970 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:46,980 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:46,989 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:46,998 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:47,007 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:47,016 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:47,025 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:47,033 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:47,042 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:47,050 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:47,060 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:47,069 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:47,078 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:47,087 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:47,096 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:47,106 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:47,115 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:47,124 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:47,133 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:47,142 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:47,151 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:47,159 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:47,168 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:47,177 - INFO - Executing LLM request for image description...\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "2025-04-29 02:25:47,187 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:47,196 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:47,206 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:47,214 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:47,223 - INFO - Executing LLM request for image description...\n2025-04-29 02:25:47,232 - INFO - Completed processing file: Rcomplete_App Leave Behind_Print_Savings Card T\u0026C Update.pdf\n2025-04-29 02:25:47,233 - INFO - Processing file: ARS RC Intro to ARS Email (from ARS).pdf\n2025-04-29 02:25:47,233 - INFO - Added results for Rcomplete_App Leave Behind_Print_Savings Card T\u0026C Update.pdf\n2025-04-29 02:25:48,731 - INFO - Reading PDF for file: /tmp/tmpmcaiaqre/document.pdf ...\n2025-04-29 02:26:24,019 - INFO - Executing LLM request for image description...\n2025-04-29 02:26:24,032 - INFO - Executing LLM request for image description...\n2025-04-29 02:26:24,043 - INFO - Completed processing file: ARS RC Intro to ARS Email (from ARS).pdf\n2025-04-29 02:26:24,044 - INFO - Processing file: Rcomplete_Insurance Explained_Print_Savings Card T\u0026C Update.pdf\n2025-04-29 02:26:24,044 - INFO - Added results for ARS RC Intro to ARS Email (from ARS).pdf\n2025-04-29 02:32:54,752 - INFO - Reading PDF for file: /tmp/tmpbixcgqxu/document.pdf ...\n",
          "name": "stderr"
        }
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true
      },
      "source": [
        "#using hugging face"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        "input_folder \u003d dataiku.Folder(\"Input\")\npaths \u003d input_folder.list_paths_in_partition()\nfor path in paths[1:2]:\n    print(path)\n    with input_folder.get_download_stream(path) as f:\n        data \u003d f.read()\n        print(data)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        ""
      ],
      "outputs": []
    }
  ]
}