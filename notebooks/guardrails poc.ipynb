{
  "metadata": {
    "kernelspec": {
      "name": "py-dku-venv-guardrails",
      "display_name": "Python (env guardrails)",
      "language": "python"
    },
    "hide_input": false,
    "language_info": {
      "name": "python",
      "version": "3.9.20",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "customFields": {},
    "creator": "chaudpx2",
    "createdOn": 1744011764365,
    "tags": [],
    "modifiedBy": "chaudpx2"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%pylab inline"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\nfrom dataiku import pandasutils as pdu\nimport pandas as pd"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example: load a DSS dataset as a Pandas dataframe\nmydataset \u003d dataiku.Dataset(\"mydataset\")\nmydataset_df \u003d mydataset.get_dataframe()"
      ],
      "outputs": []
    },
    {
      "execution_count": 1,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Libraries\nimport dataiku\nimport pandas as pd\nimport numpy as np\nfrom langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI\nfrom langchain.vectorstores import FAISS, Chroma\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain.docstore.document import Document\nfrom langchain.schema import Document as LangChainDocument\nfrom langchain.embeddings.base import Embeddings\nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom langchain.retrievers.document_compressors import LLMChainExtractor\nimport pickle\nimport os\nfrom typing import List\nimport re\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom IliadLLMWrapper import LLMWrapper\nfrom IliadEmbeddingWrapper import DSSLLMEmbeddingWrapper\nfrom typing import List\n\n\"\"\"\nhost \u003d \"https://cdl-dku-dev-desi.commercial-datalake-prod.awscloud.abbvienet.com/\"\napiKey \u003d \"BDxYeBpzVM2ZMlFD9wEcUTkpsxfsPvxk\"\nos.environ[\"DKU_CURRENT_PROJECT_KEY\"] \u003d \"CDLADMIN\" \ndataiku.set_remote_dss(host, apiKey, no_check_certificate\u003dTrue)\n\"\"\"\n\nclass ModelDefination:\n    def __init__(self, embedding_model, llm, vector_store_type\u003d\"FAISS\", azure_openai_key\u003d\"Key\"):\n        # Store model parameters\n        self.embedding_model_name \u003d embedding_model\n        self.llm_model_name \u003d llm\n        self.vector_store_type \u003d vector_store_type.upper()  # Normalize to uppercase\n        self.embedding_model \u003d None\n        self.llm \u003d None\n        self.azure_openai_key \u003d azure_openai_key\n        self.client \u003d dataiku.api_client()\n        self.project \u003d self.client.get_default_project()\n\n        # Define paths for vector stores\n        self.faiss_index_path \u003d \"./faiss_index\"\n        self.chromadb_index_path \u003d \"./chromadb_index\"\n\n        # Initialize models\n        self._initialize_embedding_model()\n        self._initialize_llm()\n\n        if self.embedding_model is None:\n            raise ValueError(\"Embedding model could not be initialized\")\n        if self.llm is None:\n            raise ValueError(\"LLM could not be initialized\")\n\n    def _initialize_embedding_model(self):\n        try:\n            if self.embedding_model_name \u003d\u003d \"text-embedding-ada-002\":\n                client \u003d dataiku.api_client()\n                connection \u003d client.get_connection(\"text-embedding-ada-002\")\n                connection_params \u003d connection.get_info()[\"params\"]\n\n                available_deployments \u003d connection_params.get(\"availableDeployments\", [])\n                if not available_deployments:\n                    raise ValueError(\"No deployments found for embedding model.\")\n\n                self.embedding_deployment_name \u003d available_deployments[0][\"name\"]\n                model_name \u003d available_deployments[0][\"underlyingModelName\"]\n                azure_openai_endpoint \u003d f\"https://{connection_params[\u0027resourceName\u0027]}.openai.azure.com/\"\n\n                self.embedding_model \u003d AzureOpenAIEmbeddings(\n                    azure_endpoint\u003dazure_openai_endpoint,\n                    api_key\u003dconnection_params.get(\"apiKey\"),\n                    deployment\u003dself.embedding_deployment_name,\n                    model\u003dmodel_name,\n                    chunk_size\u003d1000\n                )\n                print(f\"Initialized embedding model: {model_name}\")\n\n            elif \"custom:iliad-plugin-conn-prod\" in self.embedding_model_name:\n                emb_model \u003d self.project.get_llm(self.embedding_model_name)\n                self.embedding_model \u003d DSSLLMEmbeddingWrapper(emb_model)\n                print(f\"Initialized custom embedding model: {self.embedding_model_name}\")\n\n        except Exception as e:\n            print(f\"Error initializing embedding model: {str(e)}\")\n            import traceback\n            traceback.print_exc()\n\n    def _initialize_llm(self):\n        try:\n            if self.llm_model_name \u003d\u003d \"gpt-35-turbo-16k\":\n                connection \u003d self.client.get_connection(\"gpt-35-turbo-16k-2\")\n                connection_params \u003d connection.get_info()[\"params\"]\n\n                available_deployments_llm \u003d connection_params.get(\"availableDeployments\", [])\n                if not available_deployments_llm:\n                    raise ValueError(\"No deployments found for LLM.\")\n\n                llm_deployment_name \u003d available_deployments_llm[0][\"name\"]\n                llm_model_name \u003d available_deployments_llm[0][\"underlyingModelName\"]\n                azure_llm_endpoint \u003d f\"https://{connection_params[\u0027resourceName\u0027]}.openai.azure.com/\"\n\n                self.llm \u003d AzureChatOpenAI(\n                    azure_endpoint\u003dazure_llm_endpoint,\n                    api_key\u003dconnection_params.get(\"apiKey\"),\n                    deployment_name\u003dllm_deployment_name,\n                    model_name\u003dllm_model_name,\n                    temperature\u003d0.1,\n                    api_version\u003d\"2024-02-01\"\n                )\n                print(f\"Initialized LLM: {llm_model_name}\")\n\n            elif \"custom:iliad-plugin-conn-prod\" in self.llm_model_name:\n                llm_model \u003d self.project.get_llm(self.llm_model_name)\n                self.llm \u003d LLMWrapper(llm_model)\n                print(f\"Initialized custom LLM: {self.llm_model_name}\")\n\n        except Exception as e:\n            print(f\"Failed to initialize LLM: {str(e)}\")\n            raise Exception(f\"Error initializing LLM: {str(e)}\")\n\n    def document_preparation(self, df):\n        \"\"\"\n        Prepare documents and create vector store index.\n        Supports both FAISS and ChromaDB.\n        \"\"\"\n        try:\n            if not isinstance(df, pd.DataFrame):\n                raise ValueError(\"The provided \u0027df\u0027 is not a pandas DataFrame\")\n\n            documents \u003d [\n                LangChainDocument(\n                    page_content\u003drow[\"chunk_text\"],\n                    metadata\u003d{\"id\": str(index), \"metadata\": row[\"metadata\"]}\n                )\n                for index, row in df.iterrows()\n            ]\n\n            # Fix here: Check for any case version of \"FAISS\"\n            if self.vector_store_type.upper() \u003d\u003d \"FAISS\":\n                store_path \u003d self.faiss_index_path\n                os.makedirs(store_path, exist_ok\u003dTrue)\n\n                if \"embeddings\" in df.columns:\n                    embeddings \u003d np.array([eval(embed) if isinstance(embed, str) else embed for embed in df[\"embeddings\"]])\n                    vectorstore \u003d FAISS.from_embeddings(\n                        text_embeddings\u003dzip(df[\"chunk_text\"], embeddings),\n                        embedding\u003dself.embedding_model,\n                        metadatas\u003d[{\"id\": str(index), \"metadata\": row[\"metadata\"]} for index, row in df.iterrows()]\n                    )\n                else:\n                    vectorstore \u003d FAISS.from_documents(documents, embedding\u003dself.embedding_model)\n\n                vectorstore.save_local(store_path)\n                print(\"FAISS index saved successfully.\")\n                return True\n\n            # Fix here: Check for any case version of \"CHROMADB\"\n            elif self.vector_store_type.upper() \u003d\u003d \"CHROMADB\":\n                store_path \u003d self.chromadb_index_path\n                os.makedirs(store_path, exist_ok\u003dTrue)\n\n                vectorstore \u003d Chroma.from_documents(documents, embedding\u003dself.embedding_model, persist_directory\u003dstore_path)\n                vectorstore.persist()\n                print(\"ChromaDB index saved successfully.\")\n                return True\n\n            else:\n                raise ValueError(f\"Unsupported vector store type: {self.vector_store_type}\")\n\n        except Exception as e:\n            print(f\"Failed to create index in Vector Store: {str(e)}\")\n            return False\n\n    def _compute_semantic_similarity(self, query_vector, document_vectors):\n        \"\"\"\n        Compute semantic similarity between query vector and document vectors.\n        \"\"\"\n        if not document_vectors:\n            return []\n\n        query_vector \u003d np.array(query_vector).reshape(1, -1)\n        document_vectors \u003d np.array(document_vectors)\n        return cosine_similarity(query_vector, document_vectors).flatten()\n\n    def _preprocess_query(self, query):\n        \"\"\"\n        Preprocess the query to extract key terms and concepts.\n        \"\"\"\n        stop_words \u003d {\u0027a\u0027, \u0027an\u0027, \u0027the\u0027, \u0027and\u0027, \u0027or\u0027, \u0027but\u0027, \u0027if\u0027, \u0027because\u0027, \u0027as\u0027, \u0027what\u0027, \u0027when\u0027, \u0027where\u0027, \u0027how\u0027, \u0027is\u0027, \u0027are\u0027, \u0027was\u0027, \u0027were\u0027}\n        query \u003d re.sub(r\u0027[^\\w\\s]\u0027, \u0027 \u0027, query.lower())\n        words \u003d query.split()\n        key_terms \u003d [word for word in words if word not in stop_words and len(word) \u003e 2]\n        entities \u003d re.findall(r\u0027\\b[A-Z][a-zA-Z]+\\b\u0027, query)\n        return {\"original\": query, \"key_terms\": key_terms, \"entities\": entities}\n\n    def _rerank_documents(self, query, documents, embedded_query\u003dNone):\n        \"\"\"\n        Rerank documents based on semantic and lexical relevance\n        \"\"\"\n        if not documents:\n            return []\n        \n        # Get query components\n        query_info \u003d self._preprocess_query(query)\n        key_terms \u003d query_info[\u0027key_terms\u0027]\n        \n        # Get embeddings for reranking if not provided\n        if embedded_query is None and hasattr(self.embedding_model, \u0027embed_query\u0027):\n            embedded_query \u003d self.embedding_model.embed_query(query)\n        \n        # Extract document embeddings\n        doc_embeddings \u003d []\n        for doc in documents:\n            if hasattr(doc, \u0027metadata\u0027) and \u0027embedding\u0027 in doc.metadata:\n                doc_embeddings.append(doc.metadata[\u0027embedding\u0027])\n            else:\n                # If no embedding in metadata, create one\n                if hasattr(self.embedding_model, \u0027embed_documents\u0027):\n                    doc_embedding \u003d self.embedding_model.embed_documents([doc.page_content])[0]\n                    doc_embeddings.append(doc_embedding)\n        \n        # Compute semantic similarity if we have embeddings\n        if embedded_query is not None and doc_embeddings:\n            semantic_scores \u003d self._compute_semantic_similarity(embedded_query, doc_embeddings)\n        else:\n            semantic_scores \u003d [0] * len(documents)\n        \n        # Compute lexical similarity (term frequency)\n        lexical_scores \u003d []\n        for doc in documents:\n            content \u003d doc.page_content.lower()\n            term_matches \u003d sum(1 for term in key_terms if term in content)\n            lexical_scores.append(term_matches / max(1, len(key_terms)))\n        \n        # Combine scores (0.7 semantic, 0.3 lexical)\n        combined_scores \u003d [0.7 * sem + 0.3 * lex for sem, lex in zip(semantic_scores, lexical_scores)]\n        \n        # Create result tuples with score and document\n        results \u003d [(score, doc) for score, doc in zip(combined_scores, documents)]\n        \n        # Sort by score (descending)\n        results.sort(reverse\u003dTrue, key\u003dlambda x: x[0])\n        \n        # Return only the documents with their scores\n        return [(doc, score) for score, doc in results]\n\n    def retrieve_relevant_chunks(self, df, query, top_k\u003d10):\n        \"\"\"\n        Retrieve relevant chunks based on query similarity with improved relevance\n        \"\"\"\n        try:\n            print(f\"Retrieving documents for query: {query[:50]}...\")\n\n            # Prepare documents and create index if needed\n            try:\n                index_created \u003d self.document_preparation(df)\n                if not index_created:\n                    print(f\"Warning: Failed to create document index, trying to use existing index...\")\n            except Exception as prep_error:\n                print(f\"Error during document preparation: {prep_error}\")\n                # Continue anyway, in case the index already exists\n\n            # Check if index exists\n            if not os.path.exists(self.faiss_index_path):\n                print(f\"Vector store index not found at {self.faiss_index_path}. Creating new index...\")\n                try:\n                    index_created \u003d self.document_preparation(df)\n                    if not index_created:\n                        raise FileNotFoundError(f\"Failed to create vector store index at {self.faiss_index_path}\")\n                except Exception as e:\n                    print(f\"Error creating index: {e}\")\n                    return []\n\n            # Load the vector store\n            try:\n                vectorstore \u003d FAISS.load_local(self.faiss_index_path, self.embedding_model, allow_dangerous_deserialization\u003dTrue)\n                print(f\"Successfully loaded FAISS vector store\")\n            except Exception as load_error:\n                print(f\"Error loading vector store: {load_error}\")\n                return []\n\n            # Create retriever\n            try:\n                # We get more than top_k for reranking\n                retriever \u003d vectorstore.as_retriever(\n                    search_type\u003d\"similarity\", \n                    search_kwargs\u003d{\"k\": top_k * 2}  # Get more for reranking\n                )\n                print(\"Successfully created retriever\")\n            except Exception as retriever_error:\n                print(f\"Error creating retriever: {retriever_error}\")\n                return []\n\n            # Make sure the query is embedded \n            embedded_query \u003d None\n            try:\n                if hasattr(self.embedding_model, \u0027embed_query\u0027):\n                    embedded_query \u003d self.embedding_model.embed_query(query)\n                elif hasattr(self.embedding_model, \u0027encode\u0027):\n                    embedded_query \u003d self.embedding_model.encode(query)\n            except Exception as embed_error:\n                print(f\"Warning: Failed to explicitly embed query: {embed_error}\")\n\n            # Retrieve relevant documents\n            try:\n                print(\"Attempting to retrieve documents...\")\n                # Try using the newer invoke method\n                if embedded_query is not None:\n                    print(\"Using pre-embedded query\")\n                    # For FAISS specific embedding search\n                    if hasattr(vectorstore, \u0027similarity_search_by_vector\u0027):\n                        initial_docs \u003d vectorstore.similarity_search_by_vector(embedded_query, k\u003dtop_k * 2)\n                    else:\n                        initial_docs \u003d retriever.invoke(query)\n                else:\n                    initial_docs \u003d retriever.invoke(query)\n\n                print(f\"Successfully retrieved {len(initial_docs)} documents for reranking\")\n            except Exception as invoke_error:\n                print(f\"Failed to use invoke method: {invoke_error}\")\n                # Fall back to the older method\n                try:\n                    if embedded_query is not None and hasattr(vectorstore, \u0027similarity_search_by_vector\u0027):\n                        initial_docs \u003d vectorstore.similarity_search_by_vector(embedded_query, k\u003dtop_k * 2)\n                    else:\n                        initial_docs \u003d retriever.get_relevant_documents(query)\n                    print(f\"Successfully retrieved {len(initial_docs)} documents using fallback method\")\n                except Exception as retrieve_error:\n                    print(f\"Failed to retrieve documents: {retrieve_error}\")\n                    return []\n\n            # Rerank the documents for better relevance\n            reranked_docs \u003d self._rerank_documents(query, initial_docs, embedded_query)\n            \n            # Take top k after reranking\n            top_docs \u003d reranked_docs[:top_k]\n\n            # Format the results\n            results \u003d []\n            for doc, score in top_docs:\n                result \u003d {\n                    \"chunk_text\": doc.page_content,\n                    \"metadata\": doc.metadata,\n                    \"score\": score\n                }\n                results.append(result)\n\n            return results\n\n        except Exception as e:\n            print(f\"Failed to retrieve data from Vector Store: {str(e)}\")\n            return []\n\n    def generate_llm_response(self, user_query, df, llm\u003dNone, return_context\u003dTrue):\n        \"\"\"\n        Generate LLM response based on retrieved chunks with improved relevance\n        \"\"\"\n        print(f\"Generating LLM response for query: {user_query[:50]}...\")\n\n        # Use the passed LLM if provided, otherwise use the instance\u0027s LLM\n        llm_to_use \u003d llm if llm is not None else self.llm\n\n        if llm_to_use is None:\n            return (\"Error: No LLM instance available\", []) if return_context else \"Error: No LLM instance available\"\n\n        try:\n            # Extract key terms from the user query for better retrieval\n            query_info \u003d self._preprocess_query(user_query)\n            key_terms \u003d query_info[\u0027key_terms\u0027]\n            \n            print(f\"Identified key terms: {key_terms}\")\n            \n            # Create an augmented query with the key terms emphasized\n            augmented_query \u003d user_query\n            if key_terms:\n                augmented_query \u003d f\"{user_query} [KEY TERMS: {\u0027, \u0027.join(key_terms)}]\"\n            \n            # Retrieve relevant chunks using the augmented query\n            relevant_chunks \u003d self.retrieve_relevant_chunks(df, augmented_query, top_k\u003d10)\n\n            if not relevant_chunks:\n                return (\n                    \"No relevant information found in the provided documents.\", \n                    []\n                ) if return_context else \"No relevant information found in the provided documents.\"\n\n            print(\"Retrieved Relevant Chunks:\", len(relevant_chunks))\n            for i, chunk in enumerate(relevant_chunks, 1):\n                print(f\"\\nResult {i}:\")\n                print(f\"Chunk Text: {chunk[\u0027chunk_text\u0027][:100]}...\")  # Print just the beginning\n                print(f\"Metadata: {chunk[\u0027metadata\u0027]}\")\n                if chunk[\"score\"] is not None:\n                    print(f\"Relevance Score: {chunk[\u0027score\u0027]:.4f}\")\n\n            # Prepare formatted context for prompt\n            formatted_chunks \u003d \"\"\n            for i, chunk in enumerate(relevant_chunks, 1):\n                formatted_chunks +\u003d f\"CHUNK {i}:\\n{chunk[\u0027chunk_text\u0027]}\\n\"\n                formatted_chunks +\u003d f\"Source: {chunk[\u0027metadata\u0027].get(\u0027source\u0027, \u0027Unknown\u0027)}\\n\\n\"\n\n\n            # Create the prompt with improved instruction\n            prompt \u003d f\"\"\"\n                You are an AI assistant that provides precise answers from provided document chunks. Follow these steps:\n\n                1. Carefully read all the document chunks provided.\n                2. Determine what specific detail the user is asking for in their query.\n                3. Find the most relevant information in the chunks that answers this query.\n                4. Extract and return a precise answer, citing the specific chunk used (e.g., \"According to Chunk X...\").\n                5. If no relevant information is found, state that the data is insufficient.\n\n                User Query:\n                {user_query}\n\n                Document Chunks:\n                {formatted_chunks}\n\n                Answer:\n                \"\"\"\n\n            print(\"Sending prompt to LLM...\")\n            # Try different methods to call the LLM based on what\u0027s available\n            try:\n                if hasattr(llm_to_use, \u0027invoke\u0027):\n                    print(\"Using invoke method\")\n                    response \u003d llm_to_use.invoke(prompt)\n                elif hasattr(llm_to_use, \u0027predict\u0027):\n                    print(\"Using predict method\")\n                    response \u003d llm_to_use.predict(prompt)\n                elif hasattr(llm_to_use, \u0027generate\u0027):\n                    print(\"Using generate method\")\n                    response_obj \u003d llm_to_use.generate(prompt\u003dprompt)\n                    response \u003d response_obj\n                else:\n                    print(\"Trying to call the LLM object directly\")\n                    response \u003d llm_to_use(prompt)\n\n                print(\"Successfully received LLM response\")\n\n                # If the response is a DSSLLMCompletionResponse, extract its text\n                if hasattr(response, \"text\") and response.text is not None:\n                    final_response \u003d response.text\n                # If response is a tuple or list, handle unpacking issues:\n                elif isinstance(response, (tuple, list)):\n                    try:\n                        # If more than two values are returned, simply take the first one\n                        final_response \u003d response[0]\n                    except Exception as e:\n                        print(f\"Error unpacking response tuple: {str(e)}\")\n                        final_response \u003d str(response)\n                else:\n                    final_response \u003d response\n\n                # Prepare context dictionary for return\n                context_dict \u003d {\n                    \"relevant_chunks\": relevant_chunks,\n                    \"formatted_context\": formatted_chunks,\n                    \"query\": user_query,\n                    \"key_terms\": key_terms\n                }\n\n                # Return based on return_context flag\n                if return_context:\n                    return final_response, context_dict\n                else:\n                    return final_response\n\n            except ValueError as ve:\n                # Specifically catch ValueError related to unpacking\n                print(f\"ValueError encountered: {ve}\")\n                error_msg \u003d f\"Error generating response: {ve}\"\n                return (error_msg, []) if return_context else error_msg\n\n        except Exception as e:\n            print(f\"Error in generate_llm_response: {str(e)}\")\n            error_msg \u003d f\"Error generating response: {str(e)}\"\n            return (error_msg, []) if return_context else error_msg"
      ],
      "outputs": []
    },
    {
      "execution_count": 6,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Libraries\nimport dataiku\nimport pandas as pd\nimport numpy as np\nfrom langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI\nfrom langchain.vectorstores import FAISS, Chroma\nfrom langchain.docstore.document import Document\nfrom langchain.schema import Document as LangChainDocument\nfrom langchain.embeddings.base import Embeddings\nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom langchain.retrievers.document_compressors import LLMChainExtractor\nfrom retriever import ModelDefination\nimport pickle\nimport os\nfrom typing import List\nimport re\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport json\n\"\"\"\nhost \u003d \"https://cdl-dku-dev-desi.commercial-datalake-prod.awscloud.abbvienet.com/\"\napiKey \u003d \"BDxYeBpzVM2ZMlFD9wEcUTkpsxfsPvxk\"\nos.environ[\"DKU_CURRENT_PROJECT_KEY\"] \u003d \"CDLADMIN\" \ndataiku.set_remote_dss(host, apiKey, no_check_certificate\u003dTrue)\n\"\"\"\n            \nclass VectorStoreGeneration:\n    def __init__(self, input_dataset_name, output_dataset_name, user_query, embedding_model, llm, vector_store_type, top_k, use_compression, azure_openai_key):\n        self.input_dataset_name \u003d input_dataset_name\n        self.output_dataset_name \u003d output_dataset_name\n        self.user_query \u003d user_query\n        \n        # Define the embedding model and LLM first\n        self.embedding_model_name \u003d embedding_model\n        self.llm_model_name \u003d llm\n        self.vector_store_type \u003d vector_store_type\n        \n        # Create the model definition object with the model names\n        self.ModelDef \u003d ModelDefination(\n            embedding_model\u003dself.embedding_model_name, \n            llm\u003dself.llm_model_name, \n            vector_store_type\u003dvector_store_type,\n            azure_openai_key\u003dazure_openai_key,\n        )\n        \n        self.top_k \u003d top_k\n        # We don\u0027t use compression as per the requirement\n        self.use_compression \u003d False\n\n    \n    def process(self):\n        try:\n            dataset \u003d dataiku.Dataset(self.input_dataset_name)\n            df \u003d dataset.get_dataframe()\n\n            # Generate LLM response and get context\n            llm_response, context_dict \u003d self.ModelDef.generate_llm_response(\n                self.user_query, \n                df, \n                self.ModelDef.llm\n            )\n\n            # Extract the formatted context from the context dictionary\n            formatted_context \u003d \"\"\n            if isinstance(context_dict, dict) and \"formatted_context\" in context_dict:\n                formatted_context \u003d context_dict[\"formatted_context\"]\n            elif isinstance(context_dict, dict) and \"relevant_chunks\" in context_dict:\n                # Create formatted context from relevant chunks if needed\n                chunks \u003d context_dict[\"relevant_chunks\"]\n                for i, chunk in enumerate(chunks, 1):\n                    formatted_context +\u003d f\"CHUNK {i}:\\n{chunk[\u0027chunk_text\u0027]}\\n\"\n                    if \u0027metadata\u0027 in chunk and isinstance(chunk[\u0027metadata\u0027], dict):\n                        formatted_context +\u003d f\"Source: {chunk[\u0027metadata\u0027].get(\u0027source\u0027, \u0027Unknown\u0027)}\\n\\n\"\n            \n\n            # Construct the final result with additional metadata\n            result \u003d {\n                \"question\": self.user_query,\n                \"answer\": llm_response,\n                \"contexts\": formatted_context,\n            }\n            # Use json.dumps with a default function that converts non-serializable objects to strings.\n            json_result \u003d json.dumps(result, default\u003dstr)\n            return json_result\n\n        except Exception as e:\n            print(f\"Failed to generate LLM response: {str(e)}\")\n            import traceback\n            traceback.print_exc()\n            return {\n                \"question\": self.user_query,\n                \"answer\": f\"Error: {str(e)}\",\n                \"contexts\": \"\"\n            }"
      ],
      "outputs": []
    },
    {
      "execution_count": 21,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from vector_store_creation import VectorStoreGeneration\ngenerator \u003d VectorStoreGeneration(\n    input_dataset_name\u003d\"data_embedded\",\n    output_dataset_name\u003d\"llm_response\",\n    user_query\u003d\"What details do CAB form has\",\n    embedding_model\u003d\"custom:iliad-plugin-conn-prod:text-embedding-ada-002\",\n    llm\u003d\"custom:iliad-plugin-conn-prod:gpt-4o\",\n    vector_store_type \u003d \"FAISS\",\n    top_k\u003d10,\n    use_compression\u003dFalse,\n    azure_openai_key\u003d\"key\"\n)\n\nresult \u003d generator.process()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "Initialized custom embedding model: custom:iliad-plugin-conn-prod:text-embedding-ada-002\nInitialized custom LLM: custom:iliad-plugin-conn-prod:gpt-4o\nGenerating LLM response for query: What details do CAB form has...\nIdentified key terms: [\u0027details\u0027, \u0027cab\u0027, \u0027form\u0027, \u0027has\u0027]\nRetrieving documents for query: What details do CAB form has [KEY TERMS: details, ...\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "WARNING:langchain_community.vectorstores.faiss:`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\nWARNING:langchain_community.vectorstores.faiss:`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "FAISS index saved successfully.\nSuccessfully loaded FAISS vector store\nSuccessfully created retriever\nAttempting to retrieve documents...\nUsing pre-embedded query\nSuccessfully retrieved 20 documents for reranking\nRetrieved Relevant Chunks: 10\n\nResult 1:\nChunk Text: ### Extracted Text:\nModel CAB form\nDate: 07/01/2024\nProject Information:\nBusiness Information:\n\n### ...\nMetadata: {\u0027id\u0027: \u002713\u0027, \u0027metadata\u0027: \u0027{\"file_name\": \"CAB Form.docx\", \"image_links\": \"[]\", \"chunk_id\": \"CAB Form.docx_chunk_1\", \"chunk_order\": 1}\u0027}\nRelevance Score: 0.7051\n\nResult 2:\nChunk Text: .\n2. Developed by: Mention the name or abbreviation of the team that developed the model. This ensur...\nMetadata: {\u0027id\u0027: \u002748\u0027, \u0027metadata\u0027: \u0027{\"file_name\": \"CDL MLOps - Best Practices for model operationalization.docx\", \"image_links\": \"[\\\u0027CDL MLOps - Best Practices for model operationalization_image11.png\\\u0027, \\\u0027CDL MLOps - Best Practices for model operationalization_image12.png\\\u0027, \\\u0027CDL MLOps - Best Practices for model operationalization_image13.png\\\u0027, \\\u0027CDL MLOps - Best Practices for model operationalization_image14.png\\\u0027, \\\u0027CDL MLOps - Best Practices for model operationalization_image15.png\\\u0027, \\\u0027CDL MLOps - Best Practices for model operationalization_image1.png\\\u0027, \\\u0027CDL MLOps - Best Practices for model operationalization_image2.png\\\u0027, \\\u0027CDL MLOps - Best Practices for model operationalization_image3.png\\\u0027, \\\u0027CDL MLOps - Best Practices for model operationalization_image4.png\\\u0027, \\\u0027CDL MLOps - Best Practices for model operationalization_image5.png\\\u0027, \\\u0027CDL MLOps - Best Practices for model operationalization_image6.png\\\u0027, \\\u0027CDL MLOps - Best Practices for model operationalization_image7.png\\\u0027, \\\u0027CDL MLOps - Best Practices for model operationalization_image8.png\\\u0027, \\\u0027CDL MLOps - Best Practices for model operationalization_image9.png\\\u0027, \\\u0027CDL MLOps - Best Practices for model operationalization_image10.png\\\u0027]\", \"chunk_id\": \"CDL MLOps - Best Practices for model operationalization.docx_chunk_2\", \"chunk_order\": 2}\u0027}\nRelevance Score: 0.6782\n\nResult 3:\nChunk Text: ### Extracted Text:\nModel intake form\nOwner: Luis Sandoval\nBusiness Unit: Abbvie Digital Lab\nDate: 5...\nMetadata: {\u0027id\u0027: \u0027173\u0027, \u0027metadata\u0027: \u0027{\"file_name\": \"MLOps intake form.docx\", \"image_links\": \"[]\", \"chunk_id\": \"MLOps intake form.docx_chunk_1\", \"chunk_order\": 1}\u0027}\nRelevance Score: 0.6679\n\nResult 4:\nChunk Text: ### Extracted Text:\nMLOps Access form\nDate:\nAzure Repo Requirements\nEKS Cluster Requirements\nRespons...\nMetadata: {\u0027id\u0027: \u0027167\u0027, \u0027metadata\u0027: \u0027{\"file_name\": \"MLOps Access form.docx\", \"image_links\": \"[]\", \"chunk_id\": \"MLOps Access form.docx_chunk_1\", \"chunk_order\": 1}\u0027}\nRelevance Score: 0.6610\n\nResult 5:\nChunk Text: .\nModel Intake Form\nLink: Model Intake Form\nMLOps team will take approx. \u000b2 weeks to operationalize ...\nMetadata: {\u0027id\u0027: \u0027295\u0027, \u0027metadata\u0027: \u0027{\"file_name\": \"MLOps Workshop.pptx\", \"image_links\": \"\", \"chunk_id\": \"MLOps Workshop.pptx_chunk_3\", \"chunk_order\": 3}\u0027}\nRelevance Score: 0.6503\n\nResult 6:\nChunk Text: ### Extracted Text:\nModel Post Deployment Document\nOwner:\nModel Name:\nDetails on model operationaliz...\nMetadata: {\u0027id\u0027: \u0027191\u0027, \u0027metadata\u0027: \u0027{\"file_name\": \"MLOps Post Deployment Document_v0.1aj.docx\", \"image_links\": \"[]\", \"chunk_id\": \"MLOps Post Deployment Document_v0.1aj.docx_chunk_1\", \"chunk_order\": 1}\u0027}\nRelevance Score: 0.6487\n\nResult 7:\nChunk Text: .\n2.1. Infrastructure Details\n2.2 Environmental Details\n2.3 Software Resources\nThe operations of the...\nMetadata: {\u0027id\u0027: \u0027277\u0027, \u0027metadata\u0027: \u0027{\"file_name\": \"System Operations Procedure - Botox Flat files.docx\", \"image_links\": \"[\\\u0027System Operations Procedure - Botox Flat files_image1.emf\\\u0027]\", \"chunk_id\": \"System Operations Procedure - Botox Flat files.docx_chunk_3\", \"chunk_order\": 3}\u0027}\nRelevance Score: 0.6451\n\nResult 8:\nChunk Text: .\n3. Brand: Include the name of the brand or product that the machine learning model is associated w...\nMetadata: {\u0027id\u0027: \u0027182\u0027, \u0027metadata\u0027: \u0027{\"file_name\": \"MLOPS Naming Conventions.docx\", \"image_links\": \"[\\\u0027MLOPS Naming Conventions_image11.png\\\u0027, \\\u0027MLOPS Naming Conventions_image7.png\\\u0027, \\\u0027MLOPS Naming Conventions_image8.png\\\u0027, \\\u0027MLOPS Naming Conventions_image9.png\\\u0027, \\\u0027MLOPS Naming Conventions_image10.png\\\u0027, \\\u0027MLOPS Naming Conventions_image12.png\\\u0027, \\\u0027MLOPS Naming Conventions_image13.png\\\u0027, \\\u0027MLOPS Naming Conventions_image14.png\\\u0027, \\\u0027MLOPS Naming Conventions_image1.png\\\u0027, \\\u0027MLOPS Naming Conventions_image6.png\\\u0027, \\\u0027MLOPS Naming Conventions_image2.png\\\u0027, \\\u0027MLOPS Naming Conventions_image3.png\\\u0027, \\\u0027MLOPS Naming Conventions_image4.png\\\u0027, \\\u0027MLOPS Naming Conventions_image5.png\\\u0027]\", \"chunk_id\": \"MLOPS Naming Conventions.docx_chunk_3\", \"chunk_order\": 3}\u0027}\nRelevance Score: 0.6175\n\nResult 9:\nChunk Text: ### Extracted Text:\nADL Consumer Portfolio Update\nConfidential and Proprietary Information provided ...\nMetadata: {\u0027id\u0027: \u0027172\u0027, \u0027metadata\u0027: \u0027{\"file_name\": \"ML Ops Portfolio Report_KS11302023.pptx\", \"image_links\": \"\", \"chunk_id\": \"ML Ops Portfolio Report_KS11302023.pptx_chunk_1\", \"chunk_order\": 1}\u0027}\nRelevance Score: 0.6171\n\nResult 10:\nChunk Text: .\n3. Brand: Include the name of the brand or product that the machine learning model is associated\nw...\nMetadata: {\u0027id\u0027: \u00275\u0027, \u0027metadata\u0027: \u0027{\"file_name\": \"AbbVie MLOPS - Best Practices.pdf\", \"image_links\": \"\", \"chunk_id\": \"AbbVie MLOPS - Best Practices.pdf_chunk_4\", \"chunk_order\": 4}\u0027}\nRelevance Score: 0.6144\nSending prompt to LLM...\nUsing invoke method\nSuccessfully received LLM response\n",
          "name": "stdout"
        }
      ]
    },
    {
      "execution_count": 22,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 22,
          "data": {
            "text/plain": "{\u0027question\u0027: \u0027What details do CAB form has\u0027,\n \u0027answer\u0027: \u0027According to Chunk 1, the CAB form includes the following details:\\n\\n- Model Name\\n- Model Owner\\n- Model Objective\\n- Model Type (Dataiku / Other)\\n- Start Date\\n- End Date\\n- Source Table\\n- Target Table\\n- Project Functional Area: Project Explanation\\n- Project Manager\\n- Business Stakeholder\\n- Business Sponsor\\n- Brand\\n- Architecture Diagram / Design Flow\u0027,\n \u0027contexts\u0027: \u0027CHUNK 1:\\n### Extracted Text:\\nModel CAB form\\nDate: 07/01/2024\\nProject Information:\\nBusiness Information:\\n\\n### Table Data:\\n**Model Name**: \\n**Model Owner**: \\n**Model Objective**: \\n**Model Type (Dataiku / Other)**: \\n**Start Date**: \\n**End Date**: \\n**Source table**: \\n**Target table**: \\n**Project Functional Area**: Project Explanation\\n**Project Manager**: \\n**Business Stake Holder**: \\n**Business Sponsor**: \\n**Brand**: \\n**Architecture Diagram / Design flow**:\\nSource: Unknown\\n\\nCHUNK 2:\\n.\\n2. Developed by: Mention the name or abbreviation of the team that developed the model. This ensures accountability and makes it easier to identify the creators of each model.\\n3. Brand: Include the name of the brand or product that the machine learning model is associated with. This component helps in quickly understanding the context of the model.\\n4. Indication: Specify the primary medical indication or problem domain that the model addresses. This information provides clarity about the intended application of the model.\\n5. Model Name: Give a descriptive name to the machine learning model that reflects its core functionality or main algorithm used.\\nII. Dataset naming conventions\\nFormat - ${projectKey}_table_outcome_tbl\\nExample - MLOPS_LAB_PSIT_HUMIRA_ALL_ACCMAX_claims_data_tbl\\nNote: Each Dataiku project has a default variable called ${projectkey} which can be used in any recipe/connection object.\\nIII\\nSource: Unknown\\n\\nCHUNK 3:\\n### Extracted Text:\\nModel intake form\\nOwner: Luis Sandoval\\nBusiness Unit: Abbvie Digital Lab\\nDate: 5/9/2024\\nGeneral Information:\\nData Access, Connections and Onboarding requirements\\nDeployment Infrastructure planning\\nModel Operationalization details\\nSource: Unknown\\n\\nCHUNK 4:\\n### Extracted Text:\\nMLOps Access form\\nDate:\\nAzure Repo Requirements\\nEKS Cluster Requirements\\nResponse from MLOps Team\\nFor EKS Cluster/Template\\nFor Azure Repo\\nAppendix\\nAD Group Mapping\\nCluster Configuration Details\\nSource: Unknown\\n\\nCHUNK 5:\\n.\\nModel Intake Form\\nLink: Model Intake Form\\nMLOps team will take approx. \\x0b2 weeks to operationalize the model.\\nIntegration of the intake form with CDL system is in progress for seamless execution\\nMLOps Enablement\\nProject General Information\\nProject General Information\\nReq. Data Sources\\nFeature Engineering details\\nCode environment and Infrastructure details\\nType of Model\\nModel evaluation Criteria\\nInference/ Retraining Dependencies\\nStakeholder details\\nModel Intake Form\\nMLOps Alerting Mechanism\\x0b\\x0b(to proactively communicate insights about model run status \u0026 performance)\\nDetailed performance reports and outputs\\nOverall scheduled model run status\\nData disparity diagnosis reports. Useful in case the model run fails\\nSource: Unknown\\n\\nCHUNK 6:\\n### Extracted Text:\\nModel Post Deployment Document\\nOwner:\\nModel Name:\\nDetails on model operationalization:\\nFor more information on MLOPs process please refer: MLOps Site\\n\\n### Table Data:\\n**Category**: Details\\n**Project Name (with Link)**: \\n**Project Platform\\n(Dataiku/ LiveRamp/ Others)**: \\n**Final output table location (with Link)**: \\n**Cadence of the Model**: \\n**Model location in Prod environment (with Link)**: \\n**Data sources platform\\n(CDL/ Snowflake/ Flat files / CWS/ Others)**: \\n**CWS to Managed DB mapping in production**: \\n**MLOps Utilities integrated**: \\n**Scheduled Email Notifications on model execution**: \\n**SonarQube Dashboard Link**: \\n**Stakeholder configured to be notified**: \\n**Downstream Integration**: \\n**Any dependency linked to model execution (Data/ other Model/ Upstream dependency/ others)**:\\nSource: Unknown\\n\\nCHUNK 7:\\n.\\n2.1. Infrastructure Details\\n2.2 Environmental Details\\n2.3 Software Resources\\nThe operations of the platform require the usage of following software resources -\\nPutty/SSH Client – to SSH into the Edge Node\\nWinSCP – to transfer files between windows and edge node\\nNotepad++\\nWeb Browser (Google Chrome/Internet Explorer)\\nMS Word and Excel\\nPyCharm\\n3 Utility Details\\n3.1 Pre-requisites\\nThe following Databases needs write access for service account to write/update tables\\n\"abv_hcp360_digital_lab_refined\"\\n\"abv_hcp360_digital_lab_raw\"\\n\"abv_hcp360_digital_lab_work\"\\n\"abv_hcp360_digital_lab\"\\n3\\nSource: Unknown\\n\\nCHUNK 8:\\n.\\n3. Brand: Include the name of the brand or product that the machine learning model is associated with. This component helps in quickly understanding the context of the model.\\n4. Indication: Specify the primary medical indication or problem domain that the model addresses. This information provides clarity about the intended application of the model.\\n5. Model Name: Give a descriptive name to the machine learning model that reflects its core functionality or main algorithm used.\\n6. Month: Indicate the month in which the model development was completed or the model was last updated.\\n7. Year: Indicate the year in which the model development was completed or the model was last updated.\\nNote: The intermediary tables being used in the dataiku project would have the following naming convention:\\ndatabaseName . projectKey _ tableName\\nC. Benefits of the Naming Convention:\\n1\\nSource: Unknown\\n\\nCHUNK 9:\\n### Extracted Text:\\nADL Consumer Portfolio Update\\nConfidential and Proprietary Information provided for internal purposes only\\n2\\nCompleted\\nOn Track\\nAt Risk\\nLate\\nML Ops STATUS UPDATE\\n3\\nCompleted\\nOn Track\\nAt Risk\\nLate\\nML Ops STATUS UPDATE\\n4\\nCompleted\\nOn Track\\nAt Risk\\nLate\\nML Ops STATUS UPDATE\\n5\\nCompleted\\nOn Track\\nAt Risk\\nLate\\nML Ops STATUS UPDATE\\n6\\nCompleted\\nOn Track\\nAt Risk\\nLate\\nML Ops STATUS UPDATE\\n7\\nCompleted\\nOn Track\\nAt Risk\\nLate\\nML Ops STATUS UPDATE\\n8\\nCompleted\\nOn Track\\nAt Risk\\nLate\\nML Ops STATUS UPDATE\\nBusiness Owner: Jaime Date\\nBTS Owner: Rupesh Pagare\\nML Ops\\n\\x0bExecutive Summary Report\\nKey\\nWeek of 10/30\\nBusiness Owner: Jaime Date\\nBTS Owner: Rupesh Pagare\\nML Ops\\n\\x0bExecutive Summary Report\\nKey\\nWeek of 10/23\\nBusiness Owner: Jaime Date\\nBTS Owner: Rupesh Pagare\\nML Ops\\n\\x0bExecutive Summary Report\\nKey\\nWeek of 10/30\\nBusiness Owner: Jaime Date\\nBTS Owner: Rupesh Pagare\\nML Ops\\n\\x0bExecutive Summary Report\\nKey\\nWeek of 10/16\\nBusiness Owner: Jaime Date\\nBTS Owner: Rupesh Pagare\\nML Ops\\n\\x0bExecutive Summary Report\\nKey\\nWeek of 10/09\\nBusiness Owner: Jaime Date\\nBTS Owner: Rupesh Pagare\\nML Ops\\n\\x0bExecutive Summary Report\\nKey\\nWeek of 10/02\\nBusiness Owner: Jaime Date\\nBTS Owner: Rupesh Pagare\\nML Ops\\n\\x0bExecutive Summary Report\\nKey\\nWeek of 09/25\\nBusiness Owner: Jaime Date\\nBTS Owner: Rupesh Pagare\\nML Ops\\n\\x0bExecutive Summary Report\\nKey\\nWeek of 09/18\\nSource: Unknown\\n\\nCHUNK 10:\\n.\\n3. Brand: Include the name of the brand or product that the machine learning model is associated\\nwith. This component helps in quickly understanding the context of the model.\\n4. Indication: Specify the primary medical indication or problem domain that the model\\naddresses. This information provides clarity about the intended application of the model.\\n5. Model Name: Give a descriptive name to the machine learning model that reflects its core\\nfunctionality or main algorithm used.\\n6. Month: Indicate the month in which the model development was completed, or the model\\nwas last updated.\\n7. Year: Indicate the year in which the model development was completed, or the model was last\\nupdated.\\nNote: The intermediary tables being used in the dataiku project would have the following naming\\nconvention:\\ndatabaseName . projectKey _ tableName (based on outcome)\\n3\\nC. Benefits of the Naming Convention:\\n1\\nSource: Unknown\\n\\n\u0027}"
          },
          "metadata": {}
        }
      ]
    },
    {
      "execution_count": 4,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "type(result[answer])"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "dict"
          },
          "metadata": {}
        }
      ]
    },
    {
      "execution_count": 53,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#response \u003d result[\u0027answer\u0027]\nresponse \u003d \u0027According to Chunk 1, then the CAB form includes the following details:\\n\\n- Model Name\\n- Model Owner\\n- Model Objective\\n- Model Type (Dataiku / Other)\\n- Start Date\\n- End Date\\n- Source Table\\n- Target Table\\n- Project Functional Area: Project Explanation\\n- Project Manager\\n- Business Stakeholder\\n- Business Sponsor\\n- Brand\\n- Architecture Diagram / Design Flow\u0027"
      ],
      "outputs": []
    },
    {
      "execution_count": 54,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from transformers import pipeline"
      ],
      "outputs": []
    },
    {
      "execution_count": 55,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "toxic_classifier \u003d pipeline(\"text-classification\", model\u003d\"unitary/toxic-bert\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 56,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "results \u003d toxic_classifier(response)"
      ],
      "outputs": []
    },
    {
      "execution_count": 57,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "is_toxic \u003d any(r[\u0027label\u0027].lower() \u003d\u003d \u0027toxic\u0027 and r[\u0027score\u0027] \u003e 0.2 for r in results)\n\nif is_toxic:\n    print(\"Toxic content detected!\")\nelse:\n    print(results)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "[{\u0027label\u0027: \u0027toxic\u0027, \u0027score\u0027: 0.0005730493576265872}]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "execution_count": 58,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "results"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 58,
          "data": {
            "text/plain": "[{\u0027label\u0027: \u0027toxic\u0027, \u0027score\u0027: 0.0005730493576265872}]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    }
  ]
}