{
  "metadata": {
    "kernelspec": {
      "name": "py-dku-venv-cdl-p-diku-psbts-py39-llm-env",
      "display_name": "Python (env cdl-p-diku-psbts-py39-llm-env)",
      "language": "python"
    },
    "hide_input": false,
    "language_info": {
      "name": "python",
      "version": "3.9.20",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "modifiedBy": "tomarsx1",
    "customFields": {},
    "creator": "tomarsx1",
    "tags": [],
    "createdOn": 1744975944819
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%pylab inline"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\nfrom dataiku import pandasutils as pdu\nimport pandas as pd"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset \u003d dataiku.Dataset(\"input_data_extracted_custom\")\nhelp(dataset)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\nclient \u003d dataiku.api_client()\nproject \u003d client.get_default_project()\nconnection_name \u003d \"iliad-plugin-conn-prod\" \nconnection \u003d client.get_connection(connection_name)\nconnection_info \u003d connection.get_info()\nconnection_params \u003d connection_info[\"params\"]\nmodels \u003d connection_params[\u0027models\u0027]\nfor model in models:\n    print(f\"{model[\u0027capability\u0027]} {model} \\n\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# test"
      ]
    },
    {
      "execution_count": 1,
      "cell_type": "code",
      "metadata": {
        "code_folding": [
          562,
          706,
          851
        ]
      },
      "source": [
        "\"\"\"\nModular LangChain-based RAG System\n\"\"\"\nimport uuid\nimport os\nimport re\nimport json\nfrom typing import List, Dict, Any, Optional, Tuple,Union\n\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport dataiku\nfrom PIL import Image\nimport io\nimport pickle\nimport hashlib\n\n#from langchain.retrievers import MultiVectorRetriever\n#from langchain.storage import InMemoryStore\nfrom langchain_community.vectorstores import Chroma, FAISS\nfrom langchain_core.documents import Document\nfrom langchain.retrievers.document_compressors import EmbeddingsFilter\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain.retrievers.multi_vector import MultiVectorRetriever, SearchType\nfrom langchain_core.stores import InMemoryStore\nfrom langchain.storage import LocalFileStore \n\n\n# Import ChromaDB monkeypatch to ensure compatibility\nfrom dataiku.core.vector_stores.chroma_vector_store import ChromaVectorStore\nChromaVectorStore.run_the_ugly_chromadb_monkeypatch()\n\n\nclass ModelManager:\n    \"\"\"\n    Class to handle model initialization and management\n    \"\"\"\n    def __init__(\n        self, \n        embedding_model_id: str,\n        llm_id: str,\n        client\u003dNone\n    ):\n        self.client \u003d client or dataiku.api_client()\n        self.project \u003d self.client.get_default_project()\n        self.embedding_model_id \u003d embedding_model_id\n        self.llm_id \u003d llm_id\n        self.embedding_model \u003d None\n        self.llm \u003d None\n        \n        # Initialize models\n        self._initialize_embedding_model()\n        self._initialize_llm()\n    \n    def _initialize_embedding_model(self):\n        \"\"\"Initialize the embedding model with LangChain compatibility\"\"\"\n        try:\n            # Get the embedding model from the project and use the LangChain wrapper\n            emb_model \u003d self.project.get_llm(self.embedding_model_id)\n            self.embedding_model \u003d emb_model.as_langchain_embeddings()\n            # Set a smaller batch size if the model supports it\n            if hasattr(self.embedding_model, \"chunk_size\"):\n                self.embedding_model.chunk_size \u003d 1000  # Decrease from default\n            print(f\"Initialized LangChain embedding model: {self.embedding_model_id}\")\n        except Exception as e:\n            print(f\"Error initializing embedding model: {str(e)}\")\n            import traceback\n            traceback.print_exc()\n            raise\n\n    def _initialize_llm(self):\n        \"\"\"Initialize the LLM with LangChain compatibility\"\"\"\n        try:\n            # Get the LLM model from the project and use the LangChain wrapper\n            llm_model \u003d self.project.get_llm(self.llm_id)\n            self.llm \u003d llm_model.as_langchain_llm()\n            print(f\"Initialized LangChain LLM: {self.llm_id}\")\n        except Exception as e:\n            print(f\"Failed to initialize LLM: {str(e)}\")\n            import traceback\n            traceback.print_exc()\n            raise\n\n    def get_embedding_model(self):\n        \"\"\"Get the initialized embedding model\"\"\"\n        if not self.embedding_model:\n            raise ValueError(\"Embedding model not initialized\")\n        return self.embedding_model\n        \n    def get_llm(self):\n        \"\"\"Get the initialized LLM\"\"\"\n        if not self.llm:\n            raise ValueError(\"LLM not initialized\")\n        return self.llm\n\n\nclass ContentSummarizer:\n    \"\"\"\n    Class to handle summarization of different content types\n    \"\"\"\n    def __init__(self, llm):\n        self.llm \u003d llm\n        self.text_summary_prompt \u003d PromptTemplate(\n            input_variables\u003d[\"content\"],\n            template\u003d\"\"\"\n            Summarize the following text in a concise manner that captures the key information:\n            \n            {content}\n            \n            Summary:\n            \"\"\"\n        )\n        \n        self.table_summary_prompt \u003d PromptTemplate(\n            input_variables\u003d[\"content\"],\n            template\u003d\"\"\"\n            Analyze the following table data and provide a concise summary of what it contains:\n            \n            {content}\n            \n            Table Summary:\n            \"\"\"\n        )\n    \n    def summarize_text(self, content: str, max_length: int \u003d 4000) -\u003e str:\n        \"\"\"Generate a summary for text content\"\"\"\n        if not content:\n            return \"\"\n        \n        try:\n            # Truncate content to avoid token limit issues\n            truncated_content \u003d content[:max_length]\n            chain \u003d self.text_summary_prompt | self.llm\n            return chain.invoke({\"content\": truncated_content})\n        except Exception as e:\n            print(f\"Error generating text summary: {str(e)}\")\n            return content[:200] + \"...\"  # Fallback to simple truncation\n    \n    def summarize_table(self, content: str, max_length: int \u003d 4000) -\u003e str:\n        \"\"\"Generate a summary for table content\"\"\"\n        if not content:\n            return \"\"\n            \n        try:\n            # Truncate content to avoid token limit issues\n            truncated_content \u003d content[:max_length]\n            chain \u003d self.table_summary_prompt | self.llm\n            return chain.invoke({\"content\": truncated_content})\n        except Exception as e:\n            print(f\"Error generating table summary: {str(e)}\")\n            return content[:200] + \"...\"  # Fallback to simple truncation\n    \n    def process_image_with_llm(self, image_data: bytes) -\u003e Tuple[str, str]:\n        \"\"\"\n        Process an image using multimodal LLM to get detailed description and summary\n        \n        Args:\n            image_data: Raw image bytes\n            \n        Returns:\n            Tuple[str, str]: (description, summary)\n        \"\"\"\n        try:\n            # For now, we are using a placeholder since the specific multimodal handling\n            # would depend on the actual LLM implementation\n            print(\"Warning: Generic image processing used. May need adaptation for specific models.\")\n            \n            # This is a placeholder - in an actual implementation, we would:\n            # 1. Convert the LLM to a multimodal format if supported\n            # 2. Pass the image data properly formatted to the model\n            description \u003d \"Image description not available in this implementation.\"\n            summary \u003d \"Image summary not available in this implementation.\"\n            \n            return description, summary\n            \n        except Exception as e:\n            print(f\"Error processing image: {str(e)}\")\n            return f\"Failed to process image: {str(e)}\", \"Image processing error\"\n\n\nclass TextProcessor:\n    \"\"\"\n    Class to handle text processing functions like extracting tables from text\n    \"\"\"\n    def __init__(self):\n        self.text_splitter \u003d RecursiveCharacterTextSplitter(\n            chunk_size\u003d1000,\n            chunk_overlap\u003d100\n        )\n    \n    def split_text(self, text: str) -\u003e List[Document]:\n        \"\"\"Split text into chunks\"\"\"\n        return self.text_splitter.create_documents([text])\n    \n    def extract_table_from_text(self, text: str) -\u003e Tuple[bool, str]:\n        \"\"\"Extract table structure from text if present\"\"\"\n        # Simple detection of table patterns\n        has_table \u003d False\n        table_content \u003d \"\"\n        \n        # Check for markdown tables\n        if \"|\" in text and \"-|-\" in text:\n            has_table \u003d True\n            lines \u003d text.strip().split(\"\\n\")\n            table_lines \u003d []\n            for line in lines:\n                if \"|\" in line:\n                    table_lines.append(line)\n            table_content \u003d \"\\n\".join(table_lines)\n        \n        # Check for CSV-like content\n        elif text.count(\",\") \u003e 5 and \"\\n\" in text:\n            comma_counts \u003d [line.count(\",\") for line in text.split(\"\\n\") if line.strip()]\n            if len(comma_counts) \u003e 1 and max(comma_counts) \u003d\u003d min(comma_counts) and max(comma_counts) \u003e 0:\n                has_table \u003d True\n                table_content \u003d text\n        \n        return has_table, table_content\n\n\nclass ImageManager:\n    \"\"\"\n    Class to handle image-related operations\n    \"\"\"\n    def __init__(self, image_folder_name: Optional[str] \u003d None):\n        self.image_folder_name \u003d image_folder_name\n        self.image_folder \u003d None\n        \n        # Initialize image folder if provided\n        if image_folder_name:\n            self.image_folder \u003d dataiku.Folder(image_folder_name)\n    \n    def find_image_for_document(self, doc_id: str, doc_metadata: Dict) -\u003e Optional[str]:\n        \"\"\"\n        Find the corresponding image filename for a document based on ID or metadata\n        Returns the image filename if found, None otherwise\n        \"\"\"\n        if not self.image_folder:\n            return None\n            \n        try:\n            # Get list of files in the folder\n            image_files \u003d self.image_folder.list_paths_in_partition()\n            \n            # First try to find exact match based on document ID\n            doc_id_clean \u003d re.sub(r\u0027[^\\w]\u0027, \u0027_\u0027, str(doc_id))\n            for img_file in image_files:\n                if doc_id_clean in img_file:\n                    return img_file\n            \n            # Try to match using source or metadata filename\n            source \u003d doc_metadata.get(\"source\", \"\")\n            if source:\n                source_clean \u003d re.sub(r\u0027[^\\w]\u0027, \u0027_\u0027, os.path.basename(source).split(\u0027.\u0027)[0])\n                for img_file in image_files:\n                    if source_clean in img_file:\n                        return img_file\n            \n            return None\n        except Exception as e:\n            print(f\"Error finding image for document {doc_id}: {str(e)}\")\n            return None\n    \n    def get_image_data(self, image_filename: str) -\u003e Optional[bytes]:\n        \"\"\"Get image data from the Dataiku folder\"\"\"\n        if not self.image_folder:\n            return None\n        \n        try:\n            with self.image_folder.get_download_stream(image_filename) as stream:\n                return stream.read()\n        except Exception as e:\n            print(f\"Error reading image {image_filename}: {str(e)}\")\n            return None\n\n\nclass VectorStoreManager:\n    \"\"\"\n    Class to handle vector store creation and management with improved chunking\n    \"\"\"\n    def __init__(\n        self,\n        embedding_model,\n        store_type: str \u003d \"CHROMADB\",\n        persist_directory: str \u003d None,\n        dataiku_folder_name: str \u003d None\n    ):\n        self.embedding_model \u003d embedding_model\n        self.store_type \u003d store_type.upper()\n        self.dataiku_folder_name \u003d dataiku_folder_name\n        self.vector_store \u003d None\n        self.using_dataiku_folder \u003d dataiku_folder_name is not None\n\n        # Use Dataiku folder if provided, otherwise use local directory\n        if dataiku_folder_name:\n            print(f\"Using Dataiku folder: {dataiku_folder_name}\")\n            try:\n                self.dataiku_folder \u003d dataiku.Folder(dataiku_folder_name)\n                self.persist_directory \u003d self.dataiku_folder.get_path()\n                print(f\"Dataiku folder path: {self.persist_directory}\")\n                \n                # List folder contents for debugging\n                try:\n                    files \u003d self.dataiku_folder.list_paths_in_partition()\n                    print(f\"Dataiku folder contents: {files}\")\n                except Exception as e:\n                    print(f\"Error listing Dataiku folder contents: {str(e)}\")\n            except Exception as e:\n                print(f\"Error accessing Dataiku folder: {str(e)}\")\n                # Fallback to local directory\n                self.persist_directory \u003d persist_directory or \"./vector_store\"\n                print(f\"Falling back to local directory: {self.persist_directory}\")\n        else:\n            self.persist_directory \u003d persist_directory or \"./vector_store\"\n            print(f\"Using local directory: {self.persist_directory}\")\n            \n        # Initialize text splitter with SMALLER chunk size to avoid token limits\n        self.text_splitter \u003d RecursiveCharacterTextSplitter(\n            chunk_size\u003d500,  # Much smaller chunk size\n            chunk_overlap\u003d50  # Smaller overlap too\n        )\n        \n        # Create directory for vector store if it doesn\u0027t exist\n        if not self.using_dataiku_folder:\n            os.makedirs(self.persist_directory, exist_ok\u003dTrue)\n            print(f\"Created/verified directory exists: {self.persist_directory}\")\n    \n    def vector_store_exists(self) -\u003e bool:\n        \"\"\"Check if vector store exists at the specified directory - more robust implementation\"\"\"\n        try:\n            if self.store_type \u003d\u003d \"CHROMADB\":\n                # For ChromaDB, check for the chroma directory\n                if self.using_dataiku_folder:\n                    # For Dataiku folders, check if files exist in the listing\n                    try:\n                        files \u003d self.dataiku_folder.list_paths_in_partition()\n                        chroma_files \u003d [f for f in files if \"chroma\" in f]\n                        \n                        # More verbose logging for debugging\n                        print(f\"Looking for ChromaDB files in Dataiku folder\")\n                        print(f\"Found {len(chroma_files)} ChromaDB files: {chroma_files}\")\n                        \n                        return len(chroma_files) \u003e 0\n                    except Exception as e:\n                        print(f\"Error checking for ChromaDB files in Dataiku folder: {str(e)}\")\n                        return False\n                else:\n                    # For local filesystem\n                    chroma_path \u003d os.path.join(self.persist_directory, \"chroma\")\n                    exists \u003d os.path.exists(chroma_path)\n                    print(f\"Checking ChromaDB path: {chroma_path}, exists: {exists}\")\n                    return exists\n            else:\n                # For FAISS, check for the index file\n                if self.using_dataiku_folder:\n                    # For Dataiku folders, check if files exist in the listing\n                    try:\n                        files \u003d self.dataiku_folder.list_paths_in_partition()\n                        faiss_files \u003d [f for f in files if \"index.faiss\" in f]\n                        \n                        # More verbose logging\n                        print(f\"Looking for FAISS files in Dataiku folder\")\n                        print(f\"Found {len(faiss_files)} FAISS files: {faiss_files}\")\n                        \n                        return len(faiss_files) \u003e 0\n                    except Exception as e:\n                        print(f\"Error checking for FAISS files in Dataiku folder: {str(e)}\")\n                        return False\n                else:\n                    # For local filesystem\n                    faiss_path \u003d os.path.join(self.persist_directory, \"index.faiss\")\n                    exists \u003d os.path.exists(faiss_path)\n                    print(f\"Checking FAISS path: {faiss_path}, exists: {exists}\")\n                    return exists\n        except Exception as e:\n            print(f\"Error checking if vector store exists: {str(e)}\")\n            import traceback\n            traceback.print_exc()\n            return False\n    \n    def load_vector_store(self):\n        \"\"\"Load an existing vector store with enhanced error handling\"\"\"\n        print(f\"Attempting to load vector store from {self.persist_directory}\")\n        print(f\"Store type: {self.store_type}, Using Dataiku folder: {self.using_dataiku_folder}\")\n        \n        try:\n            if self.store_type \u003d\u003d \"CHROMADB\":\n                print(f\"Loading ChromaDB from: {self.persist_directory}\")\n                \n                # For ChromaDB\n                self.vector_store \u003d Chroma(\n                    persist_directory\u003dself.persist_directory,\n                    embedding_function\u003dself.embedding_model\n                )\n                \n                # Verify that the collection has data\n                if hasattr(self.vector_store, \u0027_collection\u0027):\n                    count \u003d self.vector_store._collection.count()\n                    print(f\"ChromaDB loaded with {count} vectors\")\n            else:  # FAISS\n                print(f\"Loading FAISS from: {self.persist_directory}\")\n                \n                # For FAISS\n                self.vector_store \u003d FAISS.load_local(\n                    self.persist_directory, \n                    self.embedding_model, \n                    allow_dangerous_deserialization\u003dTrue\n                )\n                print(f\"FAISS loaded successfully\")\n            \n            return self.vector_store\n        except Exception as e:\n            print(f\"Error loading vector store: {str(e)}\")\n            import traceback\n            traceback.print_exc()\n            raise\n    \n    def track_processed_documents(self, documents: List[Document], tracking_file: str \u003d \"processed_docs.pkl\"):\n        \"\"\"Track which documents have been processed to avoid reprocessing.\"\"\"\n        # Extract document content hashes instead of IDs\n        doc_hashes \u003d []\n        for doc in documents:\n            content \u003d doc.page_content\n            doc_hash \u003d hashlib.md5(content.encode()).hexdigest()\n            doc_hashes.append(doc_hash)\n\n        if hasattr(self, \u0027dataiku_folder\u0027) and self.dataiku_folder:\n            # For Dataiku folder\n            with self.dataiku_folder.get_writer(tracking_file) as writer:\n                pickle.dump(doc_hashes, writer)\n        else:\n            # Local file system\n            tracking_path \u003d os.path.join(self.persist_directory, tracking_file)\n            with open(tracking_path, \"wb\") as f:\n                pickle.dump(doc_hashes, f)\n\n        print(f\"Tracked {len(doc_hashes)} processed documents\")\n        return doc_hashes\n\n    def get_processed_documents(self, tracking_file: str \u003d \"processed_docs.pkl\") -\u003e List[str]:\n        \"\"\"Get list of already processed document hashes.\"\"\"\n        try:\n            if hasattr(self, \u0027dataiku_folder\u0027) and self.dataiku_folder:\n                # For Dataiku folder\n                with self.dataiku_folder.get_download_stream(tracking_file) as stream:\n                    doc_hashes \u003d pickle.load(stream)\n            else:\n                # Local file system\n                tracking_path \u003d os.path.join(self.persist_directory, tracking_file)\n                if not os.path.exists(tracking_path):\n                    return []\n                    \n                with open(tracking_path, \"rb\") as f:\n                    doc_hashes \u003d pickle.load(f)\n                    \n            print(f\"Found {len(doc_hashes)} previously processed documents\")\n            return doc_hashes\n        except Exception as e:\n            print(f\"Error loading processed documents: {str(e)}\")\n            return []\n    \n    def create_vector_store(self, documents: List[Document]) -\u003e Any:\n        \"\"\"Create a vector store from documents with much smaller chunks to avoid token limits.\"\"\"\n        print(f\"Creating vector store with {len(documents)} documents, using smaller chunks\")\n        \n        # If no documents provided, create empty store\n        if not documents:\n            if self.store_type \u003d\u003d \"CHROMADB\":\n                self.vector_store \u003d Chroma(\n                    persist_directory\u003dself.persist_directory,\n                    embedding_function\u003dself.embedding_model\n                )\n            else:  # FAISS\n                self.vector_store \u003d FAISS(\n                    embedding_function\u003dself.embedding_model\n                )\n            return self.vector_store\n            \n        # Chunk all documents before processing\n        all_chunks \u003d []\n        for doc in documents:\n            # Split the document into much smaller chunks\n            chunks \u003d self.text_splitter.split_text(doc.page_content)\n            # Create Document objects for each chunk with metadata\n            for i, chunk in enumerate(chunks):\n                chunk_doc \u003d Document(\n                    page_content\u003dchunk,\n                    metadata\u003d{**doc.metadata, \"chunk\": i, \"total_chunks\": len(chunks)}\n                )\n                all_chunks.append(chunk_doc)\n        \n        print(f\"Split {len(documents)} documents into {len(all_chunks)} smaller chunks\")\n        \n        # Process in very small batches to avoid token limits\n        batch_size \u003d 1  # Process one chunk at a time\n        \n        if self.store_type \u003d\u003d \"CHROMADB\":\n            print(f\"Creating ChromaDB vector store at {self.persist_directory}\")\n            \n            try:\n                # First create an empty collection\n                self.vector_store \u003d Chroma(\n                    persist_directory\u003dself.persist_directory,\n                    embedding_function\u003dself.embedding_model\n                )\n                \n                # Then add documents one by one\n                for i in range(0, len(all_chunks), batch_size):\n                    end_idx \u003d min(i + batch_size, len(all_chunks))\n                    batch \u003d all_chunks[i:end_idx]\n                    \n                    if i % 10 \u003d\u003d 0:  # Print progress every 10 chunks\n                        print(f\"Processing chunk {i+1}/{len(all_chunks)}\")\n                    \n                    # Add single chunk to vector store\n                    self.vector_store.add_documents(batch)\n                    \n                    # Persist after each batch to be safe\n                    self.vector_store.persist()\n                    \n            except Exception as e:\n                print(f\"Error creating ChromaDB vector store: {str(e)}\")\n                import traceback\n                traceback.print_exc()\n                raise\n                \n        else:  # Default to FAISS\n            print(f\"Creating FAISS vector store\")\n            \n            try:\n                # Initialize empty FAISS index\n                self.vector_store \u003d FAISS(\n                    embedding_function\u003dself.embedding_model\n                )\n                \n                # Add documents one by one\n                for i in range(0, len(all_chunks), batch_size):\n                    end_idx \u003d min(i + batch_size, len(all_chunks))\n                    batch \u003d all_chunks[i:end_idx]\n                    \n                    if i % 10 \u003d\u003d 0:  # Print progress every 10 chunks\n                        print(f\"Processing chunk {i+1}/{len(all_chunks)}\")\n                    \n                    # Add single chunk to vector store\n                    self.vector_store.add_documents(batch)\n                \n                # Save FAISS index\n                self.vector_store.save_local(self.persist_directory)\n                \n            except Exception as e:\n                print(f\"Error creating FAISS vector store: {str(e)}\")\n                import traceback\n                traceback.print_exc()\n                raise\n        \n        print(f\"Successfully created vector store with {len(all_chunks)} chunks\")\n        return self.vector_store\n    \n    \n\n\nclass DocumentProcessor:\n    \"\"\"\n    Class to prepare documents for the multi-vector store\n    \"\"\"\n    def __init__(\n        self,\n        summarizer: ContentSummarizer,\n        text_processor: TextProcessor,\n        image_manager: Optional[ImageManager] \u003d None\n    ):\n        self.summarizer \u003d summarizer\n        self.text_processor \u003d text_processor\n        self.image_manager \u003d image_manager\n        \n    def prepare_multi_vector_documents(\n        self, \n        df: pd.DataFrame, \n        text_column: str, \n        table_column: Optional[str] \u003d None\n    ) -\u003e Tuple[List[Document], List[Document], Dict]:\n        \"\"\"\n        Prepare documents for multi-vector storage with improved table and image handling\n\n        Returns:\n            Tuple[List[Document], List[Document], Dict]: (parent_docs, child_docs, id_to_children_map)\n        \"\"\"\n        all_docs \u003d []       # Original documents for the docstore\n        child_docs \u003d []     # Summary documents for the vector store\n        id_to_children \u003d {}\n\n        print(\"Preparing documents for multi-vector retrieval...\")\n\n        for idx, row in tqdm(df.iterrows(), total\u003dlen(df), desc\u003d\"Processing documents\"):\n            doc_id \u003d str(uuid.uuid4())\n            text_content \u003d str(row[text_column])\n            metadata \u003d row.get(\"metadata\", {})\n            if isinstance(metadata, str):\n                try:\n                    metadata \u003d json.loads(metadata)\n                except:\n                    metadata \u003d {\"source\": metadata}\n\n            # Add source info if not present\n            if \"source\" not in metadata:\n                metadata[\"source\"] \u003d f\"document_{idx}\"\n\n            # Base document (for docstore)\n            doc \u003d Document(\n                page_content\u003dtext_content,\n                metadata\u003d{\"id\": doc_id, **metadata, \"type\": \"original\"}\n            )\n            all_docs.append(doc)\n\n            # Create summary for vector store\n            summary \u003d self.summarizer.summarize_text(text_content)\n            summary_doc \u003d Document(\n                page_content\u003dsummary,\n                metadata\u003d{\"id\": f\"{doc_id}_summary\", \"parent_id\": doc_id, **metadata, \"type\": \"summary\"}\n            )\n            child_docs.append(summary_doc)\n\n            # Process tables if present in a separate column\n            if table_column and table_column in row and row[table_column]:\n                table_content \u003d str(row[table_column])\n\n                # Store full table in docstore\n                table_doc \u003d Document(\n                    page_content\u003dtable_content,\n                    metadata\u003d{\"id\": f\"{doc_id}_table\", **metadata, \"type\": \"table\"}\n                )\n                all_docs.append(table_doc)  # Add to docstore, not just as child\n\n                # Create table summary for vector store\n                table_summary \u003d self.summarizer.summarize_table(table_content)\n                table_summary_doc \u003d Document(\n                    page_content\u003dtable_summary,\n                    metadata\u003d{\"id\": f\"{doc_id}_table_summary\", \"parent_id\": doc_id, **metadata, \"type\": \"table_summary\"}\n                )\n                child_docs.append(table_summary_doc)\n            else:\n                # Check for tables in the text content\n                has_table, table_content \u003d self.text_processor.extract_table_from_text(text_content)\n                if has_table:\n                    # Store full table in docstore\n                    table_doc \u003d Document(\n                        page_content\u003dtable_content,\n                        metadata\u003d{\"id\": f\"{doc_id}_table\", **metadata, \"type\": \"table\"}\n                    )\n                    all_docs.append(table_doc)  # Add to docstore\n\n                    # Create table summary for vector store\n                    table_summary \u003d self.summarizer.summarize_table(table_content)\n                    table_summary_doc \u003d Document(\n                        page_content\u003dtable_summary,\n                        metadata\u003d{\"id\": f\"{doc_id}_table_summary\", \"parent_id\": doc_id, **metadata, \"type\": \"table_summary\"}\n                    )\n                    child_docs.append(table_summary_doc)\n\n            # Process associated images if image manager is configured\n            if self.image_manager:\n                image_filename \u003d self.image_manager.find_image_for_document(doc_id, metadata)\n                if image_filename:\n                    try:\n                        image_data \u003d self.image_manager.get_image_data(image_filename)\n                        if image_data:\n                            image_description, image_summary \u003d self.summarizer.process_image_with_llm(image_data)\n\n                            # Store full image description in docstore\n                            image_desc_doc \u003d Document(\n                                page_content\u003dimage_description,\n                                metadata\u003d{\n                                    \"id\": f\"{doc_id}_image\", \n                                    \"image_filename\": image_filename,\n                                    **metadata, \n                                    \"type\": \"image\"\n                                }\n                            )\n                            all_docs.append(image_desc_doc)  # Add to docstore\n\n                            # Store image summary for vector search\n                            image_summary_doc \u003d Document(\n                                page_content\u003dimage_summary,\n                                metadata\u003d{\n                                    \"id\": f\"{doc_id}_image_summary\", \n                                    \"parent_id\": doc_id,\n                                    \"image_filename\": image_filename,\n                                    **metadata, \n                                    \"type\": \"image_summary\"\n                                }\n                            )\n                            child_docs.append(image_summary_doc)\n                            print(f\"Processed image {image_filename} for document {doc_id}\")\n                    except Exception as e:\n                        print(f\"Error processing image {image_filename} for document {doc_id}: {str(e)}\")\n\n            # Store children relationships\n            id_to_children[doc_id] \u003d [\n                child.metadata[\"id\"] for child in child_docs \n                if child.metadata.get(\"parent_id\") \u003d\u003d doc_id\n            ]\n\n        return all_docs, child_docs, id_to_children\n\n\nclass MultiVectorRetrieverBuilder:\n    \"\"\"\n    Class to build and configure the multi-vector retriever\n    \"\"\"\n    def __init__(\n        self,\n        vector_store_manager: VectorStoreManager,\n        search_type: str \u003d \"similarity\",\n        k: int \u003d 5,\n        persist_directory: str \u003d \"./vector_store\"\n    ):\n        self.vector_store_manager \u003d vector_store_manager\n        self.search_type \u003d search_type\n        self.k \u003d k\n        self.persist_directory \u003d persist_directory\n        self.docstore \u003d InMemoryStore()  # Document store to keep parent documents\n        \n        # Store whether we\u0027re using Dataiku folder\n        self.using_dataiku_folder \u003d hasattr(self.vector_store_manager, \u0027dataiku_folder\u0027) and self.vector_store_manager.dataiku_folder is not None\n        \n        # Set up storage paths\n        if self.using_dataiku_folder:\n            self.metadata_filename \u003d \"doc_metadata.pkl\"\n        else:\n            # For local storage\n            self.doc_metadata_path \u003d os.path.join(persist_directory, \"doc_metadata.pkl\")\n    \n    def save_document_metadata(self, parent_docs):\n        \"\"\"Save parent document metadata to enable retriever reconstruction\"\"\"\n        doc_metadata \u003d {}\n        for doc in parent_docs:\n            doc_id \u003d doc.metadata[\"id\"]\n            doc_metadata[doc_id] \u003d {\n                \"content\": doc.page_content,\n                \"metadata\": doc.metadata\n            }\n        \n        # Save to the appropriate location\n        if self.using_dataiku_folder:\n            # Use Dataiku folder for storage\n            with self.vector_store_manager.dataiku_folder.get_writer(self.metadata_filename) as writer:\n                pickle.dump(doc_metadata, writer)\n        else:\n            # Save to local file system\n            with open(self.doc_metadata_path, \"wb\") as f:\n                pickle.dump(doc_metadata, f)\n                \n        print(f\"Saved metadata for {len(doc_metadata)} parent documents\")\n    \n    def load_document_metadata(self):\n        \"\"\"Load document metadata from file\"\"\"\n        try:\n            if self.using_dataiku_folder:\n                # Load from Dataiku folder\n                with self.vector_store_manager.dataiku_folder.get_download_stream(self.metadata_filename) as stream:\n                    doc_metadata \u003d pickle.load(stream)\n            else:\n                # Load from local file system\n                if not os.path.exists(self.doc_metadata_path):\n                    print(\"No document metadata file found\")\n                    return {}\n                    \n                with open(self.doc_metadata_path, \"rb\") as f:\n                    doc_metadata \u003d pickle.load(f)\n                    \n            print(f\"Loaded metadata for {len(doc_metadata)} parent documents\")\n            return doc_metadata\n        except FileNotFoundError:\n            print(\"No document metadata file found\")\n            return {}\n        except Exception as e:\n            print(f\"Error loading document metadata: {str(e)}\")\n            return {}\n    \n    def build_retriever(\n        self, \n        parent_docs: List[Document], \n        child_docs: List[Document],\n        update_existing: bool \u003d False\n    ) -\u003e MultiVectorRetriever:\n        \"\"\"\n        Build a multi-vector retriever with full media access\n        \"\"\"\n        # Add ALL parent documents to docstore (including tables and images)\n        for doc in parent_docs:\n            self.docstore.mset([(doc.metadata[\"id\"], doc)])\n\n        # Save document metadata for later reconstruction\n        self.save_document_metadata(parent_docs)\n\n        # Create or update vector store for child documents (summaries only)\n        if update_existing and self.vector_store_manager.vector_store is not None:\n            # Add new documents to existing vector store\n            self.vector_store_manager.vector_store.add_documents(child_docs)\n            vector_store \u003d self.vector_store_manager.vector_store\n        else:\n            # Create new vector store\n            vector_store \u003d self.vector_store_manager.create_vector_store(child_docs)\n\n        # Create the multi-vector retriever\n        retriever \u003d MultiVectorRetriever(\n            vectorstore\u003dvector_store,\n            docstore\u003dself.docstore,\n            id_key\u003d\"parent_id\",  # This links summaries to parents\n            search_type\u003dSearchType.similarity,\n            search_kwargs\u003d{\"k\": self.k}\n        )\n\n        print(f\"Multi-vector retriever built successfully with {len(parent_docs)} parent documents and {len(child_docs)} summary documents\")\n\n        return retriever\n    \n    def load_retriever(self) -\u003e MultiVectorRetriever:\n        \"\"\"\n        Load an existing retriever from a saved vector store\n        \n        Returns:\n            MultiVectorRetriever: Loaded retriever\n        \"\"\"\n        # Load existing vector store\n        vector_store \u003d self.vector_store_manager.load_vector_store()\n        \n        # Load document metadata and populate docstore\n        doc_metadata \u003d self.load_document_metadata()\n        if doc_metadata:\n            for doc_id, doc_data in doc_metadata.items():\n                doc \u003d Document(\n                    page_content\u003ddoc_data[\"content\"],\n                    metadata\u003ddoc_data[\"metadata\"]\n                )\n                self.docstore.mset([(doc_id, doc)])\n        \n        # Create the multi-vector retriever\n        retriever \u003d MultiVectorRetriever(\n            vectorstore\u003dvector_store,\n            docstore\u003dself.docstore,\n            id_key\u003d\"parent_id\",\n            search_type\u003dSearchType.similarity,\n            search_kwargs\u003d{\"k\": min(self.k, 3)} \n        )\n        \n        print(f\"Multi-vector retriever loaded successfully with {len(list(self.docstore.yield_keys()))} parent documents\")\n        return retriever\n\n\nclass QueryEngine:\n    \"\"\"\n    Class to handle document retrieval and response generation\n    \"\"\"\n    def __init__(\n        self,\n        retriever: MultiVectorRetriever,\n        embedding_model,\n        llm,\n        similarity_threshold: float \u003d 0.5\n    ):\n        self.retriever \u003d retriever\n        self.embedding_model \u003d embedding_model\n        self.llm \u003d llm\n        \n        # Add a relevance filter for better results\n        self.embeddings_filter \u003d EmbeddingsFilter(\n            embeddings\u003dembedding_model,\n            similarity_threshold\u003dsimilarity_threshold\n        )\n        \n        # Create prompt template for response generation\n        self.response_prompt \u003d PromptTemplate(\n            input_variables\u003d[\"query\", \"context\"],\n            template\u003d\"\"\"\n            You are an AI assistant that provides precise answers from provided document chunks. Follow these steps:\n\n            1. Carefully read all the document chunks provided.\n            2. Determine what specific detail the user is asking for in their query.\n            3. Identify the most relevant information in the chunks that answers this query.\n            4. Extract and return a precise answer with all details, without any extra commentary.\n            5. If no relevant information is found, state that the data is insufficient.\n            6. Never generate content that includes hate speech, offensive language, violence, threats, or misinformation.\n\n            User Query:\n            {query}\n\n            Document Chunks:\n            {context}\n\n            Answer:\n            \"\"\"\n        )\n    \n    def retrieve(self, query: str, k: int \u003d 5, filter_metadata: Optional[Dict] \u003d None) -\u003e List[Document]:\n        \"\"\"\n        Retrieve documents relevant to the query\n\n        Args:\n            query: Query string\n            k: Number of documents to retrieve\n            filter_metadata: Optional filter for metadata fields\n\n        Returns:\n            List[Document]: Retrieved documents\n        \"\"\"\n        try:\n            # Apply filters if provided            \n            # build the kwargs\n            search_args \u003d {\"k\": k}\n            if filter_metadata:\n                search_args[\"filter\"] \u003d filter_metadata\n\n            print(\"[DEBUG] docstore keys:\", list(self.retriever.docstore.yield_keys()))\n            if hasattr(self.retriever.vectorstore, \u0027_collection\u0027):\n                print(\"[DEBUG] vectorstore has\", self.retriever.vectorstore._collection.count(), \"vectors\")\n\n            # Retrieve documents safely\n            results \u003d self.retriever.get_relevant_documents(query, **search_args)\n            \n            # Filter out any non-Document objects\n            retrieved_docs \u003d []\n            for item in results:\n                if hasattr(item, \u0027metadata\u0027) and hasattr(item, \u0027page_content\u0027):\n                    retrieved_docs.append(item)\n                else:\n                    print(f\"WARNING: Retrieved non-Document object: {type(item)}\")\n                \n            print(f\"Retrieved {len(retrieved_docs)} valid documents\")\n            \n            if not retrieved_docs:\n                return []\n                \n            # Apply relevance filtering\n            try:\n                filtered_docs \u003d self.embeddings_filter.compress_documents(\n                    retrieved_docs, query\n                )\n                print(f\"Filtered to {len(filtered_docs)} relevant documents\")\n                return filtered_docs\n            except Exception as e:\n#                 print(f\"Error filtering documents: {str(e)}\")\n                # Return original results if filtering fails\n                return retrieved_docs\n\n        except Exception as e:\n            print(f\"Error retrieving documents: {str(e)}\")\n            import traceback\n            traceback.print_exc()\n            return []\n    \n    def generate_response(self, query: str, return_context: bool \u003d True) -\u003e Union[str, Tuple[str, Dict]]:\n        \"\"\"\n        Generate a response based on retrieved summaries and also return original images and tables\n        \"\"\"\n        try:\n            # Retrieve relevant documents based on summary embeddings\n            docs \u003d self.retrieve(query, k\u003d5)\n\n            if not docs:\n                return (\n                    \"No relevant information found in the provided documents.\", \n                    []\n                ) if return_context else \"No relevant information found in the provided documents.\"\n\n            # Collect related media from the docstore for all retrieved summary documents\n            related_media \u003d {\n                \u0027images\u0027: [],\n                \u0027tables\u0027: []\n            }\n\n            # Keep track of processed parent IDs to avoid duplicates\n            processed_parent_ids \u003d set()\n\n            # For each retrieved document\n            for doc in docs:\n                doc_type \u003d doc.metadata.get(\u0027type\u0027, \u0027\u0027)\n                parent_id \u003d doc.metadata.get(\u0027parent_id\u0027)\n\n                # Skip if we\u0027ve already processed this parent document\n                if parent_id in processed_parent_ids:\n                    continue\n\n                processed_parent_ids.add(parent_id)\n\n                # If this is an image summary, add the image reference to related media\n                if doc_type \u003d\u003d \u0027image_summary\u0027 and \u0027image_filename\u0027 in doc.metadata:\n                    # Find the original image description if available\n                    image_filename \u003d doc.metadata[\u0027image_filename\u0027]\n                    image_doc_id \u003d f\"{parent_id}_image\" if parent_id else None\n\n                    image_description \u003d \"\"\n                    if image_doc_id and hasattr(self.retriever, \u0027docstore\u0027):\n                        try:\n                            image_doc \u003d self.retriever.docstore.get(image_doc_id)\n                            if image_doc:\n                                image_description \u003d image_doc.page_content\n                        except:\n                            pass\n\n                    related_media[\u0027images\u0027].append({\n                        \u0027filename\u0027: image_filename,\n                        \u0027parent_id\u0027: parent_id,\n                        \u0027summary\u0027: doc.page_content,\n                        \u0027description\u0027: image_description\n                    })\n\n                # If this is a table summary, add the table content to related media\n                if doc_type \u003d\u003d \u0027table_summary\u0027:\n                    # Try to find original table content\n                    table_doc_id \u003d f\"{parent_id}_table\" if parent_id else None\n\n                    if table_doc_id and hasattr(self.retriever, \u0027docstore\u0027):\n                        try:\n                            table_doc \u003d self.retriever.docstore.get(table_doc_id)\n                            if table_doc:\n                                related_media[\u0027tables\u0027].append({\n                                    \u0027content\u0027: table_doc.page_content,\n                                    \u0027parent_id\u0027: parent_id,\n                                    \u0027summary\u0027: doc.page_content\n                                })\n                        except Exception as e:\n                            print(f\"Error retrieving table: {str(e)}\")\n\n            # Prepare formatted context using only summaries for the LLM\n            formatted_context \u003d \"\"\n            for i, doc in enumerate(docs, 1):\n                formatted_context +\u003d f\"DOCUMENT {i} ({doc.metadata.get(\u0027type\u0027, \u0027unknown\u0027)}):\\n\"\n                formatted_context +\u003d f\"{doc.page_content}\\n\"\n                formatted_context +\u003d f\"Source: {doc.metadata.get(\u0027source\u0027, \u0027Unknown\u0027)}\\n\\n\"\n\n            # Generate response using the chain\n            chain \u003d self.response_prompt | self.llm\n            final_response \u003d chain.invoke({\n                \"query\": query,\n                \"context\": formatted_context\n            })\n\n            # Return response with all related content if requested\n            if return_context:\n                context_dict \u003d {\n                    \"retrieved_docs\": [\n                        {\n                            \"content\": doc.page_content,\n                            \"metadata\": doc.metadata\n                        } for doc in docs\n                    ],\n                    \"formatted_context\": formatted_context,\n                    \"related_media\": related_media,\n                    \"query\": query\n                }\n                return final_response, context_dict\n            else:\n                return final_response\n\n        except Exception as e:\n            print(f\"Error generating response: {str(e)}\")\n            import traceback\n            traceback.print_exc()\n            error_msg \u003d f\"Error generating response: {str(e)}\"\n            return (error_msg, []) if return_context else error_msg\n    \n    \n\n\nclass RAGPipeline:\n    \"\"\"\n    Main pipeline class to orchestrate the entire RAG process\n    \"\"\"\n    def __init__(\n        self,\n        embedding_model_id: str,\n        llm_id: str,\n        vector_store_type: str \u003d \"CHROMADB\",\n        persist_directory: str \u003d \"./vector_store\",\n        image_folder_name: Optional[str] \u003d None,\n        vector_store_folder_name: Optional[str] \u003d None  # Add this parameter\n    ):\n        # Initialize model manager\n        self.model_manager \u003d ModelManager(embedding_model_id, llm_id)\n\n        # Get initialized models\n        self.embedding_model \u003d self.model_manager.get_embedding_model()\n        self.llm \u003d self.model_manager.get_llm()\n\n        # Initialize components\n        self.text_processor \u003d TextProcessor()\n        self.summarizer \u003d ContentSummarizer(self.llm)\n        self.image_manager \u003d ImageManager(image_folder_name) if image_folder_name else None\n        self.vector_store_manager \u003d VectorStoreManager(\n            self.embedding_model,\n            store_type\u003dvector_store_type,\n            persist_directory\u003dpersist_directory,\n            dataiku_folder_name\u003dvector_store_folder_name  # Pass to the manager\n        )\n        self.retriever_builder \u003d MultiVectorRetrieverBuilder(self.vector_store_manager)\n        \n        # Will be set during processing\n        self.document_processor \u003d DocumentProcessor(\n            self.summarizer,\n            self.text_processor,\n            self.image_manager\n        )\n        self.retriever \u003d None\n        self.query_engine \u003d None\n        \n        # Store configuration\n        self.config \u003d {\n            \"embedding_model_id\": embedding_model_id,\n            \"llm_id\": llm_id,\n            \"vector_store_type\": vector_store_type,\n            \"persist_directory\": persist_directory,\n            \"image_folder_name\": image_folder_name,\n            \"vector_store_folder_name\": vector_store_folder_name\n        }\n        \n        # Enhanced logging for initialization\n        print(f\"RAGPipeline initialized with: vector_store_type\u003d{vector_store_type}, \"\n              f\"persist_directory\u003d{persist_directory}, \"\n              f\"vector_store_folder_name\u003d{vector_store_folder_name}\")\n        \n        if vector_store_folder_name:\n            try:\n                folder \u003d dataiku.Folder(vector_store_folder_name)\n                print(f\"Dataiku folder path: {folder.get_path()}\")\n                print(f\"Dataiku folder files: {folder.list_paths_in_partition()}\")\n            except Exception as e:\n                print(f\"Error accessing Dataiku folder: {str(e)}\")\n    \n    def process_dataset(\n        self,\n        df: pd.DataFrame,\n        text_column: str,\n        table_column: Optional[str] \u003d None\n    ) -\u003e MultiVectorRetriever:\n        \"\"\"\n        Process a dataset to build a multi-vector retriever, \n        only processing new documents not already in the vector store\n        \"\"\"\n        # More robust check for vector store existence\n        vector_store_exists \u003d self.vector_store_manager.vector_store_exists()\n        \n        print(f\"Checking for vector store: {vector_store_exists}\")\n        \n        # Log key paths for debugging\n        if hasattr(self.vector_store_manager, \u0027persist_directory\u0027):\n            print(f\"Vector store directory: {self.vector_store_manager.persist_directory}\")\n            \n            # Check if the directory exists and is accessible\n            if os.path.exists(self.vector_store_manager.persist_directory):\n                print(f\"Directory exists and is accessible\")\n                try:\n                    files \u003d os.listdir(self.vector_store_manager.persist_directory)\n                    print(f\"Directory contents: {files}\")\n                except Exception as e:\n                    print(f\"Cannot list directory contents: {str(e)}\")\n            else:\n                print(f\"Directory does not exist or is not accessible\")\n\n        if vector_store_exists:\n            print(f\"Found existing vector store\")\n            # Get list of processed document IDs\n            processed_doc_ids \u003d self.vector_store_manager.get_processed_documents()\n            print(f\"Found {len(processed_doc_ids)} processed document hashes\")\n\n            # If no tracking file, assume we need to rebuild\n            if not processed_doc_ids:\n                print(\"No document tracking information found, processing all documents\")\n                return self._process_all_documents(df, text_column, table_column)\n\n            # Try to load existing retriever\n            try:\n                print(\"Attempting to load existing retriever\")\n                self.retriever \u003d self.retriever_builder.load_retriever()\n                print(\"Successfully loaded existing retriever\")\n\n                # Process only new documents\n                return self._process_new_documents(df, text_column, processed_doc_ids, table_column)\n            except Exception as e:\n                print(f\"Error loading existing retriever: {str(e)}\")\n                import traceback\n                traceback.print_exc()\n                print(\"Rebuilding vector store from scratch\")\n                return self._process_all_documents(df, text_column, table_column)\n        else:\n            print(\"No existing vector store found, processing all documents\")\n            return self._process_all_documents(df, text_column, table_column)\n\n    def _process_all_documents(\n        self,\n        df: pd.DataFrame,\n        text_column: str,\n        table_column: Optional[str] \u003d None\n    ) -\u003e MultiVectorRetriever:\n        \"\"\"Process all documents in the dataset\"\"\"\n        # Prepare documents\n        parent_docs, child_docs, id_to_children \u003d self.document_processor.prepare_multi_vector_documents(\n            df, text_column, table_column\n        )\n\n        # Build retriever\n        self.retriever \u003d self.retriever_builder.build_retriever(parent_docs, child_docs)\n\n        # Track processed documents\n        self.vector_store_manager.track_processed_documents(parent_docs)\n\n        # Initialize query engine\n        self.query_engine \u003d QueryEngine(\n            self.retriever,\n            self.embedding_model,\n            self.llm\n        )\n\n        return self.retriever\n\n    def _process_new_documents(\n        self,\n        df: pd.DataFrame,\n        text_column: str,\n        processed_doc_ids: List[str],\n        table_column: Optional[str] \u003d None,\n        max_token_length: int \u003d 4000  # Reduced from 7000 to ensure batches stay under limits\n    ) -\u003e MultiVectorRetriever:\n        \"\"\"\n        Process only new documents not in processed_doc_ids\n\n        Args:\n            df: DataFrame containing documents\n            text_column: Column name containing the text\n            processed_doc_ids: List of already processed document IDs/hashes\n            table_column: Optional column name for table data\n            max_token_length: Maximum token length before truncation (reduced to stay well under model limits)\n\n        Returns:\n            MultiVectorRetriever: The retriever with updated documents\n        \"\"\"\n        if not self.retriever:\n            try:\n                self.retriever \u003d self.retriever_builder.load_retriever()\n            except Exception as e:\n                print(f\"Error loading existing retriever: {str(e)}\")\n                print(\"Will create a new retriever from scratch\")\n                return self._process_all_documents(df, text_column, table_column)\n\n        # Function to estimate token count - approximate but faster than calling tokenizer\n        def estimate_token_count(text):\n            # Rough estimate: 4 chars per token for English text\n            return len(text) // 4\n\n        # Function to truncate text to max token length\n        def truncate_to_max_tokens(text, max_tokens\u003dmax_token_length):\n            estimated_tokens \u003d estimate_token_count(text)\n            if estimated_tokens \u003c\u003d max_tokens:\n                return text\n\n            # If we need to truncate, use character count as proxy\n            # (4 chars per token approximation)\n            max_chars \u003d max_tokens * 4\n            truncated \u003d text[:max_chars]\n            print(f\"Truncated document from ~{estimated_tokens} tokens to {max_tokens} tokens\")\n            return truncated\n\n        print(\"Checking for new documents...\")\n\n        # Generate content hashes for all current documents\n        current_doc_hashes \u003d []\n        for idx in range(len(df)):\n            # Get the text content, truncate if needed to avoid tokenization issues\n            text_content \u003d str(df.iloc[idx][text_column])\n            text_content \u003d truncate_to_max_tokens(text_content)\n\n            # Generate hash from the (potentially truncated) content\n            content_hash \u003d hashlib.md5(text_content.encode()).hexdigest()\n            current_doc_hashes.append(content_hash)\n\n        # Determine which documents are new by comparing hashes\n        new_indices \u003d [\n            idx for idx, hash_value in enumerate(current_doc_hashes)\n            if hash_value not in processed_doc_ids\n        ]\n\n        if not new_indices:\n            print(\"No new documents to process\")\n            # Initialize query engine with existing retriever\n            self.query_engine \u003d QueryEngine(\n                self.retriever,\n                self.embedding_model,\n                self.llm\n            )\n            return self.retriever\n\n        print(f\"Processing {len(new_indices)} new documents\")\n        new_df \u003d df.iloc[new_indices].copy()\n\n        # Truncate large documents to avoid token limit issues\n        for idx in new_df.index:\n            text_content \u003d str(new_df.loc[idx, text_column])\n            token_count \u003d estimate_token_count(text_content)\n\n            if token_count \u003e max_token_length:\n                print(f\"Document at index {idx} has ~{token_count} tokens, truncating to {max_token_length}\")\n                new_df.loc[idx, text_column] \u003d truncate_to_max_tokens(text_content)\n\n        # Process documents in small batches to avoid token limits\n        batch_size \u003d 5  # Process only 5 documents at a time\n        all_parent_docs \u003d []\n        all_child_docs \u003d []\n        all_id_to_children \u003d {}\n\n        # Split the dataframe into smaller batches\n        for i in range(0, len(new_df), batch_size):\n            end_idx \u003d min(i + batch_size, len(new_df))\n            batch_df \u003d new_df.iloc[i:end_idx]\n            print(f\"Preparing batch {i//batch_size + 1}/{(len(new_df) + batch_size - 1)//batch_size}\")\n\n            # Process this small batch of documents\n            parent_docs, child_docs, id_to_children \u003d self.document_processor.prepare_multi_vector_documents(\n                batch_df, text_column, table_column\n            )\n\n            all_parent_docs.extend(parent_docs)\n            all_child_docs.extend(child_docs)\n            all_id_to_children.update(id_to_children)\n\n        # Track the new document hashes that we\u0027re processing\n        new_doc_hashes \u003d [current_doc_hashes[idx] for idx in new_indices]\n\n        # Add new documents to existing vector store\n        if self.vector_store_manager.vector_store is None:\n            try:\n                self.vector_store_manager.load_vector_store()\n            except Exception as e:\n                print(f\"Error loading vector store: {str(e)}\")\n                print(\"Creating new vector store\")\n                self.vector_store_manager.create_vector_store([])  # Create empty store\n\n        # Process child documents in VERY small batches to avoid token limit issues\n        small_batch_size \u003d 1  # Process one document at a time for embedding\n        try:\n            for i in range(0, len(all_child_docs), small_batch_size):\n                end_idx \u003d min(i + small_batch_size, len(all_child_docs))\n                batch \u003d all_child_docs[i:end_idx]\n                print(f\"Adding document {i+1}/{len(all_child_docs)} to vector store\")\n\n                # Check total tokens in this batch\n                batch_tokens \u003d sum(estimate_token_count(doc.page_content) for doc in batch)\n                print(f\"Batch token count: ~{batch_tokens}\")\n\n                # Add to vector store\n                self.vector_store_manager.vector_store.add_documents(batch)\n\n                # Persist after each batch if supported\n                if hasattr(self.vector_store_manager.vector_store, \u0027persist\u0027):\n                    self.vector_store_manager.vector_store.persist()\n        except Exception as e:\n            print(f\"Error adding documents to vector store: {str(e)}\")\n            import traceback\n            traceback.print_exc()\n\n            # If we failed, we need to rebuild from scratch\n            print(\"Embedding failed. Rebuilding vector store with smaller chunks...\")\n            return self._process_all_documents(df, text_column, table_column)\n\n        # Add parent documents to docstore\n        for doc in all_parent_docs:\n            self.retriever.docstore.mset([(doc.metadata[\"id\"], doc)])\n\n        # Track all processed documents (existing + new)\n        all_processed_hashes \u003d processed_doc_ids + new_doc_hashes\n\n        # Update tracking file with all processed hashes\n        tracking_file \u003d \"processed_docs.pkl\"\n        \n        # Use the same storage mechanism as the document metadata\n        using_dataiku_folder \u003d hasattr(self.vector_store_manager, \u0027dataiku_folder\u0027) and self.vector_store_manager.dataiku_folder is not None\n        \n        if using_dataiku_folder:\n            # For Dataiku folder\n            with self.vector_store_manager.dataiku_folder.get_writer(tracking_file) as writer:\n                pickle.dump(all_processed_hashes, writer)\n        else:\n            # Local file system\n            tracking_path \u003d os.path.join(self.vector_store_manager.persist_directory, tracking_file)\n            with open(tracking_path, \"wb\") as f:\n                pickle.dump(all_processed_hashes, f)\n\n        print(f\"Updated tracking with {len(new_doc_hashes)} new document hashes, total: {len(all_processed_hashes)}\")\n\n        # Initialize query engine\n        self.query_engine \u003d QueryEngine(\n            self.retriever,\n            self.embedding_model,\n            self.llm\n        )\n\n        return self.retriever\n    \n    def load_retriever(self) -\u003e MultiVectorRetriever:\n        \"\"\"\n        Load an existing retriever from a saved vector store\n        \n        Returns:\n            MultiVectorRetriever: Loaded retriever\n        \"\"\"\n        self.retriever \u003d self.retriever_builder.load_retriever()\n        \n        # Initialize query engine\n        self.query_engine \u003d QueryEngine(\n            self.retriever,\n            self.embedding_model,\n            self.llm\n        )\n        \n        return self.retriever\n    \n    def query(self, query: str, return_context: bool \u003d True):\n        \"\"\"\n        Query the RAG system\n        \n        Args:\n            query: Query string\n            return_context: Whether to return the retrieved context\n            \n        Returns:\n            str or Tuple[str, Dict]: Response or (response, context)\n        \"\"\"\n        if not self.query_engine:\n            if not self.retriever:\n                self.load_retriever()\n            self.query_engine \u003d QueryEngine(\n                self.retriever,\n                self.embedding_model,\n                self.llm\n            )\n        \n        return self.query_engine.generate_response(query, return_context)\n    \n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "/data/dataiku/dss_data/code-envs/python/cdl-p-diku-psbts-py39-llm-env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n/opt/dataiku-dss-13.4.3/python/dataikuapi/dss/langchain/llm.py:138: PydanticDeprecatedSince20: `pydantic.config.Extra` is deprecated, use literal values instead (e.g. `extra\u003d\u0027allow\u0027`). Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.8/migration/\n  extra \u003d pydantic.Extra.forbid\n/opt/dataiku-dss-13.4.3/python/dataikuapi/dss/langchain/llm.py:302: PydanticDeprecatedSince20: `pydantic.config.Extra` is deprecated, use literal values instead (e.g. `extra\u003d\u0027allow\u0027`). Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.8/migration/\n  extra \u003d pydantic.Extra.forbid\n/opt/dataiku-dss-13.4.3/python/dataikuapi/dss/langchain/embeddings.py:24: PydanticDeprecatedSince20: `pydantic.config.Extra` is deprecated, use literal values instead (e.g. `extra\u003d\u0027allow\u0027`). Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.8/migration/\n  extra \u003d pydantic.Extra.forbid\n/data/dataiku/dss_data/code-envs/python/cdl-p-diku-psbts-py39-llm-env/lib/python3.9/site-packages/pydantic/_internal/_config.py:291: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.8/migration/\n  warnings.warn(DEPRECATION_MESSAGE, DeprecationWarning)\n/data/dataiku/dss_data/code-envs/python/cdl-p-diku-psbts-py39-llm-env/lib/python3.9/site-packages/pydantic/_internal/_config.py:341: UserWarning: Valid config keys have changed in V2:\n* \u0027underscore_attrs_are_private\u0027 has been removed\n  warnings.warn(message, UserWarning)\nSQLite3 is too old, Chroma would not load, trying to replace sqlite3. Please consider upgrading to sqlite3 \u003e\u003d 3.35\n",
          "name": "stderr"
        }
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "code_folding": [
          0
        ]
      },
      "source": [
        "# processor \u003d process_dataset_with_multi_vector(\n#     dataset_name\u003d\"input_data_extracted_custom\",\n#     output_dataset_name\u003d\"output_test\",\n#     text_column\u003d\"text_content\",\n#     table_column\u003d\"table_content\",\n#     image_folder_name\u003d\"input_images_extracted_custom\"\n# )\n# response \u003d query_multi_vector_store(\n#     vector_store_path\u003d\"./vector_store_input_data_extracted_custom\",\n#     query\u003d\"What is Electronic Certificate Document Number: US-IMM-180289?\",\n#     embedding_model_id\u003d\"custom:iliad-plugin-conn-prod:text-embedding-ada-002\",\n#     llm_id\u003d\"custom:iliad-plugin-conn-prod:gpt-4o\",\n# )\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# orchestrator"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "code_folding": [
          0
        ]
      },
      "source": [
        "# # Load dataset\n# dataset_name \u003d \"input_data_extracted_custom\"\n# dataset \u003d dataiku.Dataset(\"input_data_extracted_custom\")\n# df \u003d dataset.get_dataframe()\n\n# # Set default persist directory if not provided\n# persist_directory \u003d f\"./vector_store_{dataset_name}\"\n\n# text_column\u003d\"text_content\"\n# table_column\u003d\"table_content\"\n# embedding_model_id\u003d\"custom:iliad-plugin-conn-prod:text-embedding-ada-002\"\n# llm_id\u003d\"custom:iliad-plugin-conn-prod:gpt-4o\"\n# vector_store_type\u003d\"CHROMADB\"\n\n# pipeline \u003d create_and_use_rag_system(\n#     df \u003d df,\n#     text_column \u003d text_column,\n#     table_column \u003d table_column,\n#     embedding_model_id \u003d embedding_model_id,\n#     llm_id \u003d llm_id,\n#     vector_store_type \u003d vector_store_type,\n#     persist_directory \u003d persist_directory,\n#     chunk_size \u003d 1000,\n#     chunk_overlap \u003d 100,\n#     image_folder_name \u003d \"images_test\",\n#     k \u003d 5\n# )"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    },
    {
      "execution_count": 2,
      "cell_type": "code",
      "metadata": {
        "code_folding": []
      },
      "source": [
        "import uuid\nimport os\nimport re\nimport json\nfrom typing import List, Dict, Any, Optional, Tuple,Union\n\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport dataiku\nfrom PIL import Image\nimport io\nimport pickle\n# Import ChromaDB monkeypatch to ensure compatibility\nfrom dataiku.core.vector_stores.chroma_vector_store import ChromaVectorStore\nChromaVectorStore.run_the_ugly_chromadb_monkeypatch()\n\n# from Digitization.Core.RagPipeline import RAGPipeline\n\n# Load dataset\ndataset_name \u003d \"input_data_extracted_custom\"\ndataset \u003d dataiku.Dataset(\"input_data_extracted_custom\")\ndf \u003d dataset.get_dataframe()\n\n# Set default persist directory if not provided\npersist_directory \u003d f\"./vector_store_{dataset_name}\"\n\ntext_column\u003d\"text_content\"\ntable_column\u003d\"table_content\"\n\n# Initialize RAG pipeline\npipeline \u003d RAGPipeline(\n    embedding_model_id\u003d\"custom:iliad-plugin-conn-prod:text-embedding-ada-002\",\n    llm_id\u003d\"custom:iliad-plugin-conn-prod:gpt-4o\",\n    vector_store_type\u003d\"CHROMADB\",\n    persist_directory \u003d persist_directory,\n    vector_store_folder_name\u003d\"vector_store\"\n)\n\n\n# Process the dataset to build the retriever\nprint(f\"Processing dataset {dataset_name} with {len(df)} documents...\")\npipeline.process_dataset(df, text_column, table_column)\n\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "Initialized LangChain embedding model: custom:iliad-plugin-conn-prod:text-embedding-ada-002\nInitialized LangChain LLM: custom:iliad-plugin-conn-prod:gpt-4o\nUsing Dataiku folder: vector_store\nDataiku folder path: /data/dataiku/dss_data/managed_folders/GENAIPOC/tPuZfwpr\nDataiku folder contents: [\u0027/doc_metadata.pkl\u0027, \u0027/chroma.sqlite3\u0027, \u0027/24a7e438-50a8-426f-a249-1cb8a5ed0b6b/header.bin\u0027, \u0027/24a7e438-50a8-426f-a249-1cb8a5ed0b6b/data_level0.bin\u0027, \u0027/24a7e438-50a8-426f-a249-1cb8a5ed0b6b/length.bin\u0027, \u0027/24a7e438-50a8-426f-a249-1cb8a5ed0b6b/link_lists.bin\u0027, \u0027/processed_docs.pkl\u0027]\nRAGPipeline initialized with: vector_store_type\u003dCHROMADB, persist_directory\u003d./vector_store_input_data_extracted_custom, vector_store_folder_name\u003dvector_store\nDataiku folder path: /data/dataiku/dss_data/managed_folders/GENAIPOC/tPuZfwpr\nDataiku folder files: [\u0027/doc_metadata.pkl\u0027, \u0027/chroma.sqlite3\u0027, \u0027/24a7e438-50a8-426f-a249-1cb8a5ed0b6b/header.bin\u0027, \u0027/24a7e438-50a8-426f-a249-1cb8a5ed0b6b/data_level0.bin\u0027, \u0027/24a7e438-50a8-426f-a249-1cb8a5ed0b6b/length.bin\u0027, \u0027/24a7e438-50a8-426f-a249-1cb8a5ed0b6b/link_lists.bin\u0027, \u0027/processed_docs.pkl\u0027]\nProcessing dataset input_data_extracted_custom with 71 documents...\nLooking for ChromaDB files in Dataiku folder\nFound 1 ChromaDB files: [\u0027/chroma.sqlite3\u0027]\nChecking for vector store: True\nVector store directory: /data/dataiku/dss_data/managed_folders/GENAIPOC/tPuZfwpr\nDirectory exists and is accessible\nDirectory contents: [\u0027doc_metadata.pkl\u0027, \u0027chroma.sqlite3\u0027, \u002724a7e438-50a8-426f-a249-1cb8a5ed0b6b\u0027, \u0027processed_docs.pkl\u0027]\nFound existing vector store\nFound 151 previously processed documents\nFound 151 processed document hashes\nAttempting to load existing retriever\nAttempting to load vector store from /data/dataiku/dss_data/managed_folders/GENAIPOC/tPuZfwpr\nStore type: CHROMADB, Using Dataiku folder: True\nLoading ChromaDB from: /data/dataiku/dss_data/managed_folders/GENAIPOC/tPuZfwpr\nChromaDB loaded with 254 vectors\nLoaded metadata for 142 parent documents\nMulti-vector retriever loaded successfully with 142 parent documents\nSuccessfully loaded existing retriever\nChecking for new documents...\nTruncated document from ~4780 tokens to 4000 tokens\nTruncated document from ~7279 tokens to 4000 tokens\nTruncated document from ~5822 tokens to 4000 tokens\nTruncated document from ~6042 tokens to 4000 tokens\nTruncated document from ~4075 tokens to 4000 tokens\nTruncated document from ~24555 tokens to 4000 tokens\nTruncated document from ~8615 tokens to 4000 tokens\nTruncated document from ~22160 tokens to 4000 tokens\nTruncated document from ~4093 tokens to 4000 tokens\nNo new documents to process\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/tmp/ipykernel_3541150/99009926.py:392: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n  self.vector_store \u003d Chroma(\n",
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/plain": "MultiVectorRetriever(vectorstore\u003d\u003clangchain_community.vectorstores.chroma.Chroma object at 0x7fffe82ad1f0\u003e, docstore\u003d\u003clangchain_core.stores.InMemoryStore object at 0x7fffe8245bb0\u003e, id_key\u003d\u0027parent_id\u0027, search_kwargs\u003d{\u0027k\u0027: 3})"
          },
          "metadata": {}
        }
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#query\u003d\"How are bridge programs utilized in the treatment process when certain authorizations are denied?\"\n# query \u003d \"Who to call for RINVOQ reactions?\"\nquery \u003d \"What are support programs currently available under each drug brand?\"\n# Execute the query\nprint(f\"Executing query: {query}\")\nresponse, context \u003d pipeline.query(query, return_context\u003dTrue)\n# response, context \u003d pipeline.run(query)\n\nprint(f\"Query completed successfully\")\nprint(\"\\n\u003d\u003d\u003d RAG RESPONSE \u003d\u003d\u003d\")\nprint(response)\n\n# print(\"\\n\u003d\u003d\u003d RETRIEVED SOURCES \u003d\u003d\u003d\")\n# for i, doc in enumerate(context[\"retrieved_docs\"], 1):\n#     print(f\"SOURCE {i}: {doc[\u0027metadata\u0027].get(\u0027source\u0027, \u0027Unknown\u0027)}\")\n#     print(f\"TYPE: {doc[\u0027metadata\u0027].get(\u0027type\u0027, \u0027Unknown\u0027)}\")\n#     print(f\"CONTENT: {doc[\u0027content\u0027][:1500]}...\")\n#     print()\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# New heading"
      ]
    }
  ]
}