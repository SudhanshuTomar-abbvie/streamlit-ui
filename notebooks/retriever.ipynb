{
  "metadata": {
    "kernelspec": {
      "name": "py-dku-venv-cdl-p-diku-psbts-py39-llm-env",
      "display_name": "Python (env cdl-p-diku-psbts-py39-llm-env)",
      "language": "python"
    },
    "hide_input": false,
    "language_info": {
      "name": "python",
      "version": "3.9.20",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "createdOn": 1745315581103,
    "modifiedBy": "tomarsx1",
    "customFields": {},
    "creator": "tomarsx1",
    "tags": []
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%pylab inline"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\nfrom langchain_core.messages import HumanMessage, SystemMessage"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "client \u003d dataiku.api_client()\nproject \u003d client.get_default_project()\nllm_list \u003d project.list_llms()\nfor llm in llm_list:\n    print(f\"- {llm.description} (id: {llm.id})\")\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\nLLM_ID \u003d \"custom:iliad-plugin-conn-prod:gpt-4o\"  # Fill with a valid LLM_ID\nllm \u003d project.get_llm(LLM_ID)\nlcllm \u003d llm.as_langchain_llm()\nlcllmResp \u003d lcllm.invoke(\"When was the movie Citizen Kane released?\")\nprint(lcllmResp)\n\nquestion \u003d \"When was the movie Citizen Kane released?\"\nsystem_msg \u003d \"\"\"You are an expert in the history of American cinema.\nYou always answer questions with a lot of passion and enthusiasm.\n\"\"\"\n\nmessages \u003d [\n    SystemMessage(content\u003dsystem_msg),\n    HumanMessage(content\u003dquestion)\n]\n\nlcllmResp \u003d lcllm.invoke(messages)\nprint(lcllmResp)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true
      },
      "source": [
        "# ivector store chromadb"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        "import dataiku\nimport pandas as pd\nimport numpy as np\nfrom langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI\nfrom langchain.vectorstores import FAISS, Chroma\nfrom langchain.docstore.document import Document\nfrom langchain.schema import Document as LangChainDocument\nfrom langchain.embeddings.base import Embeddings\nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom langchain.retrievers.document_compressors import LLMChainExtractor\nimport pickle\nimport os\nfrom typing import List\nimport re\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom typing import List, Dict, Any, Tuple, Optional, Union\nfrom presidio_analyzer import AnalyzerEngine\nfrom presidio_anonymizer import AnonymizerEngine\nimport tiktoken\nfrom langchain.memory import ConversationBufferMemory, ConversationSummaryBufferMemory\nfrom langchain.chains import ConversationalRetrievalChain\nfrom langchain.memory import ChatMessageHistory\nfrom langchain.schema import SystemMessage, HumanMessage, AIMessage\nimport en_core_web_lg\nnlp \u003d en_core_web_lg.load()\n\n\nclass ModelDefination:\n    def __init__(self, embedding_model, llm_id, vector_store_type\u003d\"FAISS\"):\n        # Store model parameters\n        self.embedding_model_name \u003d embedding_model\n        self.llm_id \u003d llm_id\n        self.vector_store_type \u003d vector_store_type.upper()  # Normalize to uppercase\n        self.embedding_model \u003d None\n        self.llm \u003d None\n        self.client \u003d dataiku.api_client()\n        self.project \u003d self.client.get_default_project()\n\n        # Define paths for vector stores\n        self.faiss_index_path \u003d \"./faiss_index\"\n        self.chromadb_index_path \u003d \"./chromadb_index\"\n\n        # Initialize models\n        self._initialize_embedding_model()\n        self._initialize_llm()\n\n        if self.embedding_model is None:\n            raise ValueError(\"Embedding model could not be initialized\")\n        if self.llm is None:\n            raise ValueError(\"LLM could not be initialized\")\n\n        self.chat_history \u003d ChatMessageHistory()\n        self.buffer_memory \u003d ConversationBufferMemory(\n            memory_key\u003d\"chat_history\",\n            chat_memory\u003dself.chat_history,\n            return_messages\u003dTrue,\n            output_key\u003d\"answer\" \n        )\n\n        self.summary_memory \u003d ConversationSummaryBufferMemory(\n            llm\u003dself.llm,\n            memory_key\u003d\"chat_history\",\n            return_messages\u003dTrue,\n            max_token_limit\u003d1000,\n            output_key\u003d\"answer\"  \n        )\n\n        print(f\"Model Definition initialized with vector store type: {self.vector_store_type}\")\n\n    def _initialize_embedding_model(self):\n        try:\n            if self.embedding_model_name \u003d\u003d \"text-embedding-ada-002\":\n                client \u003d dataiku.api_client()\n                connection \u003d client.get_connection(\"text-embedding-ada-002\")\n                connection_params \u003d connection.get_info()[\"params\"]\n\n                available_deployments \u003d connection_params.get(\"availableDeployments\", [])\n                if not available_deployments:\n                    raise ValueError(\"No deployments found for embedding model.\")\n\n                self.embedding_deployment_name \u003d available_deployments[0][\"name\"]\n                model_name \u003d available_deployments[0][\"underlyingModelName\"]\n                azure_openai_endpoint \u003d f\"https://{connection_params[\u0027resourceName\u0027]}.openai.azure.com/\"\n\n                self.embedding_model \u003d AzureOpenAIEmbeddings(\n                    azure_endpoint\u003dazure_openai_endpoint,\n                    api_key\u003dconnection_params.get(\"apiKey\"),\n                    deployment\u003dself.embedding_deployment_name,\n                    model\u003dmodel_name,\n                    chunk_size\u003d1000\n                )\n                print(f\"Initialized embedding model: {model_name}\")\n\n            elif \"custom:iliad-plugin-conn-prod\" in self.embedding_model_name:\n                # Get the embedding model from the project and use the LangChain wrapper\n                emb_model \u003d self.project.get_llm(self.embedding_model_name)\n                self.embedding_model \u003d emb_model.as_langchain_embeddings()\n                print(f\"Initialized LangChain embedding model: {self.embedding_model_name}\")\n\n        except Exception as e:\n            print(f\"Error initializing embedding model: {str(e)}\")\n            import traceback\n            traceback.print_exc()\n\n    def _initialize_llm(self):\n        try:\n            if self.llm_id \u003d\u003d \"gpt-35-turbo-16k\":\n                connection \u003d self.client.get_connection(\"gpt-35-turbo-16k-2\")\n                connection_params \u003d connection.get_info()[\"params\"]\n\n                available_deployments_llm \u003d connection_params.get(\"availableDeployments\", [])\n                if not available_deployments_llm:\n                    raise ValueError(\"No deployments found for LLM.\")\n\n                llm_deployment_name \u003d available_deployments_llm[0][\"name\"]\n                llm_model_name \u003d available_deployments_llm[0][\"underlyingModelName\"]\n                azure_llm_endpoint \u003d f\"https://{connection_params[\u0027resourceName\u0027]}.openai.azure.com/\"\n\n                self.llm \u003d AzureChatOpenAI(\n                    azure_endpoint\u003dazure_llm_endpoint,\n                    api_key\u003dconnection_params.get(\"apiKey\"),\n                    deployment_name\u003dllm_deployment_name,\n                    model_name\u003dllm_model_name,\n                    temperature\u003d0.1,\n                    api_version\u003d\"2024-02-01\"\n                )\n                print(f\"Initialized Azure LLM: {llm_model_name}\")\n\n            elif \"custom:iliad-plugin-conn-prod\" in self.llm_id:\n                # Get the LLM model from the project and use the LangChain wrapper\n                llm_model \u003d self.project.get_llm(self.llm_id)\n                self.llm \u003d llm_model.as_langchain_llm()\n                print(f\"Initialized LangChain LLM: {self.llm_id}\")\n\n        except Exception as e:\n            print(f\"Failed to initialize LLM: {str(e)}\")\n            raise Exception(f\"Error initializing LLM: {str(e)}\")\n\n    def document_preparation(self, df):\n        \"\"\"\n        Prepare documents and create vector store index.\n        Supports both FAISS and ChromaDB.\n        \"\"\"\n        try:\n            if not isinstance(df, pd.DataFrame):\n                raise ValueError(\"The provided \u0027df\u0027 is not a pandas DataFrame\")\n\n            documents \u003d [\n                LangChainDocument(\n                    page_content\u003drow[\"chunk_text\"],\n                    metadata\u003d{\"id\": str(index), \"metadata\": row[\"metadata\"]}\n                )\n                for index, row in df.iterrows()\n            ]\n\n            # Create success flag\n            success \u003d False\n\n            if self.vector_store_type.upper() \u003d\u003d \"FAISS\":\n                store_path \u003d self.faiss_index_path\n                os.makedirs(store_path, exist_ok\u003dTrue)\n\n                if \"embeddings\" in df.columns:\n                    # Use pre-computed embeddings if available\n                    embeddings \u003d np.array([eval(embed) if isinstance(embed, str) else embed for embed in df[\"embeddings\"]])\n                    vectorstore \u003d FAISS.from_embeddings(\n                        text_embeddings\u003dzip(df[\"chunk_text\"], embeddings),\n                        embedding\u003dself.embedding_model,\n                        metadatas\u003d[{\"id\": str(index), \"metadata\": row[\"metadata\"]} for index, row in df.iterrows()]\n                    )\n                else:\n                    # Compute embeddings on-the-fly\n                    vectorstore \u003d FAISS.from_documents(documents, embedding\u003dself.embedding_model)\n\n                vectorstore.save_local(store_path)\n                print(f\"FAISS index saved successfully at {store_path}.\")\n                success \u003d True\n\n            elif self.vector_store_type.upper() \u003d\u003d \"CHROMADB\":\n                store_path \u003d self.chromadb_index_path\n                os.makedirs(store_path, exist_ok\u003dTrue)\n\n                try:\n                    # First check if we have pre-computed embeddings\n                    has_embeddings \u003d \"embeddings\" in df.columns\n                    print(f\"Creating ChromaDB with {\u0027pre-computed\u0027 if has_embeddings else \u0027on-the-fly\u0027} embeddings\")\n\n                    # With newer versions of ChromaDB and LangChain integration, we handle this differently\n                    # Create basic documents without embeddings first\n                    vectorstore \u003d Chroma.from_documents(\n                        documents\u003ddocuments,\n                        embedding\u003dself.embedding_model,\n                        persist_directory\u003dstore_path\n                    )\n\n                    # If we have pre-computed embeddings, we\u0027ll try to add them\n                    if has_embeddings:\n                        try:\n                            # Get the raw client from the LangChain wrapper\n                            collection \u003d vectorstore._collection\n\n                            # Get the document IDs\n                            ids \u003d [str(i) for i in range(len(documents))]\n\n                            # Process embeddings from DataFrame\n                            processed_embeddings \u003d []\n                            for embed in df[\"embeddings\"]:\n                                if isinstance(embed, str):\n                                    try:\n                                        # Convert string representation to list\n                                        embed_list \u003d eval(embed)\n                                        processed_embeddings.append(embed_list)\n                                    except:\n                                        print(f\"Warning: Could not parse embedding string\")\n                                        # Return None for this embedding to flag it\n                                        processed_embeddings.append(None)\n                                else:\n                                    # If it\u0027s already a list/array, just append it\n                                    processed_embeddings.append(embed)\n\n                            # Check if we have valid embeddings for all documents\n                            if all(embed is not None for embed in processed_embeddings) and len(processed_embeddings) \u003d\u003d len(documents):\n                                print(f\"Adding {len(processed_embeddings)} pre-computed embeddings to ChromaDB\")\n\n                                # Use lower-level API to update the embeddings\n                                # This approach varies by ChromaDB version, so we\u0027ll try multiple methods\n\n                                try:\n                                    # Method 1: Try using the newer ChromaDB API (v0.4.0+)\n                                    # This recreates the collection with embeddings\n                                    texts \u003d [doc.page_content for doc in documents]\n                                    metadatas \u003d [doc.metadata for doc in documents]\n\n                                    # Delete existing collection\n                                    vectorstore.delete_collection()\n\n                                    # Create new collection with embeddings\n                                    import chromadb\n                                    from chromadb.config import Settings\n\n                                    # Try to get client with proper settings\n                                    try:\n                                        client \u003d chromadb.PersistentClient(path\u003dstore_path)\n                                    except:\n                                        # Fallback to basic client\n                                        client \u003d chromadb.Client()\n\n                                    # Create collection\n                                    collection \u003d client.create_collection(name\u003d\"langchain\")\n\n                                    # Add documents with embeddings\n                                    collection.add(\n                                        documents\u003dtexts,\n                                        embeddings\u003dprocessed_embeddings,\n                                        metadatas\u003dmetadatas,\n                                        ids\u003dids\n                                    )\n\n                                    # Reconnect LangChain wrapper to the collection\n                                    vectorstore \u003d Chroma(\n                                        client\u003dclient,\n                                        collection_name\u003d\"langchain\",\n                                        embedding_function\u003dself.embedding_model,\n                                        persist_directory\u003dstore_path \n                                    )\n\n                                    print(\"Successfully added embeddings using ChromaDB native API\")\n\n                                except Exception as e1:\n                                    print(f\"Method 1 failed: {e1}\")\n\n                                    try:\n                                        # Method 2: Try using the LangChain wrapper\u0027s API\n                                        # This might work with some versions\n                                        for i, doc in enumerate(documents):\n                                            vectorstore.add_embeddings(\n                                                texts\u003d[doc.page_content],\n                                                embeddings\u003d[processed_embeddings[i]],\n                                                metadatas\u003d[doc.metadata]\n                                            )\n                                        print(\"Successfully added embeddings using LangChain wrapper\")\n\n                                    except Exception as e2:\n                                        print(f\"Method 2 failed: {e2}\")\n\n                                        try:\n                                            # Method 3: Fallback to direct collection update\n                                            # This works with some versions\n                                            for i, doc_id in enumerate(ids):\n                                                collection.update(\n                                                    ids\u003d[doc_id],\n                                                    embeddings\u003d[processed_embeddings[i]]\n                                                )\n                                            print(\"Successfully updated embeddings using direct collection update\")\n\n                                        except Exception as e3:\n                                            print(f\"Method 3 failed: {e3}\")\n                                            print(\"Could not add pre-computed embeddings, using model-generated embeddings instead\")\n                            else:\n                                print(\"Some embeddings could not be parsed, using model-generated embeddings instead\")\n                        except Exception as embed_error:\n                            print(f\"Error working with pre-computed embeddings: {embed_error}\")\n\n                    # Persist ChromaDB to disk\n                    try:\n                        vectorstore.persist()\n                        print(f\"ChromaDB index saved successfully at {store_path}.\")\n                        success \u003d True\n                    except Exception as persist_error:\n                        print(f\"Warning: Could not persist ChromaDB: {persist_error}\")\n                        # Even if persist fails, we might still have a working in-memory DB\n                        success \u003d True\n\n                except Exception as chroma_error:\n                    print(f\"ChromaDB integration error: {chroma_error}\")\n                    import traceback\n                    traceback.print_exc()\n\n                    # Try a simplified approach as last resort\n                    try:\n                        print(\"Trying simplified ChromaDB approach...\")\n                        vectorstore \u003d Chroma.from_documents(\n                            documents\u003ddocuments,\n                            embedding\u003dself.embedding_model,\n                            persist_directory\u003dstore_path\n                        )\n                        success \u003d True\n                        print(\"Simplified ChromaDB approach succeeded\")\n                    except Exception as simple_error:\n                        print(f\"Simplified ChromaDB approach failed: {simple_error}\")\n                        success \u003d False\n\n            else:\n                raise ValueError(f\"Unsupported vector store type: {self.vector_store_type}\")\n\n            return success\n\n        except Exception as e:\n            print(f\"Failed to create index in Vector Store: {str(e)}\")\n            import traceback\n            traceback.print_exc()\n            return False\n\n        \n\n    def _compute_semantic_similarity(self, query_vector, document_vectors):\n        \"\"\"\n        Compute semantic similarity between query vector and document vectors.\n        \"\"\"\n        if not document_vectors:\n            return []\n\n        query_vector \u003d np.array(query_vector).reshape(1, -1)\n        document_vectors \u003d np.array(document_vectors)\n        return cosine_similarity(query_vector, document_vectors).flatten()\n\n    def _preprocess_query(self, query):\n        \"\"\"\n        Preprocess the query to extract key terms and concepts.\n        \"\"\"\n        stop_words \u003d {\u0027a\u0027, \u0027an\u0027, \u0027the\u0027, \u0027and\u0027, \u0027or\u0027, \u0027but\u0027, \u0027if\u0027, \u0027because\u0027, \u0027as\u0027, \u0027what\u0027, \u0027when\u0027, \u0027where\u0027, \u0027how\u0027, \u0027is\u0027, \u0027are\u0027, \u0027was\u0027, \u0027were\u0027}\n        query \u003d re.sub(r\u0027[^\\w\\s]\u0027, \u0027 \u0027, query.lower())\n        words \u003d query.split()\n        key_terms \u003d [word for word in words if word not in stop_words and len(word) \u003e 2]\n        entities \u003d re.findall(r\u0027\\b[A-Z][a-zA-Z]+\\b\u0027, query)\n        return {\"original\": query, \"key_terms\": key_terms, \"entities\": entities}\n\n    def _rerank_documents(self, query, documents, embedded_query\u003dNone):\n        \"\"\"\n        Rerank documents based on semantic and lexical relevance\n        \"\"\"\n        if not documents:\n            return []\n        \n        # Get query components\n        query_info \u003d self._preprocess_query(query)\n        key_terms \u003d query_info[\u0027key_terms\u0027]\n        \n        # Get embeddings for reranking if not provided\n        if embedded_query is None and hasattr(self.embedding_model, \u0027embed_query\u0027):\n            embedded_query \u003d self.embedding_model.embed_query(query)\n        \n        # Extract document embeddings\n        doc_embeddings \u003d []\n        for doc in documents:\n            if hasattr(doc, \u0027metadata\u0027) and \u0027embedding\u0027 in doc.metadata:\n                doc_embeddings.append(doc.metadata[\u0027embedding\u0027])\n            else:\n                # If no embedding in metadata, create one\n                if hasattr(self.embedding_model, \u0027embed_documents\u0027):\n                    doc_embedding \u003d self.embedding_model.embed_documents([doc.page_content])[0]\n                    doc_embeddings.append(doc_embedding)\n        \n        # Compute semantic similarity if we have embeddings\n        if embedded_query is not None and doc_embeddings:\n            semantic_scores \u003d self._compute_semantic_similarity(embedded_query, doc_embeddings)\n        else:\n            semantic_scores \u003d [0] * len(documents)\n        \n        # Compute lexical similarity (term frequency)\n        lexical_scores \u003d []\n        for doc in documents:\n            content \u003d doc.page_content.lower()\n            term_matches \u003d sum(1 for term in key_terms if term in content)\n            lexical_scores.append(term_matches / max(1, len(key_terms)))\n        \n        # Combine scores (0.7 semantic, 0.3 lexical)\n        combined_scores \u003d [0.7 * sem + 0.3 * lex for sem, lex in zip(semantic_scores, lexical_scores)]\n        \n        # Create result tuples with score and document\n        results \u003d [(score, doc) for score, doc in zip(combined_scores, documents)]\n        \n        # Sort by score (descending)\n        results.sort(reverse\u003dTrue, key\u003dlambda x: x[0])\n        \n        # Return only the documents with their scores\n        return [(doc, score) for score, doc in results]\n    \n    def get_conversational_chain(self, retriever, memory_type\u003d\"summary\"):\n        if memory_type \u003d\u003d \"summary\":\n            memory \u003d ConversationSummaryBufferMemory(\n                llm\u003dself.llm,\n                memory_key\u003d\"chat_history\",\n                return_messages\u003dTrue,\n                max_token_limit\u003d1000,\n                output_key\u003d\"answer\"  \n            )\n        else:\n            memory \u003d ConversationBufferMemory(\n                memory_key\u003d\"chat_history\",\n                chat_memory\u003dself.chat_history,\n                return_messages\u003dTrue,\n                output_key\u003d\"answer\"  \n            )\n\n        conversation_chain \u003d ConversationalRetrievalChain.from_llm(\n            llm\u003dself.llm,\n            retriever\u003dretriever,\n            memory\u003dmemory,\n            return_source_documents\u003dTrue,\n            verbose\u003dTrue\n        )\n        return conversation_chain\n    \n    def get_retriever(self, df\u003dNone, top_k\u003d10):\n        \"\"\"\n        Get a retriever based on the configured vector store type.\n        Will use an existing index or create a new one if df is provided.\n        \"\"\"\n        try:\n            vectorstore \u003d None\n            \n            # Check if we need to create the index\n            if df is not None:\n                self.document_preparation(df)\n            \n            # Try to load the appropriate vector store\n            if self.vector_store_type.upper() \u003d\u003d \"FAISS\":\n                if os.path.exists(self.faiss_index_path):\n                    vectorstore \u003d FAISS.load_local(\n                        self.faiss_index_path, \n                        self.embedding_model, \n                        allow_dangerous_deserialization\u003dTrue\n                    )\n                    print(f\"Successfully loaded FAISS vector store from {self.faiss_index_path}\")\n                else:\n                    print(f\"FAISS index not found at {self.faiss_index_path}\")\n                    if df is not None:\n                        print(\"Creating FAISS index from provided DataFrame...\")\n                        success \u003d self.document_preparation(df)\n                        if success:\n                            vectorstore \u003d FAISS.load_local(\n                                self.faiss_index_path, \n                                self.embedding_model, \n                                allow_dangerous_deserialization\u003dTrue\n                            )\n                            print(f\"Successfully created and loaded FAISS vector store\")\n                        else:\n                            raise ValueError(\"Failed to create FAISS index\")\n                    else:\n                        raise FileNotFoundError(f\"FAISS index not found at {self.faiss_index_path} and no DataFrame provided to create it\")\n            \n            elif self.vector_store_type.upper() \u003d\u003d \"CHROMADB\":\n                if os.path.exists(self.chromadb_index_path):\n                    vectorstore \u003d Chroma(\n                        persist_directory\u003dself.chromadb_index_path,\n                        embedding_function\u003dself.embedding_model\n                    )\n                    print(f\"Successfully loaded ChromaDB vector store from {self.chromadb_index_path}\")\n                else:\n                    print(f\"ChromaDB index not found at {self.chromadb_index_path}\")\n                    if df is not None:\n                        print(\"Creating ChromaDB index from provided DataFrame...\")\n                        success \u003d self.document_preparation(df)\n                        if success:\n                            vectorstore \u003d Chroma(\n                                persist_directory\u003dself.chromadb_index_path,\n                                embedding_function\u003dself.embedding_model\n                            )\n                            print(f\"Successfully created and loaded ChromaDB vector store\")\n                        else:\n                            raise ValueError(\"Failed to create ChromaDB index\")\n                    else:\n                        raise FileNotFoundError(f\"ChromaDB index not found at {self.chromadb_index_path} and no DataFrame provided to create it\")\n            \n            # Create retriever from the loaded vector store\n            if vectorstore:\n                retriever \u003d vectorstore.as_retriever(search_type\u003d\"similarity\", search_kwargs\u003d{\"k\": top_k})\n                return retriever\n            else:\n                raise ValueError(f\"Could not create retriever for {self.vector_store_type}\")\n                \n        except Exception as e:\n            print(f\"Error getting retriever: {str(e)}\")\n            import traceback\n            traceback.print_exc()\n            return None\n    \n    def retrieve_relevant_chunks(self, df, query, top_k\u003d10):\n        \"\"\"\n        Retrieve relevant chunks based on query similarity with improved relevance\n        using the configured vector store type (FAISS or ChromaDB).\n        \"\"\"\n        try:\n            print(f\"Retrieving documents for query: {query[:50]}... using {self.vector_store_type}\")\n\n            # Get or create a retriever for the specified vector store type\n            retriever \u003d self.get_retriever(df, top_k * 2)  # Get more for reranking\n            \n            if not retriever:\n                print(\"Failed to create retriever\")\n                return []\n\n            # Make sure the query is embedded \n            embedded_query \u003d None\n            try:\n                if hasattr(self.embedding_model, \u0027embed_query\u0027):\n                    embedded_query \u003d self.embedding_model.embed_query(query)\n                elif hasattr(self.embedding_model, \u0027encode\u0027):\n                    embedded_query \u003d self.embedding_model.encode(query)\n            except Exception as embed_error:\n                print(f\"Warning: Failed to explicitly embed query: {embed_error}\")\n\n            # Retrieve relevant documents\n            try:\n                print(\"Attempting to retrieve documents...\")\n                # Use the standard LangChain invoke method\n                initial_docs \u003d retriever.invoke(query)\n                print(f\"Successfully retrieved {len(initial_docs)} documents for reranking\")\n            except Exception as invoke_error:\n                print(f\"Failed to use invoke method: {invoke_error}\")\n                # Fall back to the older method\n                try:\n                    if self.vector_store_type.upper() \u003d\u003d \"FAISS\":\n                        # For FAISS, try similarity_search_by_vector if invoke failed\n                        vectorstore \u003d FAISS.load_local(\n                            self.faiss_index_path, \n                            self.embedding_model, \n                            allow_dangerous_deserialization\u003dTrue\n                        )\n                        if embedded_query is not None:\n                            initial_docs \u003d vectorstore.similarity_search_by_vector(embedded_query, k\u003dtop_k * 2)\n                        else:\n                            initial_docs \u003d vectorstore.similarity_search(query, k\u003dtop_k * 2)\n                    \n                    elif self.vector_store_type.upper() \u003d\u003d \"CHROMADB\":\n                        # For ChromaDB, try direct search methods\n                        chromadb_store \u003d Chroma(\n                            persist_directory\u003dself.chromadb_index_path,\n                            embedding_function\u003dself.embedding_model\n                        )\n                        if embedded_query is not None:\n                            # For ChromaDB with embedded query\n                            initial_docs \u003d chromadb_store.similarity_search_by_vector(embedded_query, k\u003dtop_k * 2)\n                        else:\n                            # Standard text search\n                            initial_docs \u003d chromadb_store.similarity_search(query, k\u003dtop_k * 2)\n                    \n                    print(f\"Successfully retrieved {len(initial_docs)} documents using fallback method\")\n                except Exception as retrieve_error:\n                    print(f\"Failed to retrieve documents: {retrieve_error}\")\n                    return []\n\n            # Rerank the documents for better relevance\n            reranked_docs \u003d self._rerank_documents(query, initial_docs, embedded_query)\n\n            # Take top k after reranking\n            top_docs \u003d reranked_docs[:top_k]\n\n            # Format the results\n            results \u003d []\n            for doc, score in top_docs:\n                result \u003d {\n                    \"chunk_text\": doc.page_content,\n                    \"metadata\": doc.metadata,\n                    \"score\": score\n                }\n                results.append(result)\n\n            return results\n\n        except Exception as e:\n            print(f\"Failed to retrieve data from Vector Store: {str(e)}\")\n            import traceback\n            traceback.print_exc()\n            return []\n    \n    def mask_pii_from_chunks(self, retrieved_chunks):\n        \"\"\"\n        Mask PII in retrieved document chunks using Presidio.\n        \"\"\"\n        # Validate the input\n        if not isinstance(retrieved_chunks, list):\n            raise ValueError(\"retrieved_chunks must be a list of dictionaries\")\n\n        analyzer \u003d AnalyzerEngine()\n        anonymizer \u003d AnonymizerEngine()\n\n        masked_results \u003d []\n\n        for chunk in retrieved_chunks:\n            original_text \u003d chunk.get(\"chunk_text\", \"\")\n            metadata \u003d chunk.get(\"metadata\", {})\n            score \u003d chunk.get(\"score\", 0)\n\n            # Analyze for PII with error handling\n            try:\n                analysis_results \u003d analyzer.analyze(text\u003doriginal_text, entities\u003dNone, language\u003d\u0027en\u0027)\n            except Exception as e:\n                print(f\"PII analysis failed for chunk: {original_text[:30]}... Error: {e}\")\n                analysis_results \u003d []\n\n            # Convert PII detection results to JSON-like format\n            pii_entities \u003d [\n                {\n                    \"entity_type\": result.entity_type,\n                    \"start\": result.start,\n                    \"end\": result.end,\n                    \"score\": result.score\n                }\n                for result in analysis_results\n            ]\n\n            # Anonymize the text with error handling\n            try:\n                anonymized_result \u003d anonymizer.anonymize(text\u003doriginal_text, analyzer_results\u003danalysis_results)\n                masked_text \u003d anonymized_result.text\n            except Exception as e:\n                print(f\"Anonymization failed for chunk: {original_text[:30]}... Error: {e}\")\n                masked_text \u003d original_text  # Fallback to original if anonymization fails\n\n            masked_results.append({\n                \"original_text\": original_text,\n                \"masked_text\": masked_text,\n                \"pii_entities\": pii_entities,\n                \"metadata\": metadata,\n                \"score\": score\n            })\n\n        return masked_results\n\n    def ask_question(self, query, df\u003dNone, memory_type\u003d\"summary\"):\n        \"\"\"\n        Ask a question to the bot using the specified memory type.\n        memory_type: \u0027summary\u0027 or \u0027buffer\u0027\n        \"\"\"\n        try:\n            # Get retriever, creating index if needed\n            retriever \u003d self.get_retriever(df)\n            if not retriever:\n                return f\"Error: Could not create retriever for {self.vector_store_type}\"\n                \n            # Create the conversation chain\n            chain \u003d self.get_conversational_chain(retriever, memory_type\u003dmemory_type)\n            result \u003d chain.invoke({\"question\": query})\n            return result[\"answer\"]\n        except Exception as e:\n            print(f\"Error asking question: {e}\")\n            import traceback\n            traceback.print_exc()\n            return f\"Error processing question: {str(e)}\"\n    \n    def generate_llm_response(self, user_query, df, system_prompt\u003dNone, return_context\u003dTrue):\n        \"\"\"\n        Generate LLM response based on retrieved chunks with improved relevance and conversational memory.\n        Uses LangChain-compatible LLM invocation.\n        \"\"\"\n        print(f\"Generating LLM response for query: {user_query[:50]}... using {self.vector_store_type}\")\n\n        if self.llm is None:\n            return (\"Error: No LLM instance available\", []) if return_context else \"Error: No LLM instance available\"\n\n        try:\n            # Step 1: Extract key terms from the user query for better retrieval\n            query_info \u003d self._preprocess_query(user_query)\n            key_terms \u003d query_info[\u0027key_terms\u0027]\n\n            print(f\"Identified key terms: {key_terms}\")\n\n            # Step 2: Create an augmented query with the key terms emphasized\n            augmented_query \u003d user_query\n            if key_terms:\n                augmented_query \u003d f\"{user_query} [KEY TERMS: {\u0027, \u0027.join(key_terms)}]\"\n\n            # Step 3: Retrieve relevant chunks using the configured vector store\n            relevant_chunks \u003d self.retrieve_relevant_chunks(df, augmented_query, top_k\u003d10)\n\n            if not relevant_chunks:\n                return (\n                    \"No relevant information found in the provided documents.\",\n                    []\n                ) if return_context else \"No relevant information found in the provided documents.\"\n\n            print(\"Retrieved Relevant Chunks:\", len(relevant_chunks))\n            for i, chunk in enumerate(relevant_chunks, 1):\n                print(f\"\\nResult {i}:\")\n                print(f\"Chunk Text: {chunk[\u0027chunk_text\u0027][:100]}...\")  # Print just the beginning\n                print(f\"Metadata: {chunk[\u0027metadata\u0027]}\")\n                if chunk[\"score\"] is not None:\n                    print(f\"Relevance Score: {chunk[\u0027score\u0027]:.4f}\")\n\n            # Step 4: Mask PII in the retrieved chunks\n            print(\"Masking PII from retrieved chunks...\")\n            masked_chunks \u003d self.mask_pii_from_chunks(relevant_chunks)\n\n            # Step 5: Prepare formatted context using the masked text\n            formatted_chunks \u003d \"\"\n            for i, chunk in enumerate(masked_chunks, 1):\n                formatted_chunks +\u003d f\"CHUNK {i}:\\n{chunk[\u0027masked_text\u0027]}\\n\"\n                formatted_chunks +\u003d f\"Source: {chunk[\u0027metadata\u0027].get(\u0027source\u0027, \u0027Unknown\u0027)}\\n\\n\"\n\n            # Step 6: Create custom prompt\n            system_message_content \u003d system_prompt if system_prompt else \"\"\"\n                You are an AI assistant that delivers precise, concise answers from provided documents. Follow these guidelines:\n\n                1. Analyze ALL content thoroughly for ANY information related to the query, even partial matches.\n                2. Present information directly without phrases like \"the document chunks provide\" or \"Document 1/2/3 states.\"\n                3. If referencing a specific document, use its actual name (e.g., \"According to the iEngage Quick Reference Guide\").\n                4. Provide ONLY relevant facts without adding commentary, introductions, or conclusions.\n                5. Format information as direct statements rather than numbered lists unless specifically requested.\n                6. Include ALL potentially relevant details no matter how minimal.\n                7. Present information in a concise, summarized format.\n                8. Only state information is insufficient if absolutely nothing related can be found.\n                9. Never generate content containing hate speech, offensive language, violence, threats, or misinformation.\n\n                User Query:\n                {query}\n\n                Document Chunks:\n                {context}\n\n                Answer:\n                \"\"\"\n\n            # Step 7: Create LangChain compatible messages\n            messages \u003d [\n                SystemMessage(content\u003dsystem_message_content),\n                HumanMessage(content\u003df\"\"\"\n                User Query:\n                {user_query}\n\n                Document Chunks:\n                {formatted_chunks}\n                \"\"\")\n            ]\n\n            # Step 8: Call the LLM with the messages using LangChain invoke\n            print(\"Sending prompt to LLM...\")\n            try:\n                response \u003d self.llm.invoke(messages)\n                print(\"Successfully received LLM response\")\n\n                # Step 9: Extract the response text\n                if hasattr(response, \"content\"):\n                    final_response \u003d response.content\n                else:\n                    final_response \u003d str(response)\n\n                # Step 10: Update memory using ask_question\n                try:\n                    # Create retriever for memory update\n                    retriever \u003d self.get_retriever(df)\n                    if retriever:\n                        memory_chain \u003d self.get_conversational_chain(retriever, memory_type\u003d\"summary\")\n                        _ \u003d memory_chain.invoke({\"question\": user_query})\n                except Exception as mem_error:\n                    print(f\"Warning: Could not update conversation memory: {mem_error}\")\n\n                # Step 11: Prepare context for optional return\n                context_dict \u003d {\n                    \"relevant_chunks\": masked_chunks,\n                    \"formatted_context\": formatted_chunks,\n                    \"query\": user_query,\n                    \"key_terms\": key_terms,\n                    \"vector_store_used\": self.vector_store_type\n                }\n\n                if return_context:\n                    return final_response, context_dict\n                else:\n                    return final_response\n\n            except ValueError as ve:\n                print(f\"ValueError encountered: {ve}\")\n                error_msg \u003d f\"Error generating response: {ve}\"\n                return (error_msg, []) if return_context else error_msg\n\n        except Exception as e:\n            print(f\"Error in generate_llm_response: {str(e)}\")\n            import traceback\n            traceback.print_exc()\n            error_msg \u003d f\"Error generating response: {str(e)}\"\n            return (error_msg, []) if return_context else error_msg\n        \n    def count_tokens(self, text, encoding_name\u003d\"cl100k_base\"):  # cl100k_base is used by GPT-4 models\n        \"\"\"Count the number of tokens in the given text using tiktoken library.\"\"\"\n        try:\n            encoding \u003d tiktoken.get_encoding(encoding_name)\n            return len(encoding.encode(text))\n        except Exception as e:\n            print(f\"Error counting tokens: {str(e)}\")\n            # Fallback method: roughly estimate tokens as 4 characters per token\n            return len(text) // 4\n\n    def generate_response_from_extracted_text(self, user_query: str, extracted_text: str, token_threshold: int \u003d 3500):\n        \"\"\"\n        Generate LLM response from raw extracted document content and user query.\n        If the input size exceeds the LLM threshold, return a message to update the knowledge base, or call update_knowledgebank endpoint.\n\n        Args:\n            user_query (str): The question or query from the user.\n            extracted_text (str): Full extracted content from documents.\n            token_threshold (int): Max allowed token count before triggering overflow condition.\n\n        Returns:\n            str: Final LLM response or a message indicating token overflow.\n        \"\"\"\n        print(f\"Received query: {user_query}\")\n        if self.llm is None:\n            return \"Error: No LLM instance available.\"\n\n        try:\n            # Step 1: Count LLM input token threshold\n            estimated_tokens \u003d self.count_tokens(extracted_text)\n            print(f\"Estimated token count of context: {estimated_tokens}\")\n\n            if estimated_tokens \u003e token_threshold:\n                print(\"Token threshold exceeded.\")\n                return (\"LLM input threshold exceeded. Please use the \u0027update_knowledgebank\u0027 endpoint to reduce document size or chunk it appropriately.\")\n\n            # Step 2: Preprocess query to extract key terms\n            query_info \u003d self._preprocess_query(user_query)\n            key_terms \u003d query_info[\u0027key_terms\u0027]\n            print(f\"Identified key terms: {key_terms}\")\n\n            # Step 3: Analyze the text for PII and mask it\n            try:\n                analyzer \u003d AnalyzerEngine()\n                anonymizer \u003d AnonymizerEngine()\n\n                analysis_results \u003d analyzer.analyze(text\u003dextracted_text, entities\u003dNone, language\u003d\u0027en\u0027)\n                anonymized_result \u003d anonymizer.anonymize(text\u003dextracted_text, analyzer_results\u003danalysis_results)\n                masked_text \u003d anonymized_result.text\n\n                print(\"PII masking applied to extracted text\")\n            except Exception as pii_error:\n                print(f\"Warning: PII masking failed: {pii_error}\")\n                masked_text \u003d extracted_text  # Fallback to original if anonymization fails\n\n            # Step 4: Define prompt\n            system_prompt \u003d \"\"\"\n            You are an AI assistant that delivers precise, concise answers from provided documents. Follow these guidelines:\n                1. Analyze ALL content thoroughly for ANY information related to the query, even partial matches.\n                2. Present information directly without phrases like \"the document chunks provide\" or \"Document 1/2/3 states.\"\n                3. If referencing a specific document, use its actual name (e.g., \"According to the iEngage Quick Reference Guide\").\n                4. Provide ONLY relevant facts without adding commentary, introductions, or conclusions.\n                5. Format information as direct statements rather than numbered lists unless specifically requested.\n                6. Include ALL potentially relevant details no matter how minimal.\n                7. Present information in a concise, summarized format.\n                8. Only state information is insufficient if absolutely nothing related can be found.\n                9. Never generate content containing hate speech, offensive language, violence, threats, or misinformation.\n\n                User Query:\n                {query}\n\n                Document Context:\n                {context}\n\n                Answer:\n            \"\"\"\n\n            # Step 5: Create LangChain-compatible messages\n            messages \u003d [\n                SystemMessage(content\u003dsystem_prompt),\n                HumanMessage(content\u003df\"\"\"\n                User Query:\n                {user_query}\n\n                Key Terms: {\u0027, \u0027.join(key_terms)}\n\n                Document Context:\n                {masked_text}\n                \"\"\")\n            ]\n\n            # Step 6: Call the LLM\n            print(\"Calling LLM with query and context...\")\n            try:\n                response \u003d self.llm.invoke(messages)\n                print(\"Successfully received LLM response\")\n\n                # Step 7: Parse and return response\n                if hasattr(response, \"content\"):\n                    return response.content\n                return str(response)\n\n            except ValueError as ve:\n                print(f\"ValueError encountered: {ve}\")\n                return f\"Error generating response: {ve}\"\n\n            except Exception as llm_error:\n                print(f\"LLM invocation error: {llm_error}\")\n                return f\"Error generating response: {str(llm_error)}\"\n\n        except Exception as e:\n            print(f\"Exception in generate_response_from_extracted_text: {str(e)}\")\n            import traceback\n            traceback.print_exc()\n            return f\"Error generating response: {str(e)}\"\n\n    "
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        "# Libraries\nimport dataiku\nimport pandas as pd\nimport numpy as np\nfrom langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI\nfrom langchain.vectorstores import FAISS, Chroma\nfrom langchain.docstore.document import Document\nfrom langchain.schema import Document as LangChainDocument\nfrom langchain.embeddings.base import Embeddings\nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom langchain.retrievers.document_compressors import LLMChainExtractor\nimport pickle\nimport os\nfrom typing import List\nimport re\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport json\nfrom langchain.schema import SystemMessage, HumanMessage\n\n\"\"\"\nhost \u003d \"https://cdl-dku-dev-desi.commercial-datalake-prod.awscloud.abbvienet.com/\"\napiKey \u003d \"BDxYeBpzVM2ZMlFD9wEcUTkpsxfsPvxk\"\nos.environ[\"DKU_CURRENT_PROJECT_KEY\"] \u003d \"CDLADMIN\" \ndataiku.set_remote_dss(host, apiKey, no_check_certificate\u003dTrue)\n\"\"\"\n\n            \nclass VectorStoreGeneration:\n    def __init__(self, input_dataset_name, output_dataset_name, user_query, embedding_model, llm, vector_store_type, top_k, use_compression, azure_openai_key):\n        self.input_dataset_name \u003d input_dataset_name\n        self.output_dataset_name \u003d output_dataset_name\n        self.user_query \u003d user_query\n        \n        # Define the embedding model and LLM first\n        self.embedding_model_name \u003d embedding_model\n        self.llm_model_name \u003d llm\n        self.vector_store_type \u003d vector_store_type\n        \n        # Create the model definition object with the model names\n        self.ModelDef \u003d ModelDefination(\n            embedding_model\u003dself.embedding_model_name, \n            llm_id\u003dself.llm_model_name, \n            vector_store_type\u003dvector_store_type\n        )\n        \n        self.top_k \u003d top_k\n        # We don\u0027t use compression as per the requirement\n        self.use_compression \u003d False\n\n    \n    def process(self):\n        try:\n            dataset \u003d dataiku.Dataset(self.input_dataset_name)\n            df \u003d dataset.get_dataframe()\n\n            success \u003d self.ModelDef.document_preparation(df)\n            if not success:\n                print(\"Failed to build vector index.\")\n                return\n            \n            # Generate LLM response and get context\n            llm_response, context_dict \u003d self.ModelDef.generate_llm_response(\n                self.user_query, \n                df\n            )\n\n            # Extract the formatted context from the context dictionary\n            formatted_context \u003d \"\"\n            if isinstance(context_dict, dict) and \"formatted_context\" in context_dict:\n                formatted_context \u003d context_dict[\"formatted_context\"]\n            elif isinstance(context_dict, dict) and \"relevant_chunks\" in context_dict:\n                # Create formatted context from relevant chunks if needed\n                chunks \u003d context_dict[\"relevant_chunks\"]\n                for i, chunk in enumerate(chunks, 1):\n                    formatted_context +\u003d f\"CHUNK {i}:\\n{chunk[\u0027chunk_text\u0027]}\\n\"\n                    if \u0027metadata\u0027 in chunk and isinstance(chunk[\u0027metadata\u0027], dict):\n                        formatted_context +\u003d f\"Source: {chunk[\u0027metadata\u0027].get(\u0027source\u0027, \u0027Unknown\u0027)}\\n\\n\"\n            \n\n            # Construct the final result with additional metadata\n            result \u003d {\n                \"question\": self.user_query,\n                \"answer\": llm_response,\n                \"contexts\": formatted_context,\n            }\n            # Use json.dumps with a default function that converts non-serializable objects to strings.\n            json_result \u003d json.dumps(result, default\u003dstr)\n            #return json_result\n            return result\n\n        except Exception as e:\n            print(f\"Failed to generate LLM response: {str(e)}\")\n            import traceback\n            traceback.print_exc()\n            return {\n                \"question\": self.user_query,\n                \"answer\": f\"Error: {str(e)}\",\n                \"contexts\": \"\"\n            }"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true
      },
      "source": [
        "# Example usage"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        ""
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        "\n# Import necessary libraries\n# from vector_store_creation import VectorStoreGeneration\nimport dataiku\nimport os\nfrom transformers import pipeline\n# Import ChromaDB monkeypatch to ensure compatibility\nfrom dataiku.core.vector_stores.chroma_vector_store import ChromaVectorStore\nChromaVectorStore.run_the_ugly_chromadb_monkeypatch()\n\n# host \u003d \"https://cdl-dku-desi-p.commercial-datalake-prod.awscloud.abbvienet.com/\"\n# apiKey \u003d \"dkuaps-9ALuuZLhFJg9dTcrSgPMcsdtfP8bpPXC\"\n# os.environ[\"DKU_CURRENT_PROJECT_KEY\"] \u003d \"GENAIPOC\" \n# dataiku.set_remote_dss(host, apiKey, no_check_certificate\u003dTrue)\n\n# Dataset and vector store configuration parameters\ninput_dataset_name \u003d \"input_data_embedded\"\noutput_dataset_name \u003d \"tripadvisor_hotel_reviews_with_names\"\nvector_store_type \u003d \u0027CHROMADB\u0027\n\n# Other configuration parameters\n\nEMBEDDING_MODEL \u003d \"custom:iliad-plugin-conn-prod:text-embedding-ada-002\"\nLLM_MODEL \u003d \"custom:iliad-plugin-conn-prod:gpt-4o\"\n\nvector_store \u003d VectorStoreGeneration(\n            input_dataset_name\u003dinput_dataset_name,\n            output_dataset_name\u003doutput_dataset_name,\n            user_query\u003d\"who to call for rinvoq reactions?\",\n            embedding_model\u003dEMBEDDING_MODEL,\n            llm\u003dLLM_MODEL,\n            vector_store_type\u003dvector_store_type,\n            top_k\u003d5,\n            use_compression\u003dFalse,\n            azure_openai_key\u003d\"None\"\n        )\n        \n# Process and get response\nresponse \u003d vector_store.process()\n    \nprint(response)\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        ""
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        ""
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true
      },
      "source": [
        "# using knowledge bank"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        "import dataiku\n# Import ChromaDB monkeypatch to ensure compatibility\nfrom dataiku.core.vector_stores.chroma_vector_store import ChromaVectorStore\nChromaVectorStore.run_the_ugly_chromadb_monkeypatch()\nclient \u003d dataiku.api_client()\nproject \u003d client.get_default_project()\n# llm_list \u003d project.list_llms()\nkb_list \u003d project.list_knowledge_banks(as_type\u003d\u0027listitems\u0027)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        "for kb in kb_list:\n    print(f\"- {kb.name} - (id: {kb.id})\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        "kb_object \u003d project.get_knowledge_bank(\"dV3dIQCo\") \nkb_core \u003d kb_object.as_core_knowledge_bank()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        "vectorstore_object \u003d kb_core.as_langchain_retriever(search_type\u003d\"similarity\", search_kwargs\u003d{\"k\": 5})"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        "from Digitization.Core.ModelManager import ModelManager\nEMBEDDING_MODEL \u003d \"custom:iliad-plugin-conn-prod:text-embedding-ada-002\"\nLLM_MODEL \u003d \"custom:iliad-plugin-conn-prod:gpt-4o\"\nmodel_manager \u003d ModelManager(EMBEDDING_MODEL,LLM_MODEL)\nllm \u003d model_manager.get_llm()\nembedding_model \u003d model_manager.get_embedding_model()"
      ],
      "outputs": []
    },
    {
      "execution_count": 3,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        "import dataiku\nfrom dataiku.core.dataset import Dataset\nfrom langchain.chains import RetrievalQA\nfrom langchain.memory import ConversationSummaryBufferMemory\nfrom langchain.chains import ConversationalRetrievalChain\nfrom langchain.prompts import PromptTemplate\nfrom Digitization.Core.ModelManager import ModelManager\n\nclass DataikuQASystem:\n    \"\"\"\n    A knowledge base question answering system for Dataiku.\n    This class handles the entire process of setting up and querying the knowledge base.\n    \"\"\"\n    \n    def __init__(self, kb_id\u003d\"1to7O1GS\", embedding_model_name\u003d\"custom:iliad-plugin-conn-prod:text-embedding-ada-002\", \n                 llm_model_name\u003d\"custom:iliad-plugin-conn-prod:gpt-4o\", k\u003d5):\n        \"\"\"\n        Initialize the QA system with knowledge bank and models.\n        \n        Args:\n            kb_id (str): The ID of the knowledge bank to use. If None, must be set later with set_kb.\n            embedding_model_name (str): The name of the embedding model to use.\n            llm_model_name (str): The name of the LLM model to use.\n            k (int): Number of documents to retrieve from the vector store.\n        \"\"\"\n        # Ensure ChromaDB compatibility\n        from dataiku.core.vector_stores.chroma_vector_store import ChromaVectorStore\n        ChromaVectorStore.run_the_ugly_chromadb_monkeypatch()\n        \n        # Initialize API client and project\n        self.client \u003d dataiku.api_client()\n        self.project \u003d self.client.get_default_project()\n        \n        # Set up models\n        self.model_manager \u003d ModelManager(embedding_model_name, llm_model_name)\n        self.llm \u003d self.model_manager.get_llm()\n        self.embedding_model \u003d self.model_manager.get_embedding_model()\n        \n        # Retrieval parameters\n        self.k \u003d k\n        self.vectorstore_object \u003d None\n        \n        # Set up knowledge bank if ID is provided\n        if kb_id:\n            self.set_kb(kb_id)\n        # Using summary buffer memory\n        self.memory \u003d ConversationSummaryBufferMemory(\n            llm\u003dself.llm,              # needs the LLM to produce summaries\n            max_token_limit\u003d512,       # how big the summary can grow\n            memory_key\u003d\"chat_history\", # name under which the summary is exposed\n            return_messages\u003dFalse,      # summary is plain text, not Message objects\n            input_key\u003d\"question\",       # matches the key passed into the chain\n            output_key\u003d\"answer\"         # pick the LLM’s answer as the memory output\n        )\n        \n        # QA chain\n        self.qa_chain \u003d None\n    \n    def set_kb(self, kb_id):\n        \"\"\"\n        Set the knowledge bank to use.\n        \n        Args:\n            kb_id (str): The ID of the knowledge bank.\n        \"\"\"\n        kb_object \u003d self.project.get_knowledge_bank(kb_id)\n        kb_core \u003d kb_object.as_core_knowledge_bank()\n        self.vectorstore_object \u003d kb_core.as_langchain_retriever(\n            search_type\u003d\"similarity\", \n            search_kwargs\u003d{\"k\": self.k}\n        )\n        # Reset the QA chain since we have a new knowledge bank\n        self.qa_chain \u003d None\n        \n        return self\n    \n    def _setup_qa_system(self):\n        \"\"\"\n        Set up the QA system with the current LLM and vector store.\n        This is called internally when needed.\n        \n        Returns:\n            RetrievalQA: A configured QA chain\n        \"\"\"\n        if not self.vectorstore_object:\n            raise ValueError(\"No knowledge bank set. Call set_kb() first.\")\n            \n        # Create a custom prompt template that includes instructions\n        template \u003d \"\"\"\n        You are an AI assistant that delivers precise, concise answers from provided documents. \n        Users may rephrase their question in many ways, but your job is to understand the intent and always answer from our knowledge base. \n        Below is a brief summary of our conversation so far:\n        {chat_history}\n        Follow these guidelines:\n\n                1. Analyze ALL content thoroughly for ANY information related to the query, even partial matches.\n                2. Present information directly without meta commentary\n                3. If referencing a specific document, use its actual name (e.g., \"According to the iEngage Quick Reference Guide\").\n                4. Provide ONLY relevant facts without adding commentary, introductions, or conclusions.\n                5. Format information as direct statements rather than numbered lists unless specifically requested.\n                6. Include ALL potentially relevant details no matter how minimal.\n                7. Present information in a concise, summarized format.\n                8. Only state information is insufficient if absolutely nothing related can be found.\n                9. Never generate content containing hate speech, offensive language, violence, threats, or misinformation.\n        \n        Context:\n        {context}\n        \n        Question: {question}\n        \n        Please provide a detailed answer based only on the context provided.\n        \"\"\"\n        \n        prompt \u003d PromptTemplate(\n            template\u003dtemplate,\n            input_variables\u003d[\"chat_history\", \"context\", \"question\"]\n        )\n        \n        # Create the QA chain\n#         self.qa_chain \u003d RetrievalQA.from_chain_type(\n#             llm\u003dself.llm,\n#             chain_type\u003d\"stuff\",  # Simple approach that \"stuffs\" all retrieved docs into the prompt\n#             retriever\u003dself.vectorstore_object,\n#             return_source_documents\u003dTrue,  # Include source documents in the response\n#             chain_type_kwargs\u003d{\"prompt\": prompt}\n#         )\n        self.qa_chain \u003d ConversationalRetrievalChain.from_llm(\n            llm\u003dself.llm,\n            retriever\u003dself.vectorstore_object,\n            memory\u003dself.memory,\n            return_source_documents\u003dTrue,\n            combine_docs_chain_kwargs\u003d{\"prompt\": prompt}\n#             chain_type_kwargs\u003d{\"prompt\": prompt}\n        )\n        \n        return self.qa_chain\n    \n    def query(self, user_query):\n        \"\"\"\n        Process a user query against the knowledge base and return answer + context.\n        \"\"\"\n        if not self.qa_chain:\n            self._setup_qa_system()\n\n        response \u003d self.qa_chain({\"question\": user_query})\n\n        answer \u003d response[\"answer\"]\n        source_documents \u003d response.get(\"source_documents\", [])\n\n        # Return raw answer and context separately\n        return answer, self._format_sources(source_documents)\n    \n    def _format_sources(self, sources):\n        \"\"\"\n        Format the context sources separately for display.\n        \"\"\"\n        formatted_sources \u003d \"\"\n\n        if sources:\n            for i, doc in enumerate(sources):\n                try:\n                    source_text \u003d doc.page_content[:150] + \"...\" if len(doc.page_content) \u003e 150 else doc.page_content\n                    metadata \u003d doc.metadata if hasattr(doc, \u0027metadata\u0027) else {}\n                    source_info \u003d f\"Document {i+1}\"\n\n                    if metadata:\n                        if \u0027source\u0027 in metadata:\n                            source_info +\u003d f\" (Source: {metadata[\u0027source\u0027]})\"\n                        elif \u0027title\u0027 in metadata:\n                            source_info +\u003d f\" (Title: {metadata[\u0027title\u0027]})\"\n\n                    formatted_sources +\u003d f\"\\n{source_info}:\\n{source_text}\\n\"\n                except AttributeError:\n                    formatted_sources +\u003d f\"\\nDocument {i+1}: [Format not recognized]\\n\"\n\n        return formatted_sources.strip()\n\n\n# Example usage:\n\"\"\"\n# Initialize the QA system with a knowledge bank ID\nqa_system \u003d DataikuQASystem(kb_id\u003d\"dV3dIQCo\")\n\n# Or initialize and set the knowledge bank separately\nqa_system \u003d DataikuQASystem()\nqa_system.set_kb(\"dV3dIQCo\")\n\n# Query the system\nresponse \u003d qa_system.query(\"What is the policy on medical leave?\")\nprint(response)\n\"\"\""
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "\u0027\\n# Initialize the QA system with a knowledge bank ID\\nqa_system \u003d DataikuQASystem(kb_id\u003d\"dV3dIQCo\")\\n\\n# Or initialize and set the knowledge bank separately\\nqa_system \u003d DataikuQASystem()\\nqa_system.set_kb(\"dV3dIQCo\")\\n\\n# Query the system\\nresponse \u003d qa_system.query(\"What is the policy on medical leave?\")\\nprint(response)\\n\u0027"
          },
          "metadata": {}
        }
      ]
    },
    {
      "execution_count": 5,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        "# Initialize the QA system with a knowledge bank ID\nqa_system \u003d DataikuQASystem(kb_id\u003d\"1to7O1GS\")\n\n# Or initialize and set the knowledge bank separately\nqa_system \u003d DataikuQASystem()\n# qa_system.set_kb(\"1to7O1GS\")\n\n# Query the system\nresponse \u003d qa_system.query(\"What are some common reactions?\")\nprint(response)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "Initialized LangChain embedding model: custom:iliad-plugin-conn-prod:text-embedding-ada-002\nInitialized LangChain LLM: custom:iliad-plugin-conn-prod:gpt-4o\nInitialized LangChain embedding model: custom:iliad-plugin-conn-prod:text-embedding-ada-002\nInitialized LangChain LLM: custom:iliad-plugin-conn-prod:gpt-4o\n(\u0027For reactions related to RINVOQ, you can call 1-800-2RINVOQ (1-800-274-6867) for assistance. This number provides 24/7 access to registered nurses, with immediate assistance available Monday through Friday from 8 am to 8 pm ET.\u0027, \"Document 1:\\n.  It starts with your RINVOQ Complete Nurse Ambassador who is always there to get to know you.\u0027, \u0027CTA 1\u0027: \u0027Call 1-800-2RINVOQ (1-800-274-6867)\u0027, \u0027CTA...\\n\\nDocument 2:\\n.  It starts with your RINVOQ Complete Nurse Ambassador who is always there to get to know you.\u0027, \u0027CTA 1\u0027: \u0027Call 1-800-2RINVOQ (1-800-274-6867)\u0027, \u0027CTA...\\n\\nDocument 3:\\n. Message and data rates may apply.\\nDEALING WITH A TREATMENT DISRUPTION? Visit SeeRTerms.com and see Terms of Use at abbv.ie/s-use. Text STOP to 33101...\\n\\nDocument 4:\\n.800.2RINVOQ (1.800.274.6867). Help is available\\nMonday through Friday from 8:00 AM to 8:00 PM ET, except for holidays.\\nVISUALLY CONNECT\\nCreate a sens...\\n\\nDocument 5:\\n. | look forward to getting to know you and\\nsupporting you with RINVOQ Complete resources.\\nIfyour insurance changes or life throws you a curve ball, |...\")\n",
          "name": "stdout"
        }
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        ""
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        ""
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "hidden": true
      },
      "source": [
        ""
      ],
      "outputs": []
    }
  ]
}