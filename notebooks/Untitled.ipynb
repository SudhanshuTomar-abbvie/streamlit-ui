{
  "metadata": {
    "kernelspec": {
      "name": "py-dku-venv-cdl-p-diku-psbts-py39-llm-env",
      "display_name": "Python (env cdl-p-diku-psbts-py39-llm-env)",
      "language": "python"
    },
    "hide_input": false,
    "language_info": {
      "name": "python",
      "version": "3.9.20",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "createdOn": 1744200433820,
    "modifiedBy": "chaudpx2",
    "customFields": {},
    "creator": "chaudpx2",
    "tags": []
  },
  "nbformat": 4,
  "nbformat_minor": 5,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip3 install locust"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import locust\nprint(locust.__version__)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "locust"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from locust import HttpUser, task\n\nclass HelloWorldUser(HttpUser):\n    @task\n    def hello_world(self):\n        self.client.get(\"/hello\")\n        self.client.get(\"/world\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"hello\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataikuapi\n\nclient \u003d dataikuapi.APINodeClient(\"http://k8s-default-dkumadse-f38d97347b-066209af6eb01332.elb.us-east-1.amazonaws.com:12000\", \"chat\")\n\nresult \u003d client.run_function(\"generate\",\n        user_query \u003d \"What is dqm?\")\nprint(\"Function result: %s\" % result.get(\"response\"))\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataikuapi\n\nclient \u003d dataikuapi.APINodeClient(\"http://k8s-default-dkumadse-f38d97347b-066209af6eb01332.elb.us-east-1.amazonaws.com:80\", \"chat\")\n\nresult \u003d client.run_function(\"read_multiple_files\",\n        files \u003d [{\"filename\":\"file1.txt\",\"content\":\"SGVsbG8gd29ybGQh\"},{\"filename\":\"file2.txt\",\"content\":\"VGhpcyBpcyBhIHRlc3QgZmlsZS4\u003d\"}])\nprint(\"Function result: %s\" % result.get(\"response\"))\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "{\u0027combined_text\u0027: \u0027--- file1.txt ---\\nHello world!\\n\\n--- file2.txt ---\\nThis is a test file.\\n\u0027, \u0027file_count\u0027: 2}"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    },
    {
      "execution_count": 15,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\nimport uuid\nimport logging\nimport shutil\nimport dataiku\nimport base64\nimport io\nfrom retriever import ModelDefination\nfrom process_and_embed_uploaded_files import process_and_embed_uploaded_files\n\n\nhost \u003d \"https://cdl-dku-desi-p.commercial-datalake-prod.awscloud.abbvienet.com/\"\napiKey \u003d \"dkuaps-9ALuuZLhFJg9dTcrSgPMcsdtfP8bpPXC\"\nos.environ[\"DKU_CURRENT_PROJECT_KEY\"] \u003d \"GENAIPOC\" \ndataiku.set_remote_dss(host, apiKey, no_check_certificate\u003dTrue)\n\n# Other configuration parameters\nEMBEDDING_MODEL \u003d \"custom:iliad-plugin-conn-prod:text-embedding-ada-002\"\nLLM_MODEL \u003d \"custom:iliad-plugin-conn-prod:gpt-4o\"\n\ndef read_multiple_files(data): \n    \"\"\"\n    Dataiku API endpoint to read and combine content from multiple uploaded files.\n\n    Args:\n        data (dict): Dictionary containing base64 encoded files.\n                     Expected format: {\"files\": [{\"filename\": ..., \"content\": ...}, ...]}\n\n    Returns:\n        dict: Combined content of all files.\n    \"\"\"\n    try:\n        uploaded_files \u003d data.get(\"files\", [])\n\n        if not uploaded_files:\n            return {\"error\": \"No files received.\"}\n\n        all_contents \u003d []\n\n        for file_info in uploaded_files:\n            filename \u003d file_info.get(\"filename\")\n            content_b64 \u003d file_info.get(\"content\")\n\n            if not filename or not content_b64:\n                continue\n\n            # Decode base64 content\n            file_bytes \u003d base64.b64decode(content_b64)\n            file_stream \u003d io.BytesIO(file_bytes)\n\n            # Assuming text-based files (e.g., .txt, .csv, .json)\n            try:\n                file_text \u003d file_stream.read().decode(\"utf-8\")\n            except UnicodeDecodeError:\n                file_text \u003d \"Unable to decode file: \" + filename\n\n            all_contents.append(f\"--- {filename} ---\\n{file_text}\\n\")\n\n        model \u003d ModelDefination(embedding_model\u003dEMBEDDING_MODEL, llm_id\u003dLLM_MODEL)\n\n        # Process and write to managed folder\n        result \u003d process_and_embed_uploaded_files(all_contents, model)\n        # print(result)\n        \n        return {\n            \"combined_text\": \"\\n\".join(all_contents),\n            \"file_count\": len(all_contents),\n            \"result\": result\n        }\n\n    except Exception as e:\n        return {\n            \"error\": \"Failed to process files.\",\n            \"details\": str(e)\n        }"
      ],
      "outputs": []
    },
    {
      "execution_count": 24,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\nimport pandas as pd\nimport re\nimport pickle\nfrom typing import List, Tuple\n\ndef extract_files_from_combined_text(combined_text: str) -\u003e List[Tuple[str, str]]:\n    \"\"\"\n    Extract filename and content pairs from the combined_text.\n    \"\"\"\n    pattern \u003d r\"--- (.*?) ---\\n(.*?)(?\u003d(?:---|$))\"\n    matches \u003d re.findall(pattern, combined_text, re.DOTALL)\n    if not matches:\n        print(\"No matches found in combined_text.\")\n    return [(filename.strip(), content.strip()) for filename, content in matches]\n\n\ndef chunk_text(text: str, chunk_size: int \u003d 500, overlap: int \u003d 100) -\u003e List[str]:\n    \"\"\"\n    Simple reusable chunking function.\n    \"\"\"\n    chunks \u003d []\n    start \u003d 0\n    while start \u003c len(text):\n        end \u003d start + chunk_size\n        chunks.append(text[start:end])\n        start \u003d end - overlap\n    return chunks\n\n\ndef process_and_embed_uploaded_files(output_dict: dict, model):\n    combined_text \u003d output_dict[0]\n    print(\"Debug: Combined Text:\\n\", combined_text)\n\n    file_entries \u003d extract_files_from_combined_text(combined_text)\n    print(\"Debug: Extracted Files:\\n\", file_entries)\n\n    all_chunks \u003d []\n\n    for filename, content in file_entries:\n        chunks \u003d chunk_text(content, chunk_size\u003d100, overlap\u003d50)\n        if not chunks:\n            print(f\"No chunks generated for {filename}.\")\n        print(f\"Debug: Chunks for {filename}:\\n\", chunks)\n\n        if chunks:\n            embeddings \u003d model.embedding_model.embed_documents(chunks)\n#             print(f\"Debug: Embeddings for {filename}:\\n\", embeddings)\n\n            chunk_data \u003d [\n                {\n                    \"chunk_text\": chunk,\n                    \"metadata\": {\"filename\": filename}\n                }\n                for chunk, embedding in zip(chunks, embeddings)\n            ]\n            all_chunks.extend(chunk_data)\n\n    new_df \u003d pd.DataFrame(all_chunks)\n    print(\"Debug: New DataFrame:\\n\", new_df)\n\n    embedding_dataset \u003d dataiku.Dataset(\"input_data_chunked\")\n    try:\n        existing_df \u003d embedding_dataset.get_dataframe()\n    except Exception as e:\n        existing_df \u003d pd.DataFrame(columns\u003d[\"chunk_text\", \"metadata\"])\n\n#     combined_df \u003d pd.concat([existing_df, new_df], ignore_index\u003dTrue)\n#     print(\"Debug: Combined DataFrame:\\n\", combined_df)\n\n#     embedding_dataset.write_with_schema(combined_df)\n\n    return {\n        \"status\": \"success\",\n        \"chunks_added\": len(new_df),\n        \"total_rows\": len(new_df)\n    }"
      ],
      "outputs": []
    },
    {
      "execution_count": 25,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dict \u003d {\n   \"files\": [\n      {\n         \"filename\": \"file1.txt\",\n         \"content\": \"SGVsbG8sIHRoaXMgaXMgYSBuZXcgbWVzc2FnZSE\u003d\"\n      },\n      {\n         \"filename\": \"file2.txt\",\n         \"content\": \"VGhpcyBpcyBhbm90aGVyIHRlc3QgZmlsZS4\u003d\"\n      }\n   ]\n}\n\nprint(read_multiple_files(dict))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "2025-05-15 04:39:43,348 - INFO - Initialized LangChain embedding model: custom:iliad-plugin-conn-prod:text-embedding-ada-002\n2025-05-15 04:39:43,349 - INFO - Initialized LangChain LLM: custom:iliad-plugin-conn-prod:gpt-4o\n2025-05-15 04:39:43,350 - INFO - Model Definition initialized with vector store type: FAISS\n2025-05-15 04:39:43,351 - INFO - Performing embedding of 1 texts\n/data/dataiku/dss_data/code-envs/python/cdl-p-diku-psbts-py39-llm-env/lib/python3.9/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host \u0027cdl-dku-desi-p.commercial-datalake-prod.awscloud.abbvienet.com\u0027. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n  warnings.warn(\n2025-05-15 04:39:43,375 - INFO - Finished a chunk. Embedded 1 of 1 texts\n2025-05-15 04:39:43,376 - INFO - Done performing embedding of 1 texts\n2025-05-15 04:39:43,378 - INFO - Reading dataset GENAIPOC.input_data_chunked as dataframe\n/data/dataiku/dss_data/code-envs/python/cdl-p-diku-psbts-py39-llm-env/lib/python3.9/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host \u0027cdl-dku-desi-p.commercial-datalake-prod.awscloud.abbvienet.com\u0027. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n  warnings.warn(\n/data/dataiku/dss_data/code-envs/python/cdl-p-diku-psbts-py39-llm-env/lib/python3.9/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host \u0027cdl-dku-desi-p.commercial-datalake-prod.awscloud.abbvienet.com\u0027. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n  warnings.warn(\n/data/dataiku/dss_data/code-envs/python/cdl-p-diku-psbts-py39-llm-env/lib/python3.9/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host \u0027cdl-dku-desi-p.commercial-datalake-prod.awscloud.abbvienet.com\u0027. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n  warnings.warn(\n2025-05-15 04:39:43,452 - INFO - Done reading dataset GENAIPOC.input_data_chunked as dataframe rows\u003d1253 cols\u003d3 mem\u003d1.99MB\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "Debug: Combined Text:\n --- file1.txt ---\nHello, this is a new message!\n\nDebug: Extracted Files:\n [(\u0027file1.txt\u0027, \u0027Hello, this is a new message!\u0027)]\nDebug: Chunks for file1.txt:\n [\u0027Hello, this is a new message!\u0027]\nDebug: New DataFrame:\n                       chunk_text                   metadata\n0  Hello, this is a new message!  {\u0027filename\u0027: \u0027file1.txt\u0027}\n{\u0027combined_text\u0027: \u0027--- file1.txt ---\\nHello, this is a new message!\\n\\n--- file2.txt ---\\nThis is another test file.\\n\u0027, \u0027file_count\u0027: 2, \u0027result\u0027: {\u0027status\u0027: \u0027success\u0027, \u0027chunks_added\u0027: 1, \u0027total_rows\u0027: 1}}\n",
          "name": "stdout"
        }
      ]
    },
    {
      "execution_count": 28,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\nimport dataiku\nimport base64\nimport io\nimport pandas as pd\nimport re\nfrom retriever import ModelDefination\n\n\n# DSS connection setup\nhost \u003d \"https://cdl-dku-desi-p.commercial-datalake-prod.awscloud.abbvienet.com/\"\napiKey \u003d \"dkuaps-9ALuuZLhFJg9dTcrSgPMcsdtfP8bpPXC\"\nos.environ[\"DKU_CURRENT_PROJECT_KEY\"] \u003d \"GENAIPOC\"\ndataiku.set_remote_dss(host, apiKey, no_check_certificate\u003dTrue)\n\n# Model configuration\nEMBEDDING_MODEL \u003d \"custom:iliad-plugin-conn-prod:text-embedding-ada-002\"\nLLM_MODEL \u003d \"custom:iliad-plugin-conn-prod:gpt-4o\"\n\n\ndef read_multiple_files(data):\n    \"\"\"\n    Dataiku API endpoint to read and combine content from multiple uploaded files.\n\n    Args:\n        data (dict): Dictionary containing base64 encoded files.\n                     Expected format: {\"files\": [{\"filename\": ..., \"content\": ...}, ...]}\n\n    Returns:\n        dict: Combined content of all files.\n    \"\"\"\n    try:\n        uploaded_files \u003d data.get(\"files\", [])\n        if not uploaded_files:\n            return {\"error\": \"No files received.\"}\n\n        all_contents \u003d []\n\n        for file_info in uploaded_files:\n            filename \u003d file_info.get(\"filename\")\n            content_b64 \u003d file_info.get(\"content\")\n\n            if not filename or not content_b64:\n                continue\n\n            # Decode base64 content\n            file_bytes \u003d base64.b64decode(content_b64)\n            file_stream \u003d io.BytesIO(file_bytes)\n\n            # Assuming text-based files (e.g., .txt, .csv, .json)\n            try:\n                file_text \u003d file_stream.read().decode(\"utf-8\")\n            except UnicodeDecodeError:\n                file_text \u003d f\"Unable to decode file: {filename}\"\n\n            all_contents.append(f\"--- {filename} ---\\n{file_text}\\n\")\n\n        # Combine all file contents into a single string\n        combined_text \u003d \"\\n\".join(all_contents)\n        print(\"Debug: Combined Text:\\n\", combined_text)\n\n        # Initialize the model\n        model \u003d ModelDefination(embedding_model\u003dEMBEDDING_MODEL, llm_id\u003dLLM_MODEL)\n\n        # Process and embed files\n        result \u003d process_and_embed_uploaded_files({\"combined_text\": combined_text}, model)\n\n        return {\n            \"combined_text\": combined_text,\n            \"file_count\": len(uploaded_files),\n            \"result\": result\n        }\n\n    except Exception as e:\n        return {\n            \"error\": \"Failed to process files.\",\n            \"details\": str(e)\n        }\n\n\ndef extract_files_from_combined_text(combined_text: str):\n    \"\"\"\n    Extract filename and content pairs from the combined_text.\n    \"\"\"\n    # Updated regex to correctly match multiple files\n    pattern \u003d r\"--- (.*?) ---\\n(.*?)(?\u003d\\n---|$)\"\n    matches \u003d re.findall(pattern, combined_text, re.DOTALL)\n    print(\"Debug: Extracted Files:\\n\", matches)\n    return [(filename.strip(), content.strip()) for filename, content in matches]\n\n\ndef chunk_text(text: str, chunk_size: int \u003d 500, overlap: int \u003d 100):\n    \"\"\"\n    Simple reusable chunking function.\n    \"\"\"\n    chunks \u003d []\n    start \u003d 0\n    while start \u003c len(text):\n        end \u003d start + chunk_size\n        chunks.append(text[start:end])\n        start \u003d end - overlap\n    return chunks\n\n\ndef process_and_embed_uploaded_files(output_dict: dict, model):\n    \"\"\"\n    Process the uploaded files\u0027 text, chunk and embed them, and store back to the same managed folder.\n    \"\"\"\n    combined_text \u003d output_dict[\"combined_text\"]\n    file_entries \u003d extract_files_from_combined_text(combined_text)\n\n    all_chunks \u003d []\n\n    for filename, content in file_entries:\n        # Chunk the text\n        chunks \u003d chunk_text(content, chunk_size\u003d500, overlap\u003d100)\n        print(f\"Debug: Chunks for {filename}:\\n\", chunks)\n\n        # Embedding logic (uncomment and replace with your actual embedding logic)\n        # embeddings \u003d model.embedding_model.embed_documents(chunks)\n        embeddings \u003d [None] * len(chunks)  # Placeholder embeddings for debugging\n\n        # Prepare chunk data\n        chunk_data \u003d [\n            {\n                \"chunk_text\": chunk,\n                \"metadata\": {\"filename\": filename}\n            }\n            for chunk in chunks  # zip(chunks, embeddings) if embeddings are available\n        ]\n        all_chunks.extend(chunk_data)\n\n    # Convert to DataFrame\n    new_df \u003d pd.DataFrame(all_chunks)\n    print(\"Debug: New DataFrame:\\n\", new_df)\n\n    # Load existing DataFrame from managed folder (as pickle)\n    embedding_dataset \u003d dataiku.Dataset(\"input_data_chunked\")\n\n    try:\n        existing_df \u003d embedding_dataset.get_dataframe()\n    except Exception:\n        # If the dataset doesn\u0027t exist, start with an empty DataFrame\n        existing_df \u003d pd.DataFrame(columns\u003d[\"chunk_text\", \"metadata\"])\n\n    # Combine with new data\n    combined_df \u003d pd.concat([existing_df, new_df], ignore_index\u003dTrue)\n    print(\"Debug: Combined DataFrame:\\n\", combined_df)\n\n    # Save back to the dataset\n    embedding_dataset.write_with_schema(combined_df)\n\n    return {\n        \"status\": \"success\",\n        \"chunks_added\": len(new_df),\n        \"total_rows\": len(combined_df)\n    }\n\n\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 29,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example input\ndict_data \u003d {\n    \"files\": [\n        {\n            \"filename\": \"file1.txt\",\n            \"content\": \"SGVsbG8sIHRoaXMgaXMgYSBuZXcgbWVzc2FnZSE\u003d\"\n        },\n        {\n            \"filename\": \"file2.txt\",\n            \"content\": \"VGhpcyBpcyBhbm90aGVyIHRlc3QgZmlsZS4\u003d\"\n        }\n    ]\n}\n\nprint(read_multiple_files(dict_data))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "2025-05-15 04:42:52,801 - INFO - Initialized LangChain embedding model: custom:iliad-plugin-conn-prod:text-embedding-ada-002\n2025-05-15 04:42:52,802 - INFO - Initialized LangChain LLM: custom:iliad-plugin-conn-prod:gpt-4o\n2025-05-15 04:42:52,803 - INFO - Model Definition initialized with vector store type: FAISS\n2025-05-15 04:42:52,806 - INFO - Reading dataset GENAIPOC.input_data_chunked as dataframe\n/data/dataiku/dss_data/code-envs/python/cdl-p-diku-psbts-py39-llm-env/lib/python3.9/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host \u0027cdl-dku-desi-p.commercial-datalake-prod.awscloud.abbvienet.com\u0027. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n  warnings.warn(\n/data/dataiku/dss_data/code-envs/python/cdl-p-diku-psbts-py39-llm-env/lib/python3.9/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host \u0027cdl-dku-desi-p.commercial-datalake-prod.awscloud.abbvienet.com\u0027. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n  warnings.warn(\n/data/dataiku/dss_data/code-envs/python/cdl-p-diku-psbts-py39-llm-env/lib/python3.9/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host \u0027cdl-dku-desi-p.commercial-datalake-prod.awscloud.abbvienet.com\u0027. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n  warnings.warn(\n2025-05-15 04:42:52,885 - INFO - Done reading dataset GENAIPOC.input_data_chunked as dataframe rows\u003d1253 cols\u003d3 mem\u003d1.99MB\n/data/dataiku/dss_data/code-envs/python/cdl-p-diku-psbts-py39-llm-env/lib/python3.9/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host \u0027cdl-dku-desi-p.commercial-datalake-prod.awscloud.abbvienet.com\u0027. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n  warnings.warn(\n2025-05-15 04:42:52,899 - INFO - Initializing dataset writer for dataset GENAIPOC.input_data_chunked\n2025-05-15 04:42:52,899 - INFO - Initializing write session\n/data/dataiku/dss_data/code-envs/python/cdl-p-diku-psbts-py39-llm-env/lib/python3.9/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host \u0027cdl-dku-desi-p.commercial-datalake-prod.awscloud.abbvienet.com\u0027. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n  warnings.warn(\n2025-05-15 04:42:52,907 - INFO - Starting RemoteStreamWriter\n/data/dataiku/dss_data/code-envs/python/cdl-p-diku-psbts-py39-llm-env/lib/python3.9/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host \u0027cdl-dku-desi-p.commercial-datalake-prod.awscloud.abbvienet.com\u0027. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n  warnings.warn(\n2025-05-15 04:42:52,909 - INFO - Initializing write data stream (8e1noIkzPT)\n/data/dataiku/dss_data/code-envs/python/cdl-p-diku-psbts-py39-llm-env/lib/python3.9/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host \u0027cdl-dku-desi-p.commercial-datalake-prod.awscloud.abbvienet.com\u0027. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n  warnings.warn(\n2025-05-15 04:42:52,957 - INFO - Remote Stream Writer: start generate\n2025-05-15 04:42:52,958 - INFO - Waiting for data to send ...\n2025-05-15 04:42:52,958 - INFO - Sending data (1396181)\n2025-05-15 04:42:52,960 - INFO - Waiting for data to send ...\n2025-05-15 04:42:52,960 - INFO - Remote Stream Writer closed\n2025-05-15 04:42:52,961 - INFO - Got end mark, ending send\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "Debug: Combined Text:\n --- file1.txt ---\nHello, this is a new message!\n\n--- file2.txt ---\nThis is another test file.\n\nDebug: Extracted Files:\n [(\u0027file1.txt\u0027, \u0027Hello, this is a new message!\\n\u0027), (\u0027file2.txt\u0027, \u0027This is another test file.\u0027)]\nDebug: Chunks for file1.txt:\n [\u0027Hello, this is a new message!\u0027]\nDebug: Chunks for file2.txt:\n [\u0027This is another test file.\u0027]\nDebug: New DataFrame:\n                       chunk_text                   metadata\n0  Hello, this is a new message!  {\u0027filename\u0027: \u0027file1.txt\u0027}\n1     This is another test file.  {\u0027filename\u0027: \u0027file2.txt\u0027}\nDebug: Combined DataFrame:\n                                              chunk_text                                           metadata                                         embeddings\n0     ### Extracted Text:\\nPSIT Patient Journey AI M...  {\"file_name\": \"Phase 1 Requirements - PSIT Pat...                                                NaN\n1     . The funnel image below illustrates the patie...  {\"file_name\": \"Phase 1 Requirements - PSIT Pat...                                                NaN\n2     . Understanding a patient’s current status is ...  {\"file_name\": \"Phase 1 Requirements - PSIT Pat...                                                NaN\n3     .\\nLevel 1: Patient Status Log Table (Base Lay...  {\"file_name\": \"Phase 1 Requirements - PSIT Pat...                                                NaN\n4     . These sequential numbers facilitate a chrono...  {\"file_name\": \"Phase 1 Requirements - PSIT Pat...                                                NaN\n...                                                 ...                                                ...                                                ...\n1250  ### Extracted Text:\\nUnsupported file type: RI...  {\"file_name\": \"RIN AD PA Journey Map Image 10-...                                                NaN\n1251                                       Hello world!                          {\u0027filename\u0027: \u0027file1.txt\u0027}  [0.005530369933694601, 0.0034913793206214905, ...\n1252                      Hello, this is a new message!                          {\u0027filename\u0027: \u0027file1.txt\u0027}                                                NaN\n1253                      Hello, this is a new message!                          {\u0027filename\u0027: \u0027file1.txt\u0027}                                                NaN\n1254                         This is another test file.                          {\u0027filename\u0027: \u0027file2.txt\u0027}                                                NaN\n\n[1255 rows x 3 columns]\n1255 rows successfully written (8e1noIkzPT)\n{\u0027combined_text\u0027: \u0027--- file1.txt ---\\nHello, this is a new message!\\n\\n--- file2.txt ---\\nThis is another test file.\\n\u0027, \u0027file_count\u0027: 2, \u0027result\u0027: {\u0027status\u0027: \u0027success\u0027, \u0027chunks_added\u0027: 2, \u0027total_rows\u0027: 1255}}\n",
          "name": "stdout"
        }
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    },
    {
      "execution_count": 42,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\nimport dataiku\nimport base64\nimport io\nimport pandas as pd\nimport re\nfrom retriever import ModelDefination\n\n# DSS connection setup\nhost \u003d \"https://cdl-dku-desi-p.commercial-datalake-prod.awscloud.abbvienet.com/\"\napiKey \u003d \"dkuaps-9ALuuZLhFJg9dTcrSgPMcsdtfP8bpPXC\"\nos.environ[\"DKU_CURRENT_PROJECT_KEY\"] \u003d \"GENAIPOC\"\ndataiku.set_remote_dss(host, apiKey, no_check_certificate\u003dTrue)\n\n# Model configuration\nEMBEDDING_MODEL \u003d \"custom:iliad-plugin-conn-prod:text-embedding-ada-002\"\nLLM_MODEL \u003d \"custom:iliad-plugin-conn-prod:gpt-4o\"\n\n\ndef read_multiple_files(data):\n    \"\"\"\n    Dataiku API endpoint to read and combine content from multiple uploaded files.\n\n    Args:\n        data (dict): Dictionary containing base64 encoded files.\n                     Expected format: {\"files\": [{\"filename\": ..., \"content\": ...}, ...]}\n\n    Returns:\n        dict: Combined content of all files.\n    \"\"\"\n    try:\n        uploaded_files \u003d data.get(\"files\", [])\n        if not uploaded_files:\n            return {\"error\": \"No files received.\"}\n\n        all_contents \u003d []\n\n        for file_info in uploaded_files:\n            filename \u003d file_info.get(\"filename\")\n            content_b64 \u003d file_info.get(\"content\")\n\n            if not filename or not content_b64:\n                continue\n\n            # Decode base64 content\n            file_bytes \u003d base64.b64decode(content_b64)\n            file_stream \u003d io.BytesIO(file_bytes)\n\n            # Assuming text-based files (e.g., .txt, .csv, .json)\n            try:\n                file_text \u003d file_stream.read().decode(\"utf-8\")\n            except UnicodeDecodeError:\n                file_text \u003d f\"Unable to decode file: {filename}\"\n\n            all_contents.append(f\"--- {filename} ---\\n{file_text}\\n\")\n\n        # Combine all file contents into a single string\n        combined_text \u003d \"\\n\".join(all_contents)\n        print(\"Debug: Combined Text:\\n\", combined_text)\n\n        # Initialize the model\n        model \u003d ModelDefination(embedding_model\u003dEMBEDDING_MODEL, llm_id\u003dLLM_MODEL)\n\n        # Process and embed files (write to input_data_chunked dataset)\n        result \u003d process_and_embed_uploaded_files({\"combined_text\": combined_text}, model)\n\n        #✅ Trigger the plugin recipe\n        client \u003d dataiku.api_client()\n        project \u003d client.get_project(\"GENAIPOC\")  # Use your actual project key\n        recipe \u003d project.get_recipe(\"compute_input_data_chunked_embedded\")  # Recipe name\n        job \u003d recipe.run()\n        job.wait_for_completion()\n        print(\"✅ Plugin recipe \u0027compute_input_data_chunked_embedded\u0027 executed.\")\n\n        return {\n            \"combined_text\": combined_text,\n            \"file_count\": len(uploaded_files),\n            \"result\": result,\n            \"plugin_status\": \"triggered\"\n        }\n\n    except Exception as e:\n        return {\n            \"error\": \"Failed to process files.\",\n            \"details\": str(e)\n        }\n\n\ndef extract_files_from_combined_text(combined_text: str):\n    \"\"\"\n    Extract filename and content pairs from the combined_text.\n    \"\"\"\n    # Updated regex to correctly match multiple files\n    pattern \u003d r\"--- (.*?) ---\\n(.*?)(?\u003d\\n---|$)\"\n    matches \u003d re.findall(pattern, combined_text, re.DOTALL)\n    print(\"Debug: Extracted Files:\\n\", matches)\n    return [(filename.strip(), content.strip()) for filename, content in matches]\n\n\ndef chunk_text(text: str, chunk_size: int \u003d 500, overlap: int \u003d 100):\n    \"\"\"\n    Simple reusable chunking function.\n    \"\"\"\n    chunks \u003d []\n    start \u003d 0\n    while start \u003c len(text):\n        end \u003d start + chunk_size\n        chunks.append(text[start:end])\n        start \u003d end - overlap\n    return chunks\n\n\ndef process_and_embed_uploaded_files(output_dict: dict, model):\n    \"\"\"\n    Process the uploaded files\u0027 text, chunk and embed them, and store back to the same managed folder.\n    Ensures:\n        - Proper metadata format for each chunk.\n        - Duplicate file names are not reprocessed.\n    \"\"\"\n    combined_text \u003d output_dict[\"combined_text\"]\n    file_entries \u003d extract_files_from_combined_text(combined_text)\n\n    # Load existing DataFrame from managed folder (as pickle)\n    embedding_dataset \u003d dataiku.Dataset(\"input_data_chunked\")\n\n    try:\n        existing_df \u003d embedding_dataset.get_dataframe()\n    except Exception:\n        existing_df \u003d pd.DataFrame(columns\u003d[\"chunk_text\", \"metadata\"])\n\n    # Extract existing file names from metadata\n    existing_file_names \u003d set()\n    if not existing_df.empty:\n        existing_file_names \u003d {\n            meta.get(\"file_name\")\n            for meta in existing_df[\"metadata\"].dropna().apply(lambda x: eval(x) if isinstance(x, str) else x)\n        }\n\n    all_chunks \u003d []\n\n    for filename, content in file_entries:\n        if filename in existing_file_names:\n            print(f\"Skipping {filename} as it already exists in the dataset.\")\n            continue\n\n        # Chunk the text\n        chunks \u003d chunk_text(content, chunk_size\u003d500, overlap\u003d100)\n        print(f\"Debug: Chunks for {filename}:\\n\", chunks)\n\n        # Embedding logic (placeholder)\n        embeddings \u003d [None] * len(chunks)  # Replace with actual embeddings if needed\n\n        # Prepare chunk data with proper metadata format\n        chunk_data \u003d [\n            {\n                \"chunk_text\": chunk,\n                \"metadata\": {\n                    \"file_name\": filename,\n                    \"image_links\": \"\",  # Assuming no image_links for now\n                    \"chunk_id\": f\"{filename}_chunk_{i+1}\",\n                    \"chunk_order\": i + 1\n                }\n            }\n            for i, chunk in enumerate(chunks)\n        ]\n\n        all_chunks.extend(chunk_data)\n\n    # If no new chunks, return\n    if not all_chunks:\n        return {\n            \"status\": \"no_new_chunks\",\n            \"message\": \"No new data was added because all files already exist.\",\n            \"chunks_added\": 0,\n            \"total_rows\": len(existing_df)\n        }\n\n    # Convert to DataFrame\n    new_df \u003d pd.DataFrame(all_chunks)\n\n    # Combine with existing data\n    combined_df \u003d pd.concat([existing_df, new_df], ignore_index\u003dTrue)\n\n    # Save back to the dataset\n    embedding_dataset.write_with_schema(combined_df)\n\n    return {\n        \"status\": \"success\",\n        \"chunks_added\": len(new_df),\n        \"total_rows\": len(combined_df)\n    }\n\n\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 43,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example input\ndict_data \u003d {\n    \"files\": [\n        {\n            \"filename\": \"file3.txt\",\n            \"content\": \"U29tZSBjb250ZW50IGZvciBmaWxlMy4\u003d\"\n        }\n#         {\n#             \"filename\": \"file4.txt\",\n#             \"content\": \"V2VsbCwgSG93IGFyZSB5b3U/\"\n#         }\n    ]\n}\n\nprint(read_multiple_files(dict_data))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "2025-05-15 05:32:41,908 - INFO - Initialized LangChain embedding model: custom:iliad-plugin-conn-prod:text-embedding-ada-002\n2025-05-15 05:32:41,909 - INFO - Initialized LangChain LLM: custom:iliad-plugin-conn-prod:gpt-4o\n2025-05-15 05:32:41,909 - INFO - Model Definition initialized with vector store type: FAISS\n2025-05-15 05:32:41,910 - INFO - Reading dataset GENAIPOC.input_data_chunked as dataframe\n/data/dataiku/dss_data/code-envs/python/cdl-p-diku-psbts-py39-llm-env/lib/python3.9/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host \u0027cdl-dku-desi-p.commercial-datalake-prod.awscloud.abbvienet.com\u0027. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n  warnings.warn(\n/data/dataiku/dss_data/code-envs/python/cdl-p-diku-psbts-py39-llm-env/lib/python3.9/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host \u0027cdl-dku-desi-p.commercial-datalake-prod.awscloud.abbvienet.com\u0027. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n  warnings.warn(\n/data/dataiku/dss_data/code-envs/python/cdl-p-diku-psbts-py39-llm-env/lib/python3.9/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host \u0027cdl-dku-desi-p.commercial-datalake-prod.awscloud.abbvienet.com\u0027. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n  warnings.warn(\n2025-05-15 05:32:41,990 - INFO - Done reading dataset GENAIPOC.input_data_chunked as dataframe rows\u003d1259 cols\u003d3 mem\u003d1.99MB\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "Debug: Combined Text:\n --- file3.txt ---\nSome content for file3.\n\nDebug: Extracted Files:\n [(\u0027file3.txt\u0027, \u0027Some content for file3.\u0027)]\nSkipping file3.txt as it already exists in the dataset.\n{\u0027error\u0027: \u0027Failed to process files.\u0027, \u0027details\u0027: \"Recipe has unsupported output type COMPUTABLE_RETRIEVABLE_KNOWLEDGE, can\u0027t run it\"}\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/data/dataiku/dss_data/code-envs/python/cdl-p-diku-psbts-py39-llm-env/lib/python3.9/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host \u0027cdl-dku-desi-p.commercial-datalake-prod.awscloud.abbvienet.com\u0027. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n  warnings.warn(\n",
          "name": "stderr"
        }
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    }
  ]
}