{
  "metadata": {
    "kernelspec": {
      "name": "py-dku-venv-cdl-p-diku-psbts-py39-llm-env",
      "display_name": "Python (env cdl-p-diku-psbts-py39-llm-env)",
      "language": "python"
    },
    "hide_input": false,
    "language_info": {
      "name": "python",
      "version": "3.9.20",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "tags": [],
    "customFields": {},
    "createdOn": 1744872819562,
    "creator": "tomarsx1",
    "modifiedBy": "tomarsx1"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%pylab inline"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\nfrom dataiku import pandasutils as pdu\nimport pandas as pd"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example: load a DSS dataset as a Pandas dataframe\nmydataset \u003d dataiku.Dataset(\"mydataset\")\nmydataset_df \u003d mydataset.get_dataframe()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\nimport shutil\nimport zipfile\nimport tempfile\nimport logging\nimport time\nfrom io import BytesIO\nimport pandas as pd\nimport concurrent.futures\nfrom functools import partial\n\nimport dataiku\nfrom docx import Document\nimport pdfplumber\nfrom pptx import Presentation\n\nfrom concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\nimport multiprocessing\nfrom tqdm import tqdm  # For progress tracking\n\n# Configure logging\nlogging.basicConfig(level\u003dlogging.INFO, format\u003d\u0027%(asctime)s - %(levelname)s - %(message)s\u0027)\nlogger \u003d logging.getLogger(__name__)\n\n\nclass BaseExtractor:\n    \"\"\"\n    Base extractor that handles file reading from a Dataiku Folder.\n    \"\"\"\n    def __init__(self, folder_id):\n        self.data_source \u003d dataiku.Folder(folder_id)\n        # Cache for small file data to avoid repeated downloads\n        self._file_cache \u003d {}\n        \n    def get_file_data(self, file_path):\n        \"\"\"\n        Reads file data from the Dataiku Folder with caching for small files.\n        \"\"\"\n        if file_path in self._file_cache:\n            return self._file_cache[file_path]\n        \n        with self.data_source.get_download_stream(file_path) as f:\n            data \u003d f.read()\n            # Only cache files smaller than 10MB to avoid memory issues\n            if len(data) \u003c 10 * 1024 * 1024:\n                self._file_cache[file_path] \u003d data\n            return data\n    \n    def get_file_size(self, file_path):\n        \"\"\"Get the size of a file without downloading its entire content.\"\"\"\n        try:\n            file_info \u003d self.data_source.get_path_details(file_path)\n            return file_info.get(\u0027size\u0027, 0)\n        except Exception as e:\n            logger.warning(f\"Could not get size for {file_path}: {e}\")\n            return 0\n\n\nclass WordExtractor(BaseExtractor):\n    \"\"\"\n    Extracts text, tables, and images from Word documents.\n    \"\"\"\n    def extract_text_and_tables(self, file_path):\n        \"\"\"\n        Extracts text and table content from a Word document.\n        \"\"\"\n        try:\n            file_data \u003d self.get_file_data(file_path)\n            doc_stream \u003d BytesIO(file_data)\n            doc \u003d Document(doc_stream)\n\n            # Extract text from paragraphs\n            text_content \u003d [para.text.strip() for para in doc.paragraphs if para.text.strip()]\n\n            # Extract table data\n            table_data \u003d []\n            for table in doc.tables:\n                table_content \u003d []\n                for row in table.rows:\n                    row_data \u003d [cell.text.strip() for cell in row.cells]\n                    table_content.append(row_data)\n                table_data.append(table_content)\n\n            return {\"text\": \"\\n\".join(text_content), \"tables\": table_data}\n        except Exception as e:\n            logger.error(f\"Error reading Word document {file_path}: {e}\")\n            return {\"error\": f\"Error reading Word document: {e}\"}\n\n    def extract_images(self, file_path, output_folder_id):\n        \"\"\"\n        Extracts images from a Word document and saves them to a managed folder.\n        Uses chunking for large files.\n        \"\"\"\n        try:\n            file_size \u003d self.get_file_size(file_path)\n            file_data \u003d self.get_file_data(file_path)\n            file_name \u003d os.path.basename(file_path)\n            base_name \u003d os.path.splitext(file_name)[0]\n\n            # Get the output folder for images\n            image_folder \u003d dataiku.Folder(output_folder_id)\n            \n            # List existing files in the output folder\n            existing_images \u003d set(image_folder.list_paths_in_partition())\n\n            # Create a temporary directory for extraction\n            temp_dir \u003d tempfile.mkdtemp()\n            temp_docx_path \u003d os.path.join(temp_dir, file_name)\n            with open(temp_docx_path, \"wb\") as temp_file:\n                temp_file.write(file_data)\n\n            images_extracted \u003d []\n            \n            # Process ZIP file in chunks for efficiency\n            with zipfile.ZipFile(temp_docx_path, \"r\") as docx_zip:\n                # Get all media files first\n                media_files \u003d [f for f in docx_zip.namelist() if f.startswith(\"word/media/\")]\n                \n                # Process media files in chunks to avoid memory issues with large files\n                chunk_size \u003d 20  # Adjust based on average image size\n                for i in range(0, len(media_files), chunk_size):\n                    media_chunk \u003d media_files[i:i+chunk_size]\n                    \n                    for file_info_name in media_chunk:\n                        image_name \u003d os.path.basename(file_info_name)\n                        image_path \u003d f\"{base_name}_{image_name}\"\n                        \n                        # Skip if image already exists\n                        if image_path in existing_images:\n                            images_extracted.append(image_path)\n                            continue\n                            \n                        image_data \u003d docx_zip.read(file_info_name)\n                        with image_folder.get_writer(image_path) as writer:\n                            writer.write(image_data)\n                        images_extracted.append(image_path)\n\n            shutil.rmtree(temp_dir, ignore_errors\u003dTrue)\n            return images_extracted\n        except Exception as e:\n            logger.error(f\"Error extracting images from {file_path}: {e}\")\n            return {\"error\": f\"Error extracting images: {e}\"}\n\n\nclass PDFExtractor(BaseExtractor):\n    \"\"\"\n    Extracts text from PDF documents using chunking for large files.\n    \"\"\"\n    def extract_text(self, file_path):\n        \"\"\"\n        Extracts text content from a PDF file processing pages in chunks.\n        \"\"\"\n        try:\n            file_data \u003d self.get_file_data(file_path)\n            pdf_stream \u003d BytesIO(file_data)\n            \n            text_chunks \u003d []\n            with pdfplumber.open(pdf_stream) as pdf:\n                total_pages \u003d len(pdf.pages)\n                logger.info(f\"Processing PDF {file_path} with {total_pages} pages\")\n                \n                # Process pages in chunks to manage memory for large PDFs\n                chunk_size \u003d 20  # Number of pages to process at once\n                for i in range(0, total_pages, chunk_size):\n                    end_idx \u003d min(i + chunk_size, total_pages)\n                    logger.debug(f\"Processing PDF pages {i+1}-{end_idx} of {total_pages}\")\n                    \n                    # Extract text from the current chunk of pages\n                    chunk_text \u003d []\n                    for page_num in range(i, end_idx):\n                        page \u003d pdf.pages[page_num]\n                        extracted \u003d page.extract_text()\n                        if extracted:\n                            chunk_text.append(extracted)\n                    \n                    text_chunks.extend(chunk_text)\n            \n            return \"\\n\".join(text_chunks)\n        except Exception as e:\n            logger.error(f\"Error reading PDF {file_path}: {e}\")\n            return f\"Error reading PDF: {e}\"\n\n\nclass PowerPointExtractor(BaseExtractor):\n    \"\"\"\n    Extracts text from PowerPoint presentations.\n    \"\"\"\n    def extract_text(self, file_path):\n        \"\"\"\n        Extracts text from each slide in a PowerPoint file.\n        \"\"\"\n        try:\n            file_data \u003d self.get_file_data(file_path)\n            ppt_stream \u003d BytesIO(file_data)\n            prs \u003d Presentation(ppt_stream)\n            \n            text_parts \u003d []\n            total_slides \u003d len(prs.slides)\n            logger.info(f\"Processing PowerPoint {file_path} with {total_slides} slides\")\n            \n            # Process slides in chunks for very large presentations\n            for slide_idx, slide in enumerate(prs.slides):\n                if slide_idx % 20 \u003d\u003d 0:  # Log progress for large files\n                    logger.debug(f\"Processing slide {slide_idx+1}/{total_slides}\")\n                \n                # Extract text from each shape that has text\n                for shape in slide.shapes:\n                    if hasattr(shape, \"text\") and shape.text:\n                        text_parts.append(shape.text.strip())\n                    \n            return \"\\n\".join(text_parts)\n        except Exception as e:\n            logger.error(f\"Error reading PowerPoint {file_path}: {e}\")\n            return f\"Error reading PowerPoint: {e}\"\n\n\nclass SpreadsheetExtractor(BaseExtractor):\n    \"\"\"\n    Extracts data from Excel and CSV files with chunking for large files.\n    \"\"\"\n    def extract_data(self, file_path):\n        try:\n            file_size \u003d self.get_file_size(file_path)\n            file_data \u003d self.get_file_data(file_path)\n            file_ext \u003d os.path.splitext(file_path)[1].lower()\n            \n            # Large file handling (\u003e50MB)\n            is_large_file \u003d file_size \u003e 50 * 1024 * 1024\n            logger.info(f\"Processing spreadsheet {file_path} (size: {file_size/1024/1024:.2f}MB, large file: {is_large_file})\")\n\n            if file_ext in [\u0027.xls\u0027, \u0027.xlsx\u0027]:\n                result \u003d {}\n                \n                if is_large_file:\n                    # For large Excel files, process sheet by sheet\n                    xl_file \u003d pd.ExcelFile(file_data)\n                    for sheet_name in xl_file.sheet_names:\n                        logger.debug(f\"Processing large Excel sheet: {sheet_name}\")\n                        \n                        # Process the sheet in chunks\n                        chunks \u003d []\n                        chunk_size \u003d 10000  # Rows per chunk\n                        for chunk_num, chunk_df in enumerate(pd.read_excel(\n                                file_data, sheet_name\u003dsheet_name, chunksize\u003dchunk_size)):\n                            chunks.append(chunk_df)\n                            logger.debug(f\"Processed chunk {chunk_num+1} of sheet {sheet_name}\")\n                        \n                        # Combine chunks\n                        if chunks:\n                            sheet_df \u003d pd.concat(chunks, ignore_index\u003dTrue)\n                            result[sheet_name] \u003d sheet_df.to_dict(orient\u003d\u0027records\u0027)\n                        else:\n                            result[sheet_name] \u003d []\n                else:\n                    # For smaller files, read all at once\n                    excel_data \u003d pd.read_excel(file_data, sheet_name\u003dNone)\n                    result \u003d {sheet_name: df.to_dict(orient\u003d\u0027records\u0027) \n                              for sheet_name, df in excel_data.items()}\n                \n                return result\n                \n            elif file_ext \u003d\u003d \u0027.csv\u0027:\n                if is_large_file:\n                    # Process large CSV in chunks\n                    chunks \u003d []\n                    chunk_size \u003d 50000  # Rows per chunk\n                    for chunk_num, chunk_df in enumerate(pd.read_csv(\n                            BytesIO(file_data), chunksize\u003dchunk_size)):\n                        chunks.append(chunk_df)\n                        logger.debug(f\"Processed CSV chunk {chunk_num+1}\")\n                    \n                    # Combine chunks\n                    if chunks:\n                        df \u003d pd.concat(chunks, ignore_index\u003dTrue)\n                        return {\"Sheet1\": df.to_dict(orient\u003d\u0027records\u0027)}\n                    return {\"Sheet1\": []}\n                else:\n                    # For smaller files, read all at once\n                    df \u003d pd.read_csv(BytesIO(file_data))\n                    return {\"Sheet1\": df.to_dict(orient\u003d\u0027records\u0027)}\n            else:\n                return {\"error\": f\"Unsupported spreadsheet file type: {file_ext}\"}\n        except Exception as e:\n            logger.error(f\"Error reading spreadsheet {file_path}: {e}\")\n            return {\"error\": f\"Error reading spreadsheet: {e}\"}       \n        \n\nclass FileProcessor:\n    def __init__(self, folder_id, image_output_folder_id\u003dNone, max_workers\u003dNone, max_retries\u003d3, \n                 retry_delay\u003d1, chunk_size\u003d25, large_file_threshold_mb\u003d50):\n        self.folder_id \u003d folder_id\n        self.image_output_folder_id \u003d image_output_folder_id\n        # Default to using about half the cores for worker threads to avoid overloading\n        self.max_workers \u003d max_workers or max(2, os.cpu_count() // 2)\n        self.max_retries \u003d max_retries\n        self.retry_delay \u003d retry_delay\n        self.chunk_size \u003d chunk_size\n        self.large_file_threshold_mb \u003d large_file_threshold_mb\n        \n        # Shared extractors for efficiency\n        self._pdf_extractor \u003d PDFExtractor(folder_id)\n        self._word_extractor \u003d WordExtractor(folder_id)\n        self._ppt_extractor \u003d PowerPointExtractor(folder_id)\n        self._spreadsheet_extractor \u003d SpreadsheetExtractor(folder_id)\n        \n        logger.info(f\"Initialized FileProcessor with {self.max_workers} workers, \"\n                   f\"{self.max_retries} retry attempts, chunk size of {self.chunk_size}, \"\n                   f\"and large file threshold of {self.large_file_threshold_mb}MB\")\n    \n    def process_file(self, file_path, retry_count\u003d0):\n        \"\"\"\n        Process a single file with retry logic and chunking for large files.\n        \"\"\"\n        file_name \u003d os.path.basename(file_path)\n        ext \u003d os.path.splitext(file_name)[1].lower()\n        result \u003d {\"file_name\": file_name, \"file_path\": file_path}\n        \n        try:\n            # Check file size to determine if special handling is needed\n            file_size_mb \u003d self._pdf_extractor.get_file_size(file_path) / (1024 * 1024)\n            is_large_file \u003d file_size_mb \u003e self.large_file_threshold_mb\n            \n            if is_large_file:\n                logger.info(f\"Processing large file: {file_name} ({file_size_mb:.2f}MB)\")\n\n            if ext \u003d\u003d \".pdf\":\n                result[\"text_content\"] \u003d self._pdf_extractor.extract_text(file_path)\n                result[\"table_content\"] \u003d \"\"\n                result[\"images_extracted\"] \u003d \"\"\n\n            elif ext \u003d\u003d \".docx\":\n                content \u003d self._word_extractor.extract_text_and_tables(file_path)\n                result[\"text_content\"] \u003d content.get(\"text\", content.get(\"error\", \"\"))\n                result[\"table_content\"] \u003d str(content.get(\"tables\", \"\"))\n                \n                if self.image_output_folder_id:\n                    images \u003d self._word_extractor.extract_images(file_path, self.image_output_folder_id)\n                    result[\"images_extracted\"] \u003d str(images)\n                else:\n                    result[\"images_extracted\"] \u003d \"\"\n\n            elif ext in [\".ppt\", \".pptx\"]:\n                result[\"text_content\"] \u003d self._ppt_extractor.extract_text(file_path)\n                result[\"table_content\"] \u003d \"\"\n                result[\"images_extracted\"] \u003d \"\"\n\n            elif ext in [\".xls\", \".xlsx\", \".csv\"]:\n                data \u003d self._spreadsheet_extractor.extract_data(file_path)\n                result[\"text_content\"] \u003d \"\" if \"error\" not in data else data[\"error\"]\n                # Convert to string representation, handling potentially large data\n                if \"error\" not in data:\n                    # Check for extremely large data structure before conversion\n                    total_records \u003d sum(len(sheet_data) for sheet_data in data.values())\n                    if total_records \u003e 100000:\n                        # For very large data, provide a summary instead\n                        sheet_summaries \u003d [f\"{sheet}: {len(records)} records\" \n                                         for sheet, records in data.items()]\n                        result[\"table_content\"] \u003d f\"Large dataset: {total_records} total records across sheets: \" + \\\n                                                 \", \".join(sheet_summaries)\n                    else:\n                        result[\"table_content\"] \u003d str(data)\n                else:\n                    result[\"table_content\"] \u003d \"\"\n                result[\"images_extracted\"] \u003d \"\"\n\n            else:\n                result[\"text_content\"] \u003d f\"Unsupported file type: {file_name}\"\n                result[\"table_content\"] \u003d \"\"\n                result[\"images_extracted\"] \u003d \"\"\n\n            # Check if there was an error message in the text_content\n            if isinstance(result[\"text_content\"], str) and result[\"text_content\"].startswith(\"Error:\"):\n                raise Exception(result[\"text_content\"])\n                \n            return result\n\n        except Exception as e:\n            logger.warning(f\"Error processing file {file_name} (attempt {retry_count + 1}/{self.max_retries + 1}): {e}\")\n            \n            # Check if we should retry\n            if retry_count \u003c self.max_retries:\n                # Use exponential backoff for retry delay\n                backoff_delay \u003d self.retry_delay * (2 ** retry_count)\n                logger.info(f\"Retrying file {file_name} in {backoff_delay} seconds...\")\n                time.sleep(backoff_delay)\n                return self.process_file(file_path, retry_count + 1)\n            else:\n                logger.error(f\"Failed to process file {file_name} after {self.max_retries + 1} attempts\")\n                result[\"text_content\"] \u003d f\"Error: {e} (failed after {self.max_retries + 1} attempts)\"\n                result[\"table_content\"] \u003d \"\"\n                result[\"images_extracted\"] \u003d \"\"\n                return result\n    \n    def _process_file_batch(self, file_batch):\n        \"\"\"Process a batch of files and return results.\"\"\"\n        results \u003d []\n        for idx, file_path in enumerate(file_batch):\n            try:\n                result \u003d self.process_file(file_path)\n                results.append(result)\n                logger.info(f\"Processed {idx+1}/{len(file_batch)} in current batch: {file_path}\")\n            except Exception as e:\n                logger.error(f\"Unexpected error with file {file_path}: {e}\")\n                results.append({\n                    \"file_name\": os.path.basename(file_path),\n                    \"file_path\": file_path,\n                    \"text_content\": f\"Fatal error: {e}\",\n                    \"table_content\": \"\",\n                    \"images_extracted\": \"\"\n                })\n        return results\n    \n    def process_all_files(self, file_list):\n        \"\"\"\n        Processes all files in batches for better performance and memory management.\n        \"\"\"\n        logger.info(f\"Starting processing of {len(file_list)} files in batches of {self.chunk_size}\")\n        \n        # Split files into batches for processing\n        file_batches \u003d [file_list[i:i + self.chunk_size] \n                       for i in range(0, len(file_list), self.chunk_size)]\n        \n        all_results \u003d []\n        \n        # Process each batch with progress tracking\n        with tqdm(total\u003dlen(file_list), desc\u003d\"Processing files\") as pbar:\n            # Process batches in parallel\n            with ThreadPoolExecutor(max_workers\u003dself.max_workers) as executor:\n                batch_futures \u003d [executor.submit(self._process_file_batch, batch) \n                                for batch in file_batches]\n                \n                for future in as_completed(batch_futures):\n                    try:\n                        batch_results \u003d future.result()\n                        all_results.extend(batch_results)\n                        pbar.update(len(batch_results))\n                    except Exception as e:\n                        logger.error(f\"Error processing batch: {e}\")\n        \n        logger.info(f\"Completed processing {len(all_results)} files\")\n        return pd.DataFrame(all_results)\n\n\ndef distribute_files_by_size(file_list, extractor, num_groups\u003d4):\n    \"\"\"\n    Distribute files into groups balancing by file size to improve parallel processing.\n    \"\"\"\n    # Get file sizes\n    file_sizes \u003d []\n    for file_path in file_list:\n        size \u003d extractor.get_file_size(file_path)\n        file_sizes.append((file_path, size))\n    \n    # Sort by size (descending)\n    file_sizes.sort(key\u003dlambda x: x[1], reverse\u003dTrue)\n    \n    # Distribute files to balance size across groups\n    groups \u003d [[] for _ in range(num_groups)]\n    group_sizes \u003d [0] * num_groups\n    \n    for file_path, size in file_sizes:\n        # Find the group with the smallest total size\n        min_group \u003d group_sizes.index(min(group_sizes))\n        groups[min_group].append(file_path)\n        group_sizes[min_group] +\u003d size\n    \n    return groups\n\n\ndef main(folder_id, output_folder_id\u003dNone):\n    \"\"\"Main function to demonstrate balanced parallel processing.\"\"\"\n    # Step 1: Initialize the processor\n    processor \u003d FileProcessor(\n        folder_id\u003dfolder_id,\n        image_output_folder_id\u003doutput_folder_id,\n        max_workers\u003dmax(2, os.cpu_count() // 2),\n        chunk_size\u003d25,\n        large_file_threshold_mb\u003d50\n    )\n    \n    # Step 2: Get the list of files\n    folder \u003d dataiku.Folder(folder_id)\n    file_list \u003d folder.list_paths_in_partition()\n    \n    # Step 3: Distribute files by size for balanced processing\n    base_extractor \u003d BaseExtractor(folder_id)\n    file_groups \u003d distribute_files_by_size(file_list, base_extractor, \n                                          num_groups\u003dprocessor.max_workers)\n    \n    # Step 4: Process each group in parallel\n    all_results \u003d []\n    with ThreadPoolExecutor(max_workers\u003dprocessor.max_workers) as executor:\n        group_futures \u003d [executor.submit(processor.process_all_files, group) \n                        for group in file_groups]\n        \n        for future in as_completed(group_futures):\n            try:\n                group_results \u003d future.result()\n                all_results.append(group_results)\n            except Exception as e:\n                logger.error(f\"Error processing group: {e}\")\n    \n    # Combine results from all groups\n    final_results \u003d pd.concat(all_results, ignore_index\u003dTrue)\n    return final_results\n\nmain()"
      ],
      "outputs": []
    }
  ]
}